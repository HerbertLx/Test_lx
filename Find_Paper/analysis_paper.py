import json
import os
import random
from typing import Dict, Any, List

import pandas as pd
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import itertools
from tqdm import tqdm

# ===========================
# ğŸ¤– æç¤ºè¯ï¼ˆPromptï¼‰é…ç½®åŒº
# ===========================

# ç¿»è¯‘æ¨¡æ¿
TRANSLATION_TEMPLATE = {
    "æ ‡é¢˜": "æ ‡é¢˜çš„ä¸­æ–‡ç¿»è¯‘",
    "æ‘˜è¦": "æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘"
}

# åˆ†ææ¨¡æ¿
ANALYSIS_TEMPLATE = {
    "title": "è®ºæ–‡æ ‡é¢˜",
    "extracted_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "..."],
    "classification": {
        "platform": "é€‰å®šçš„æ ‡ç­¾",
        "methodology": "é€‰å®šçš„æ ‡ç­¾",
        "application": "é€‰å®šçš„æ ‡ç­¾"
    },
    "summary": "ç”¨ä¸€å¥è¯ç²¾ç‚¼æ¦‚æ‹¬æ ¸å¿ƒè´¡çŒ®"
}

# ç¿»è¯‘æç¤ºè¯
TRANSLATION_PROMPT = '''ä½ æ˜¯ä¸€åæœºå™¨äººé¢†åŸŸçš„ç§‘ç ”åŠ©ç†ï¼Œå½“å‰ä»»åŠ¡æ˜¯å°†è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ã€‚

è¾“å…¥æ ¼å¼ï¼š
è®ºæ–‡æ ‡é¢˜ï¼š[è‹±æ–‡æ ‡é¢˜]
æ‘˜è¦ï¼š[è‹±æ–‡æ‘˜è¦]

è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. **ç¿»è¯‘**ï¼šå°†è‹±æ–‡æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡

è¯·ä¸¥æ ¼ä»…è¾“å‡º JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜æˆ– Markdown æ ¼å¼ã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{
  "æ ‡é¢˜": "æ ‡é¢˜çš„ä¸­æ–‡ç¿»è¯‘",
  "æ‘˜è¦": "æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘"
}
'''

# åˆ†ææç¤ºè¯
ANALYSIS_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººé¢†åŸŸèµ„æ·±ä¸“å®¶ï¼Œè¯·é˜…è¯»è®ºæ–‡æ ‡é¢˜ä¸æ‘˜è¦ï¼Œæ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

è¾“å…¥æ ¼å¼ï¼š
è®ºæ–‡æ ‡é¢˜ï¼š[è‹±æ–‡æ ‡é¢˜]
æ‘˜è¦ï¼š[è‹±æ–‡æ‘˜è¦]

Task 1: å…³é”®è¯æå– è‡ªä¸»æå– 3-5 ä¸ªåæ˜ è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯ã€å¯¹è±¡æˆ–åœºæ™¯çš„ä¸­æ–‡å…³é”®è¯ã€‚

Task 2: æ ‡å‡†åŒ–åˆ†ç±» åŸºäºæå–çš„ç†è§£ï¼Œä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦å„é€‰å‡ºä¸€ä¸ªæœ€åŒ¹é…çš„æ ‡ç­¾ï¼ˆè‹¥ä¸åŒ¹é…æˆ–æœªæåŠï¼Œç»Ÿä¸€å½’ç±»ä¸º"å…¶å®ƒ"ï¼‰ï¼š

ç¡¬ä»¶å¹³å° (Platform): æœºæ¢°è‡‚ã€äººå½¢æœºå™¨äººã€è¶³å¼æœºå™¨äººã€æ— äººæœºã€æ™ºèƒ½å°è½¦ã€æ°´ä¸‹/ç©ºé—´æœºå™¨äººã€åŒ»ç–—/åº·å¤æœºå™¨äººã€æŸ”æ€§/å¾®çº³æœºå™¨äººã€å…¶å®ƒã€‚

æŠ€æœ¯æ–¹æ³• (Methodology): å¼ºåŒ–å­¦ä¹ ã€æ¨¡ä»¿å­¦ä¹ ã€å¤§æ¨¡å‹/å…·èº«AIã€SLAM/å¯¼èˆªã€è®¡ç®—æœºè§†è§‰/æ„ŸçŸ¥ã€è·¯å¾„è§„åˆ’ã€æ§åˆ¶ç†è®ºã€äººæœºäº¤äº’ã€å¤šæœºååŒã€å…¶å®ƒã€‚

åº”ç”¨åœºæ™¯ (Application): å·¥ä¸šåˆ¶é€ ã€ç‰©æµä»“å‚¨ã€å®¶åº­æœåŠ¡ã€é‡å¤–/æœæ•‘ã€è‡ªåŠ¨é©¾é©¶ã€å†œä¸š/ç¯å¢ƒã€æ‰‹æœ¯/è¯Šç–—ã€æ•™è‚²/å¨±ä¹ã€å…¶å®ƒã€‚

è¯·ä¸¥æ ¼ä»…è¾“å‡º JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜æˆ– Markdown æ ¼å¼ã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{
  "title": "è®ºæ–‡æ ‡é¢˜",
  "extracted_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "..."],
  "classification": {
    "platform": "é€‰å®šçš„æ ‡ç­¾",
    "methodology": "é€‰å®šçš„æ ‡ç­¾",
    "application": "é€‰å®šçš„æ ‡ç­¾"
  },
  "summary": "ç”¨ä¸€å¥è¯ç²¾ç‚¼æ¦‚æ‹¬æ ¸å¿ƒè´¡çŒ®"
}
'''

# ===========================
# æ ¸å¿ƒå‡½æ•°ï¼šDeepSeek è®ºæ–‡åˆ†æï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ===========================

def deepseek_translate_paper(title: str, abstract: str, api_key: str) -> Dict[str, Any]:
    """ä½¿ç”¨DeepSeek APIç¿»è¯‘è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦"""
    user_prompt = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{abstract}"
    
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": TRANSLATION_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={'type': 'json_object'},
            stream=False,
            temperature=0.0
        )
        raw_output = response.choices[0].message.content.strip()
        result = json.loads(raw_output)
        return result

    except Exception as e:
        return {
            "æ ‡é¢˜": f"[ç¿»è¯‘] {title}",
            "æ‘˜è¦": f"[ç¿»è¯‘] {abstract[:100]}..."
        }


def deepseek_analyze_paper_json(title: str, abstract: str, api_key: str) -> Dict[str, Any]:
    """ä½¿ç”¨DeepSeek APIåˆ†æè®ºæ–‡"""
    user_prompt = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{abstract}"
    
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": ANALYSIS_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={'type': 'json_object'},
            stream=False,
            temperature=0.0
        )
        raw_output = response.choices[0].message.content.strip()
        result = json.loads(raw_output)
        result['title'] = title
        return result

    except Exception as e:
        return {
            "title": title,
            "extracted_keywords": ["N/A"],
            "classification": {
                "platform": "å…¶å®ƒ",
                "methodology": "å…¶å®ƒ",
                "application": "å…¶å®ƒ"
            },
            "summary": "åˆ†æå¤±è´¥"
        }

# ===========================
# æ‰¹é‡å¤„ç† CSVï¼šè‡ªåŠ¨è¾“å‡ºè·¯å¾„ + æ–­ç‚¹ç»­ä¼  + å¹¶å‘
# ===========================

def batch_process_csv(input_path: str, output_path: str, api_keys: List[str], test_mode: bool = False, test_size: int = 10):
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)

    # 1. è¯»å–åŸå§‹æ•°æ®
    df_original = pd.read_csv(input_path)
    if "Title" not in df_original.columns or "Abstract" not in df_original.columns:
        raise ValueError("è¾“å…¥ CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼šTitle, Abstract")

    # 2. å°è¯•åŠ è½½å·²æœ‰ç»“æœï¼ˆç”¨äºç»­ä¼ ï¼‰
    df_results = None
    if os.path.exists(output_path):
        print(f"âœ… æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå°è¯•è¯»å–å·²å¤„ç†ç»“æœï¼š{output_path}")
        df_results = pd.read_csv(output_path)
        # æ¸…ç†åˆ—åä¸­çš„ç©ºæ ¼
        df_results.columns = df_results.columns.str.strip()
        if "title" not in df_results.columns:
            print("âš ï¸ è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ç¼ºå°‘ 'title' åˆ—ï¼Œå°†å¿½ç•¥å·²å­˜åœ¨çš„ç»“æœã€‚")
            df_results = None

    # 3. åˆå§‹åŒ–æœ€ç»ˆ DataFrame
    df_final = df_original.copy()
    # ç¡®ä¿åˆ—åæ ¼å¼æ­£ç¡®
    desired_columns = ["æ ‡é¢˜", "æ‘˜è¦", "å…³é”®è¯", "å¹³å°", "æ–¹æ³•", "åº”ç”¨åœºæ™¯", "æ€»ç»“"]
    for col in desired_columns:
        if col not in df_final.columns:
            df_final[col] = None

    # 4. åŒæ­¥å·²æœ‰ç»“æœ
    synced_count = 0
    if df_results is not None:
        # æ„å»ºå¤„ç†æ•°æ®çš„æ˜ å°„
        processed_data = {}
        for idx, row in df_results.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title:
                continue
            processed_data[title] = {
                "æ ‡é¢˜": row.get("æ ‡é¢˜", ""),
                "æ‘˜è¦": row.get("æ‘˜è¦", ""),
                "å…³é”®è¯": row.get("å…³é”®è¯", "N/A"),
                "å¹³å°": row.get("å¹³å°", "å…¶å®ƒ"),
                "æ–¹æ³•": row.get("æ–¹æ³•", "å…¶å®ƒ"),
                "åº”ç”¨åœºæ™¯": row.get("åº”ç”¨åœºæ™¯", "å…¶å®ƒ"),
                "æ€»ç»“": row.get("æ€»ç»“", "N/A")
            }
        
        # åŒæ­¥æ•°æ®
        for idx, row in df_final.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title or title.lower() in ("nan", "none"):
                continue
            if title in processed_data:
                data = processed_data[title]
                df_final.loc[idx, "æ ‡é¢˜"] = data.get("æ ‡é¢˜", "")
                df_final.loc[idx, "æ‘˜è¦"] = data.get("æ‘˜è¦", "")
                df_final.loc[idx, "å…³é”®è¯"] = data.get("å…³é”®è¯", "N/A")
                df_final.loc[idx, "å¹³å°"] = data.get("å¹³å°", "å…¶å®ƒ")
                df_final.loc[idx, "æ–¹æ³•"] = data.get("æ–¹æ³•", "å…¶å®ƒ")
                df_final.loc[idx, "åº”ç”¨åœºæ™¯"] = data.get("åº”ç”¨åœºæ™¯", "å…¶å®ƒ")
                df_final.loc[idx, "æ€»ç»“"] = data.get("æ€»ç»“", "N/A")
                synced_count += 1
        print(f"   å·²åŒæ­¥ {synced_count} æ¡å·²å¤„ç†ç»“æœåˆ°å½“å‰æ‰¹æ¬¡ã€‚")

    # 5. æ”¶é›†å¾…å¤„ç†ä»»åŠ¡
    tasks_to_run = []
    total_rows = len(df_final)
    skipped_rows = 0
    need_process_count = 0
    
    print(f"\nğŸ” æ‰«æ {total_rows} è¡Œæ•°æ®ï¼Œç­›é€‰éœ€è¦å¤„ç†çš„ä»»åŠ¡...")
    
    for idx, row in df_final.iterrows():
        # æ£€æŸ¥Titleå’ŒAbstractæ˜¯å¦å®Œæ•´
        title = str(row.get("Title", "")).strip()
        abstract = str(row.get("Abstract", "")).strip()
        
        if not title or not abstract:
            skipped_rows += 1
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç¼ºå°‘çš„å†…å®¹
        need_process = False
        
        # æ£€æŸ¥ç¿»è¯‘
        if pd.isna(row.get("æ ‡é¢˜")) or str(row.get("æ ‡é¢˜")).strip() == "":
            need_process = True
        if pd.isna(row.get("æ‘˜è¦")) or str(row.get("æ‘˜è¦")).strip() == "":
            need_process = True
        
        # æ£€æŸ¥åˆ†æ
        if pd.isna(row.get("å…³é”®è¯")) or str(row.get("å…³é”®è¯")).strip() == "":
            need_process = True
        if pd.isna(row.get("å¹³å°")) or str(row.get("å¹³å°")).strip() == "":
            need_process = True
        if pd.isna(row.get("æ–¹æ³•")) or str(row.get("æ–¹æ³•")).strip() == "":
            need_process = True
        if pd.isna(row.get("åº”ç”¨åœºæ™¯")) or str(row.get("åº”ç”¨åœºæ™¯")).strip() == "":
            need_process = True
        if pd.isna(row.get("æ€»ç»“")) or str(row.get("æ€»ç»“")).strip() == "":
            need_process = True

        if need_process:
            tasks_to_run.append({'index': idx, 'title': title, 'abstract': abstract})
            need_process_count += 1

    print(f"   è·³è¿‡ {skipped_rows} è¡Œï¼ˆTitleæˆ–Abstractä¸å®Œæ•´ï¼‰")
    print(f"   éœ€è¦å¤„ç† {need_process_count} è¡Œï¼ˆç¼ºå°‘ç¿»è¯‘æˆ–åˆ†æç»“æœï¼‰")

    # æµ‹è¯•æ¨¡å¼ï¼šæŒ‰é¡ºåºå–å‰Nä¸ªä»»åŠ¡
    if test_mode and tasks_to_run:
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å¼ï¼šä» {len(tasks_to_run)} ä¸ªä»»åŠ¡ä¸­æŒ‰é¡ºåºå–å‰ {test_size} ä¸ªè¿›è¡Œæµ‹è¯•")
        tasks_to_run = tasks_to_run[:min(test_size, len(tasks_to_run))]

    rows_to_process = len(tasks_to_run)
    print(f"\n--- å¾…å¤„ç†æ€»è¡Œæ•°: {total_rows} | å®é™…å¤„ç†ä»»åŠ¡æ•°: {rows_to_process} ---")

    if rows_to_process == 0:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€è¿è¡Œæ–°ä»»åŠ¡ã€‚")
        return

    # 6. å¹¶å‘å¤„ç†
    api_key_cycler = itertools.cycle(api_keys)
    max_workers = min(len(api_keys), 10)  # é˜²æ­¢çº¿ç¨‹è¿‡å¤šï¼ˆå¯è°ƒï¼‰
    results_list = []
    success_count = 0
    failure_count = 0

    print(f"\nğŸš€ å¼€å§‹å¤„ç† {rows_to_process} ä¸ªä»»åŠ¡...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for task in tasks_to_run:
            idx = task['index']
            title = task['title']
            abstract = task['abstract']
            key = next(api_key_cycler)
            
            # å…ˆç¿»è¯‘ï¼Œå†åˆ†æ
            def process_task(idx, title, abstract, key):
                # ç¿»è¯‘
                try:
                    translation = deepseek_translate_paper(title, abstract, key)
                except Exception as e:
                    translation = {"æ ‡é¢˜": f"[ç¿»è¯‘] {title}", "æ‘˜è¦": f"[ç¿»è¯‘] {abstract[:100]}..."}
                # åˆ†æ
                try:
                    analysis = deepseek_analyze_paper_json(title, abstract, key)
                except Exception as e:
                    analysis = {
                        "title": title,
                        "extracted_keywords": ["N/A"],
                        "classification": {
                            "platform": "å…¶å®ƒ",
                            "methodology": "å…¶å®ƒ",
                            "application": "å…¶å®ƒ"
                        },
                        "summary": "åˆ†æå¤±è´¥"
                    }
                return idx, translation, analysis
            
            future = executor.submit(process_task, idx, title, abstract, key)
            futures[future] = idx

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=rows_to_process, desc="å¤„ç†è¿›åº¦", unit="ç¯‡", ncols=100) as pbar:
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    idx, translation, analysis = future.result()
                    results_list.append((idx, translation, analysis))
                    success_count += 1
                    # æ˜¾ç¤ºå½“å‰å¤„ç†çš„è¡Œå·å’Œå¹³å°
                    platform = analysis.get('classification', {}).get('platform', 'å…¶å®ƒ')
                    pbar.update(1)
                    pbar.set_postfix({"è¡Œå·": idx+1, "å¹³å°": platform, "å‰©ä½™": rows_to_process - (success_count + failure_count)})
                except Exception as e:
                    failure_count += 1
                    pbar.update(1)
                    pbar.set_postfix({"è¡Œå·": idx+1, "çŠ¶æ€": "å¤±è´¥", "å‰©ä½™": rows_to_process - (success_count + failure_count)})

    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   æ€»ä»»åŠ¡æ•°: {rows_to_process}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {failure_count}")

    # 7. æ›´æ–°å¹¶ä¿å­˜
    print(f"\nğŸ’¾ æ›´æ–°å¹¶ä¿å­˜ç»“æœ...")
    for idx, translation, analysis in results_list:
        # æ›´æ–°ç¿»è¯‘
        df_final.at[idx, "æ ‡é¢˜"] = translation.get("æ ‡é¢˜", "")
        df_final.at[idx, "æ‘˜è¦"] = translation.get("æ‘˜è¦", "")
        # æ›´æ–°åˆ†æ
        df_final.at[idx, "å…³é”®è¯"] = ", ".join(analysis.get("extracted_keywords", ["N/A"]))
        df_final.at[idx, "å¹³å°"] = analysis.get("classification", {}).get("platform", "å…¶å®ƒ")
        df_final.at[idx, "æ–¹æ³•"] = analysis.get("classification", {}).get("methodology", "å…¶å®ƒ")
        df_final.at[idx, "åº”ç”¨åœºæ™¯"] = analysis.get("classification", {}).get("application", "å…¶å®ƒ")
        df_final.at[idx, "æ€»ç»“"] = analysis.get("summary", "N/A")

    # 8. ä¿å­˜ç»“æœ
    # åªä¿å­˜éœ€è¦çš„åˆ—
    output_columns = ["Title", "DOI", "Abstract", "æ ‡é¢˜", "æ‘˜è¦", "å…³é”®è¯", "å¹³å°", "æ–¹æ³•", "åº”ç”¨åœºæ™¯", "æ€»ç»“"]
    df_output = df_final[[col for col in output_columns if col in df_final.columns]]
    
    df_output.to_csv(output_path, index=False, encoding='utf-8-sig')

    # 9. æ£€æŸ¥ç©ºå•å…ƒæ ¼
    print(f"\nğŸ” æ£€æŸ¥ç©ºå•å…ƒæ ¼...")
    
    # è¯»å–ä¿å­˜åçš„æ–‡ä»¶è¿›è¡Œæ£€æŸ¥
    df_check = pd.read_csv(output_path)
    df_check.columns = df_check.columns.str.strip()
    
    # ç›®æ ‡åˆ—
    target_columns = ["æ ‡é¢˜", "æ‘˜è¦", "å…³é”®è¯", "å¹³å°", "æ–¹æ³•", "åº”ç”¨åœºæ™¯", "æ€»ç»“"]
    
    # ç»Ÿè®¡ç©ºå•å…ƒæ ¼
    empty_cells = {}
    total_empty = 0
    
    for col in target_columns:
        if col in df_check.columns:
            # æ£€æŸ¥ç©ºå€¼
            empty_count = df_check[col].isna().sum()
            # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²
            empty_string_count = df_check[col].astype(str).str.strip().eq('').sum()
            # æ£€æŸ¥"N/A"å€¼
            na_count = df_check[col].astype(str).str.strip().eq('N/A').sum()
            
            # æ€»ç©ºå€¼æ•°
            total_empty_in_col = empty_count + empty_string_count + na_count
            empty_cells[col] = total_empty_in_col
            total_empty += total_empty_in_col
    
    # è¾“å‡ºæ£€æŸ¥ç»“æœ
    print(f"\nğŸ“Š ç©ºå•å…ƒæ ¼æ£€æŸ¥ç»“æœ:")
    print(f"   ç›®æ ‡è¿è¡Œè¡Œæ•°: {total_rows}")
    print(f"   å®é™…å¤„ç†è¡Œæ•°: {rows_to_process}")
    print(f"   æ€»ç©ºå•å…ƒæ ¼æ•°: {total_empty}")
    print(f"   å„åˆ—ç©ºå•å…ƒæ ¼æ•°:")
    for col, count in empty_cells.items():
        print(f"      {col}: {count}")
    
    # æ£€æŸ¥å…·ä½“å“ªäº›è¡Œæœ‰ç©ºå•å…ƒæ ¼
    print(f"\nğŸ” æ£€æŸ¥å…·ä½“ç©ºå•å…ƒæ ¼ä½ç½®...")
    empty_rows = []
    
    for idx, row in df_check.iterrows():
        row_empty = False
        empty_cols = []
        
        for col in target_columns:
            if col in df_check.columns:
                value = row.get(col, "")
                if pd.isna(value) or str(value).strip() == "" or str(value).strip() == "N/A":
                    row_empty = True
                    empty_cols.append(col)
        
        if row_empty:
            empty_rows.append((idx+1, empty_cols))  # è¡Œå·ä»1å¼€å§‹
    
    # è¾“å‡ºæœ‰ç©ºå•å…ƒæ ¼çš„è¡Œ
    if empty_rows:
        print(f"\nâš ï¸ å‘ç° {len(empty_rows)} è¡Œå­˜åœ¨ç©ºå•å…ƒæ ¼:")
        # åªè¾“å‡ºå‰10è¡Œï¼Œé¿å…è¾“å‡ºè¿‡å¤š
        for i, (row_num, cols) in enumerate(empty_rows[:10]):
            print(f"      è¡Œ {row_num}: ç©ºåˆ— - {', '.join(cols)}")
        if len(empty_rows) > 10:
            print(f"      ... è¿˜æœ‰ {len(empty_rows) - 10} è¡Œæœªæ˜¾ç¤º")
    else:
        print(f"\nâœ… æ‰€æœ‰å•å…ƒæ ¼å‡å·²å¡«å……ï¼Œæ— ç©ºå•å…ƒæ ¼ï¼")
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼å…±å¤„ç† {rows_to_process} è¡Œæ•°æ®ã€‚")
    print(f"ğŸ“ æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    print(f"{'='*60}")

# ======================  
# ä¸»ç¨‹åºå…¥å£
# ======================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨DeepSeek APIåˆ†æè®ºæ–‡")
    parser.add_argument("conference", type=str, help="ä¼šè®®åç§°ï¼Œå¦‚ICRA2024")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†å°‘é‡æ•°æ®")
    parser.add_argument("--test-size", type=int, default=10, help="æµ‹è¯•æ¨¡å¼å¤„ç†çš„æ•°æ®é‡")
    
    args = parser.parse_args()
    conference = args.conference
    test_mode = args.test
    test_size = args.test_size
    
    # APIå¯†é’¥åˆ—è¡¨
    API_KEYS = [
        "sk-8c38624bafb9477fb237ab2e58948c1b",
        "sk-4d5cc37a8b17417c87ed33b94d7b06a7",
        "sk-ab036985ba2c452f8262f4922e8ab50c",
        "sk-989bea94ce2b47e59714145005afc87e",
        "sk-29bc65dfb2de4ed3a983741f815ad2d7",
        "sk-064831a959c84353b6c4979fa90d16f3",
        "sk-82923973f2ff4ef895824595474f0df6",
        "sk-f0556cdf91f74729b7615d46fad4091c",
        "sk-6019eb2fda48482aa2218d3283f8332d",
        "sk-c19ad12b9f4b4b078270651858bb7f68",
    ]

    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_dir = f"/home/cuhk/Documents/Test_lx/Find_Paper/{conference}"
    input_csv = f"{input_dir}/{conference}_Title_DOI_Abstract.csv"
    output_csv = f"{input_dir}/{conference}_Title_DOI_Abstract_æ ‡é¢˜_æ‘˜è¦_è§£æ.csv"

    print(f"ä¼šè®®: {conference}")
    print(f"è¾“å…¥æ–‡ä»¶: {input_csv}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_csv}")
    print(f"æµ‹è¯•æ¨¡å¼: {test_mode}")
    if test_mode:
        print(f"æµ‹è¯•æ•°æ®é‡: {test_size}")
    print()

    # è¿è¡Œæ‰¹é‡å¤„ç†
    batch_process_csv(input_csv, output_csv, API_KEYS, test_mode, test_size)
