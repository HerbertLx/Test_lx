import json
import os
import re
from typing import Dict, Any, List

import pandas as pd
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import itertools

# ===========================
# ğŸ¤– æç¤ºè¯ï¼ˆPromptï¼‰é…ç½®åŒº
# ===========================

JSON_OUTPUT_TEMPLATE = {
    "Title": "è®ºæ–‡åŸæ ‡é¢˜å ä½ç¬¦",
    "æ ‡é¢˜": "æ ‡é¢˜çš„ä¸­æ–‡ç¿»è¯‘",
    "DOI": "DOIå ä½ç¬¦",
    "Abstract": "æ‘˜è¦åŸæ–‡æœ¬å ä½ç¬¦",
    "æ‘˜è¦": "æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘",
    "å…³é”®è¯": "æå–çš„å…³é”®è¯ï¼Œç”¨åˆ†å·åˆ†éš”",
    "è®¾å¤‡": "è®¾å¤‡åˆ†ç±»ç»“æœï¼Œå¦‚ï¼šæœºæ¢°è‡‚/æ— äººæœº/æœºå™¨ç‹—/äººå½¢æœºå™¨äºº/æ— äººå°è½¦/å…¶ä»–"
}

SYSTEM_PROMPT = f'''ä½ æ˜¯ä¸€åæœºå™¨äººé¢†åŸŸçš„ç§‘ç ”åŠ©ç†ï¼Œå½“å‰ä»»åŠ¡æ˜¯æ ¹æ®è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æã€‚
è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. **ç¿»è¯‘**ï¼šå°†è‹±æ–‡æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡
2. **å…³é”®è¯æå–**ï¼šä»æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–5-8ä¸ªæ ¸å¿ƒå…³é”®è¯
3. **è®¾å¤‡åˆ†ç±»**ï¼šæ ¹æ®è®ºæ–‡å†…å®¹å¯¹æ¶‰åŠçš„è®¾å¤‡è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»é€‰é¡¹åŒ…æ‹¬ï¼š
   - æœºæ¢°è‡‚
   - æ— äººæœº
   - æœºå™¨ç‹—
   - äººå½¢æœºå™¨äºº
   - æ— äººå°è½¦
   - å…¶ä»–

è¯·ä¸¥æ ¼ä»…è¾“å‡º JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜æˆ– Markdown æ ¼å¼ã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ **JSON** æ ¼å¼è¾“å‡ºï¼š
{json.dumps(JSON_OUTPUT_TEMPLATE, ensure_ascii=False, indent=2)}
'''

# ===========================
# æ ¸å¿ƒå‡½æ•°ï¼šDeepSeek è®ºæ–‡åˆ†æï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ===========================

def deepseek_analyze_paper_json(title: str, doi: str, abstract: str, api_key: str) -> Dict[str, Any]:
    user_prompt = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\nDOIï¼š{doi}\næ‘˜è¦ï¼š{abstract}"
    
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={'type': 'json_object'},
            stream=False,
            temperature=0.0
        )
        raw_output = response.choices[0].message.content.strip()
        result = json.loads(raw_output)
        result['Title'] = title
        result['DOI'] = doi
        result['Abstract'] = abstract
        return result

    except Exception as e:
        error_msg = f"API æˆ–è§£æå¤±è´¥: {str(e)[:50]}"
        print(f"âŒ é”™è¯¯: {error_msg} (æ ‡é¢˜: {title[:30]}...)")
        return {
            "Title": title,
            "æ ‡é¢˜": "N/A",
            "DOI": doi,
            "Abstract": abstract,
            "æ‘˜è¦": "N/A",
            "å…³é”®è¯": "N/A",
            "è®¾å¤‡": "å…¶ä»–"
        }

# ===========================
# æ‰¹é‡å¤„ç† CSVï¼šè‡ªåŠ¨è¾“å‡ºè·¯å¾„ + æ–­ç‚¹ç»­ä¼  + å¹¶å‘
# ===========================

def batch_process_csv(input_path: str, output_path: str, api_keys: List[str]):
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)

    RESULT_COL_MAP = {
        "æ ‡é¢˜": "æ ‡é¢˜",
        "æ‘˜è¦": "æ‘˜è¦",
        "å…³é”®è¯": "å…³é”®è¯",
        "è®¾å¤‡": "è®¾å¤‡"
    }
    RESULT_COLS = list(RESULT_COL_MAP.values())

    # 1. è¯»å–åŸå§‹æ•°æ®
    df_original = pd.read_csv(input_path)
    if "Title" not in df_original.columns or "DOI" not in df_original.columns or "Abstract" not in df_original.columns:
        raise ValueError("è¾“å…¥ CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼šTitle, DOI, Abstract")

    # 2. å°è¯•åŠ è½½å·²æœ‰ç»“æœï¼ˆç”¨äºç»­ä¼ ï¼‰
    df_results = None
    if os.path.exists(output_path):
        print(f"âœ… æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå°è¯•è¯»å–å·²å¤„ç†ç»“æœï¼š{output_path}")
        df_results = pd.read_csv(output_path)
        # æ¸…ç†åˆ—åä¸­çš„ç©ºæ ¼
        df_results.columns = df_results.columns.str.strip()
        if "Title" not in df_results.columns:
            print("âš ï¸ è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ç¼ºå°‘ 'Title' åˆ—ï¼Œå°†å¿½ç•¥å·²å­˜åœ¨çš„ç»“æœã€‚")
            df_results = None

    # 3. åˆå§‹åŒ–æœ€ç»ˆ DataFrame
    df_final = df_original.copy()
    # ç¡®ä¿åˆ—åæ ¼å¼æ­£ç¡®
    desired_columns = ["Title", "æ ‡é¢˜", "DOI", "Abstract", "æ‘˜è¦", "å…³é”®è¯", "è®¾å¤‡"]
    for col in desired_columns:
        if col not in df_final.columns:
            df_final[col] = None

    # 4. åŒæ­¥å·²æœ‰ç»“æœ
    synced_count = 0
    if df_results is not None:
        # æ„å»ºå¤„ç†æ•°æ®çš„æ˜ å°„
        processed_data = {}
        for idx, row in df_results.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title:
                continue
            processed_data[title] = {
                "æ ‡é¢˜": row.get("æ ‡é¢˜", "N/A"),
                "æ‘˜è¦": row.get("æ‘˜è¦", "N/A"),
                "å…³é”®è¯": row.get("å…³é”®è¯", "N/A"),
                "è®¾å¤‡": row.get("è®¾å¤‡", "å…¶ä»–")
            }
        
        # åŒæ­¥æ•°æ®
        for idx, row in df_final.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title or title.lower() in ("nan", "none"):
                continue
            if title in processed_data:
                data = processed_data[title]
                device_value = data.get("è®¾å¤‡", "")
                is_filled = (
                    isinstance(device_value, str)
                    and device_value.strip().lower() not in ["", "none", "nan", "n/a"]
                    and pd.notna(device_value)
                )
                if is_filled:
                    df_final.loc[idx, "æ ‡é¢˜"] = data.get("æ ‡é¢˜", "N/A")
                    df_final.loc[idx, "æ‘˜è¦"] = data.get("æ‘˜è¦", "N/A")
                    df_final.loc[idx, "å…³é”®è¯"] = data.get("å…³é”®è¯", "N/A")
                    df_final.loc[idx, "è®¾å¤‡"] = data.get("è®¾å¤‡", "å…¶ä»–")
                    synced_count += 1
        print(f"   å·²åŒæ­¥ {synced_count} æ¡å·²å¤„ç†ç»“æœåˆ°å½“å‰æ‰¹æ¬¡ã€‚")

    # 5. æ”¶é›†å¾…å¤„ç†ä»»åŠ¡
    tasks_to_run = []
    total_rows = len(df_final)
    for idx, row in df_final.iterrows():
        title = str(row.get("Title", "")).strip()
        doi = str(row.get("DOI", "")).strip()
        abstract = str(row.get("Abstract", "")).strip()
        
        if not title or title.lower() in ("nan", "none"):
            continue

        device_value = row.get("è®¾å¤‡")
        is_processed = False
        if isinstance(device_value, str) and device_value.strip().lower() not in ["", "none", "nan", "n/a"]:
            is_processed = True
        elif pd.notna(device_value):
            is_processed = False

        if is_processed:
            print(f"â© è·³è¿‡å·²å¤„ç† [{idx+1}/{total_rows}]: {title[:50]}...")
            continue

        tasks_to_run.append({'index': idx, 'title': title, 'doi': doi, 'abstract': abstract})

    rows_to_process = len(tasks_to_run)
    print(f"\n--- å¾…å¤„ç†æ€»è¡Œæ•°: {total_rows} | æ–°ä»»åŠ¡æ•°: {rows_to_process} ---")

    if rows_to_process == 0:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€è¿è¡Œæ–°ä»»åŠ¡ã€‚")
        return

    # 6. å¹¶å‘å¤„ç†
    api_key_cycler = itertools.cycle(api_keys)
    max_workers = min(len(api_keys), 10)  # é˜²æ­¢çº¿ç¨‹è¿‡å¤šï¼ˆå¯è°ƒï¼‰
    results_list = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for task in tasks_to_run:
            idx = task['index']
            title = task['title']
            doi = task['doi']
            abstract = task['abstract']
            key = next(api_key_cycler)
            future = executor.submit(deepseek_analyze_paper_json, title, doi, abstract, key)
            futures[future] = idx

        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                results_list.append((idx, result))
                print(f"âœ” å®Œæˆ [{idx+1}/{total_rows}] | è®¾å¤‡: {result.get('è®¾å¤‡', 'N/A')} | æ ‡é¢˜: {result.get('Title', 'N/A')[:80]}...")
            except Exception as e:
                print(f"âŒ çº¿ç¨‹æ‰§è¡Œé”™è¯¯ (Index: {idx}): {e}")

    # 7. æ›´æ–°å¹¶ä¿å­˜
    for idx, result in results_list:
        for json_key, csv_col in RESULT_COL_MAP.items():
            df_final.at[idx, csv_col] = result.get(json_key, "N/A")

    df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼å…±å¤„ç† {rows_to_process} è¡Œæ–°æ•°æ®ã€‚")
    print(f"ğŸ“ æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    print(f"{'='*60}")

# ======================
# ä¸»ç¨‹åºå…¥å£
# ======================
if __name__ == "__main__":
    # APIå¯†é’¥åˆ—è¡¨
    API_KEYS = [
        "sk-8c38624bafb9477fb237ab2e58948c1b",
        "sk-4d5cc37a8b17417c87ed33b94d7b06a7",
        "sk-ab036985ba2c452f8262f4922e8ab50c",
        "sk-989bea94ce2b47e59714145005afc87e",
        "sk-29bc65dfb2de4ed3a983741f815ad2d7",
        "sk-064831a959c84353b6c4979fa90d16f3",
        "sk-82923973f2ff4ef895824595474f0df6",
        "sk-f0556cdf91f74729b7615d46fad4091c",
        "sk-6019eb2fda48482aa2218d3283f8332d",
        "sk-c19ad12b9f4b4b078270651858bb7f68",
    ]

    # è¾“å…¥è¾“å‡ºè·¯å¾„
    INPUT_CSV = r"/home/cuhk/Documents/Test_lx/Find_Paper/ICRA2024_Title_DOI_Abstract.csv"
    OUTPUT_CSV = r"/home/cuhk/Documents/Test_lx/Find_Paper/ICRA2024_Title_æ ‡é¢˜_DOI_Abstract_æ‘˜è¦_å…³é”®è¯.csv"

    batch_process_csv(INPUT_CSV, OUTPUT_CSV, API_KEYS)
