"""
ç¿»è¯‘ICRA2025è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦
"""
import json
import os
import time
import itertools
from typing import Dict, Any, List

import pandas as pd
from openai import OpenAI
from tqdm import tqdm

# ===========================
# ğŸ¤– æç¤ºè¯ï¼ˆPromptï¼‰é…ç½®åŒº
# ===========================

JSON_OUTPUT_TEMPLATE = {
    "Title": "è®ºæ–‡åŸæ ‡é¢˜å ä½ç¬¦",
    "æ ‡é¢˜": "æ ‡é¢˜çš„ä¸­æ–‡ç¿»è¯‘",
    "DOI": "DOIå ä½ç¬¦",
    "Abstract": "æ‘˜è¦åŸæ–‡æœ¬å ä½ç¬¦",
    "æ‘˜è¦": "æ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘"
}

SYSTEM_PROMPT = f'''ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œæ“…é•¿å°†è‹±æ–‡ç§‘æŠ€è®ºæ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚
è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. **ç¿»è¯‘**ï¼šå°†è‹±æ–‡æ ‡é¢˜å’Œæ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡
2. **ä¿æŒä¸“ä¸šæ€§**ï¼šç¿»è¯‘è¦å‡†ç¡®åæ˜ åŸæ–‡çš„ä¸“ä¸šæœ¯è¯­å’Œå†…å®¹
3. **æµç•…è‡ªç„¶**ï¼šç¿»è¯‘åçš„ä¸­æ–‡è¦é€šé¡ºæ˜“æ‡‚

è¯·ä¸¥æ ¼ä»…è¾“å‡º JSON æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜æˆ– Markdown æ ¼å¼ã€‚
ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ **JSON** æ ¼å¼è¾“å‡ºï¼š
{json.dumps(JSON_OUTPUT_TEMPLATE, ensure_ascii=False, indent=2)}
'''

# ===========================
# æ ¸å¿ƒå‡½æ•°ï¼šDeepSeek ç¿»è¯‘ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ===========================

def deepseek_translate_paper_json(title: str, doi: str, abstract: str, api_key: str) -> Dict[str, Any]:
    # å¤„ç†ç©ºæ‘˜è¦çš„æƒ…å†µ
    if not abstract or abstract.strip() in ["", "N/A", "nan", "None"]:
        return {
            "Title": title,
            "æ ‡é¢˜": "",
            "DOI": doi,
            "Abstract": abstract,
            "æ‘˜è¦": ""
        }
    
    user_prompt = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\nDOIï¼š{doi}\næ‘˜è¦ï¼š{abstract}"
    
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            response_format={'type': 'json_object'},
            stream=False,
            temperature=0.0
        )
        raw_output = response.choices[0].message.content.strip()
        result = json.loads(raw_output)
        result['Title'] = title
        result['DOI'] = doi
        result['Abstract'] = abstract
        return result

    except Exception as e:
        error_msg = f"API æˆ–è§£æå¤±è´¥: {str(e)[:50]}"
        print(f"âŒ é”™è¯¯: {error_msg} (æ ‡é¢˜: {title[:30]}...)")
        return {
            "Title": title,
            "æ ‡é¢˜": "",
            "DOI": doi,
            "Abstract": abstract,
            "æ‘˜è¦": ""
        }

# ===========================
# æ‰¹é‡å¤„ç† CSVï¼šè‡ªåŠ¨è¾“å‡ºè·¯å¾„ + æ–­ç‚¹ç»­ä¼ 
# ===========================

def batch_process_csv(input_path: str, output_path: str, api_keys: List[str]):
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)

    RESULT_COL_MAP = {
        "æ ‡é¢˜": "æ ‡é¢˜",
        "æ‘˜è¦": "æ‘˜è¦"
    }
    RESULT_COLS = list(RESULT_COL_MAP.values())

    # 1. è¯»å–åŸå§‹æ•°æ®
    # ä½¿ç”¨nameså‚æ•°æ¥æŒ‡å®šåˆ—åï¼Œå¿½ç•¥ç¬¬ä¸€è¡Œçš„åˆ—å
    df_original = pd.read_csv(input_path, names=['Title', 'DOI', 'Abstract'], skiprows=1)
    
    # å¤„ç†å¯èƒ½çš„ç©ºå€¼
    df_original = df_original.fillna('')

    # 2. å°è¯•åŠ è½½å·²æœ‰ç»“æœï¼ˆç”¨äºç»­ä¼ ï¼‰
    df_results = None
    if os.path.exists(output_path):
        print(f"âœ… æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºæ–‡ä»¶ï¼Œå°è¯•è¯»å–å·²å¤„ç†ç»“æœï¼š{output_path}")
        df_results = pd.read_csv(output_path)
        # æ¸…ç†åˆ—åä¸­çš„ç©ºæ ¼
        df_results.columns = df_results.columns.str.strip()
        if "Title" not in df_results.columns:
            print("âš ï¸ è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ç¼ºå°‘ 'Title' åˆ—ï¼Œå°†å¿½ç•¥å·²å­˜åœ¨çš„ç»“æœã€‚")
            df_results = None

    # 3. åˆå§‹åŒ–æœ€ç»ˆ DataFrame
    df_final = df_original.copy()
    # ç¡®ä¿åˆ—åæ ¼å¼æ­£ç¡®
    desired_columns = ["Title", "æ ‡é¢˜", "DOI", "Abstract", "æ‘˜è¦"]
    for col in desired_columns:
        if col not in df_final.columns:
            df_final[col] = None

    # 4. åŒæ­¥å·²æœ‰ç»“æœ
    synced_count = 0
    if df_results is not None:
        # æ„å»ºå¤„ç†æ•°æ®çš„æ˜ å°„
        processed_data = {}
        for idx, row in df_results.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title:
                continue
            processed_data[title] = {
                "æ ‡é¢˜": row.get("æ ‡é¢˜", ""),
                "æ‘˜è¦": row.get("æ‘˜è¦", "")
            }
        
        # åŒæ­¥æ•°æ®
        for idx, row in df_final.iterrows():
            title = str(row.get("Title", "")).strip()
            if not title or title.lower() in ("nan", "none"):
                continue
            if title in processed_data:
                data = processed_data[title]
                df_final.loc[idx, "æ ‡é¢˜"] = data.get("æ ‡é¢˜", "")
                df_final.loc[idx, "æ‘˜è¦"] = data.get("æ‘˜è¦", "")
                synced_count += 1
        print(f"   å·²åŒæ­¥ {synced_count} æ¡å·²å¤„ç†ç»“æœåˆ°å½“å‰æ‰¹æ¬¡ã€‚")

    # 5. æ”¶é›†å¾…å¤„ç†ä»»åŠ¡
    tasks_to_run = []
    total_rows = len(df_final)
    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰10æ¡
    max_tests = 10
    test_count = 0
    
    for idx, row in df_final.iterrows():
        title = str(row.get("Title", "")).strip()
        doi = str(row.get("DOI", "")).strip()
        abstract = str(row.get("Abstract", "")).strip()
        
        if not title or title.lower() in ("nan", "none"):
            continue

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        title_translated = row.get("æ ‡é¢˜")
        abstract_translated = row.get("æ‘˜è¦")
        is_processed = False
        if isinstance(title_translated, str) and title_translated.strip():
            is_processed = True

        if is_processed:
            print(f"â© è·³è¿‡å·²å¤„ç† [{idx+1}/{total_rows}]: {title[:50]}...")
            continue

        # æµ‹è¯•æ¨¡å¼ï¼šåªæ·»åŠ å‰10ä¸ªæœªå¤„ç†çš„
        if test_count < max_tests:
            tasks_to_run.append({'index': idx, 'title': title, 'doi': doi, 'abstract': abstract})
            test_count += 1
        else:
            break

    rows_to_process = len(tasks_to_run)
    print(f"\n--- å¾…å¤„ç†æ€»è¡Œæ•°: {total_rows} | æ–°ä»»åŠ¡æ•°: {rows_to_process} (æµ‹è¯•æ¨¡å¼) ---")

    if rows_to_process == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•æ•°æ®å‡å·²å¤„ç†å®Œæˆï¼Œæ— éœ€è¿è¡Œæ–°ä»»åŠ¡ã€‚")
        return

    # 6. å¤„ç†ä»»åŠ¡ï¼ˆå¸¦è¿›åº¦æ¡å’Œæ—¶é—´ä¼°è®¡ï¼‰
    api_key_cycler = itertools.cycle(api_keys)
    results_list = []
    total_time = 0

    # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
    with tqdm(total=rows_to_process, desc="ç¿»è¯‘è®ºæ–‡", unit="ç¯‡") as pbar:
        for i, task in enumerate(tasks_to_run):
            start_time = time.time()
            idx = task['index']
            title = task['title']
            doi = task['doi']
            abstract = task['abstract']
            
            # è·å–APIå¯†é’¥
            api_key = next(api_key_cycler)
            
            # ç¿»è¯‘è®ºæ–‡
            result = deepseek_translate_paper_json(title, doi, abstract, api_key)
            results_list.append((idx, result))
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            total_time += processing_time
            
            # ä¼°è®¡å‰©ä½™æ—¶é—´
            avg_time_per_paper = total_time / (i + 1)
            remaining_papers = rows_to_process - (i + 1)
            estimated_remaining_time = avg_time_per_paper * remaining_papers
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix_str(f"å‰©ä½™æ—¶é—´: {estimated_remaining_time:.2f}ç§’")
            pbar.update(1)
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            print(f"âœ” å®Œæˆ [{idx+1}/{total_rows}] | æ ‡é¢˜: {result.get('Title', 'N/A')[:80]}...")

    # 7. æ›´æ–°å¹¶ä¿å­˜
    for idx, result in results_list:
        for json_key, csv_col in RESULT_COL_MAP.items():
            df_final.at[idx, csv_col] = result.get(json_key, "")

    df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\n{'='*60}")
    print(f"ğŸ‰ æµ‹è¯•å®Œæˆï¼å…±å¤„ç† {rows_to_process} æ¡æ•°æ®ã€‚")
    print(f"ğŸ“ æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    print(f"{'='*60}")
    print("\nè¦è¿è¡Œå®Œæ•´ç‰ˆæœ¬ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„æµ‹è¯•æ¨¡å¼é™åˆ¶")

# ======================
# ä¸»ç¨‹åºå…¥å£
# ======================
if __name__ == "__main__":
    # APIå¯†é’¥åˆ—è¡¨
    API_KEYS = [
        "sk-8c38624bafb9477fb237ab2e58948c1b",
        "sk-4d5cc37a8b17417c87ed33b94d7b06a7",
        "sk-ab036985ba2c452f8262f4922e8ab50c",
        "sk-989bea94ce2b47e59714145005afc87e",
        "sk-29bc65dfb2de4ed3a983741f815ad2d7",
        "sk-064831a959c84353b6c4979fa90d16f3",
        "sk-82923973f2ff4ef895824595474f0df6",
        "sk-f0556cdf91f74729b7615d46fad4091c",
        "sk-6019eb2fda48482aa2218d3283f8332d",
        "sk-c19ad12b9f4b4b078270651858bb7f68",
    ]

    # è¾“å…¥è¾“å‡ºè·¯å¾„
    INPUT_CSV = r"/home/cuhk/Documents/Test_lx/Find_Paper/ICRA2025/ICRA2025_Title_DOI_Abstract.csv"
    OUTPUT_CSV = r"/home/cuhk/Documents/Test_lx/Find_Paper/ICRA2025/ICRA2025_Title_DOI_Abstract_æ ‡é¢˜_æ‘˜è¦.csv"

    batch_process_csv(INPUT_CSV, OUTPUT_CSV, API_KEYS)