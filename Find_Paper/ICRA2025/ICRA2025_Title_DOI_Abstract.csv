Title,DOI
Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong Multi-Agent Path Finding.,https://doi.org/10.1109/ICRA55743.2025.11127445,"Lifelong Multi-Agent Path Finding (LMAPF) repeatedly finds collision-free paths for multiple agents that are continually assigned new goals when they reach current ones. Recently, this field has embraced learning-based methods, which reactively generate single-step actions based on individual local observations. However, it is still challenging for them to match the performance of the best search-based algorithms, especially in large-scale settings. This work proposes an imitation-learning-based LMAPF solver that introduces a novel communication module as well as systematic single-step collision resolution and global guidance techniques. Our proposed solver, Scalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning speed of learning-based methods and the high solution quality of search-based methods with the help of modern GPUs. Across six large-scale maps with up to <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{1 0, 0 0 0}$</tex> agents and varying obstacle structures, SILLM surpasses the best learning- and search-based baselines, achieving average throughput improvements of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{137.7 \%}$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{1 6. 0 \%}$</tex>, respectively. Furthermore, SILLM also beats the winning solution of the 2023 League of Robot Runners, an international LMAPF competition. Finally, we validated SILLM with 10 real robots and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{1 0 0}$</tex> virtual robots in a mock warehouse environment."
Computationally and Sample Efficient Safe Reinforcement Learning Using Adaptive Conformal Prediction.,https://doi.org/10.1109/ICRA55743.2025.11127570,"Safety is a critical concern in learning-enabled autonomous systems especially when deploying these systems in real-world scenarios. An important challenge is accurately quantifying the uncertainty of unknown models to generate provably safe control policies that facilitate the gathering of informative data, thereby achieving both safe and optimal policies. Additionally, the selection of the data-driven model can significantly impact both the real-time implementation and the uncertainty quantification process. In this paper, we propose a provably sample efficient episodic safe learning framework that remains robust across various model choices with quantified uncertainty for online control tasks. Specifically, we first employ Quadrature Fourier Features (QFF) for kernel function approximation of Gaussian Processes (GPs) to enable efficient approximation of unknown dynamics. Then the Adaptive Conformal Prediction (ACP) is used to quantify the uncertainty from online observations and combined with the Control Barrier Functions (CBF) to characterize the uncertainty-aware safe control constraints under learned dynamics. Finally, an optimism-based exploration strategy is integrated with ACP-based CBFs for safe exploration and near-optimal safe nonlinear control. Theoretical proofs and simulation results are provided to demonstrate the effectiveness and efficiency of the proposed framework."
A Magnetic-Actuated Vision-Based Whisker Array for Contact Perception and Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128051,"Tactile sensing and the manipulation of delicate objects are critical challenges in robotics. This study presents a vision-based magnetic-actuated whisker array sensor that integrates these functions. The sensor features eight whiskers arranged circularly, supported by an elastomer membrane and actuated by electromagnets and permanent magnets. A camera tracks whisker movements, enabling high-resolution tactile feedback. The sensor's performance was evaluated through object classification and grasping experiments. In the classification experiment, the sensor approached objects from four directions and accurately identified five distinct objects with a classification accuracy of 99.17% using a Multi-Layer Perceptron model. In the grasping experiment, the sensor tested configurations of eight, four, and two whiskers, achieving the highest success rate of 87% with eight whiskers. These results highlight the sensor's potential for precise tactile sensing and reliable manipulation."
SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding.,https://doi.org/10.1109/ICRA55743.2025.11127434,"The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learningbased approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments."
Open-RGBT: Open-Vocabulary RGB-T Zero-Shot Semantic Segmentation in Open-World Environments.,https://doi.org/10.1109/ICRA55743.2025.11127432,"Semantic segmentation is a critical technique for effective scene understanding. Traditional RGB-T semantic segmentation models often struggle to generalize across diverse scenarios due to their reliance on pretrained models and predefined categories. Recent advancements in Visual Language Models (VLMs) have facilitated a shift from closedset to open-vocabulary semantic segmentation methods. However, these models face challenges in dealing with intricate scenes, primarily due to the heterogeneity between RGB and thermal modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary RGB-T semantic segmentation model. Specifically, we obtain instance-level detection proposals by incorporating visual prompts to enhance category understanding. Additionally, we employ the CLIP model to assess image-text similarity, which helps correct semantic consistency and mitigates ambiguities in category identification. Empirical evaluations demonstrate that Open-RGBT achieves superior performance in diverse and challenging real-world scenarios, even in the wild, significantly advancing the field of RGB-T semantic segmentation. The project page of Open-RGBT is available at https://OpenRGBT.github.io/."
TDFANet: Encoding Sequential 4D Radar Point Clouds Using Trajectory-Guided Deformable Feature Aggregation for Place Recognition.,https://doi.org/10.1109/ICRA55743.2025.11127648,"Place recognition is essential for achieving closedloop or global positioning in autonomous vehicles and mobile robots. Despite recent advancements in place recognition using 2D cameras or 3D LiDAR, it remains to be seen how to use 4D radar for place recognition - an increasingly popular sensor for its robustness against adverse weather and lighting conditions. Compared to LiDAR point clouds, radar data are drastically sparser, noisier and in much lower resolution, which hampers their ability to effectively represent scenes, posing significant challenges for 4D radar-based place recognition. This work addresses these challenges by leveraging multimodal information from sequential 4D radar scans and effectively extracting and aggregating spatio-temporal features. Our approach follows a principled pipeline that comprises (1) dynamic points removal and ego-velocity estimation from velocity property, (2) bird's eye view (BEV) feature encoding on the refined point cloud, (3) feature alignment using BEV feature map motion trajectory calculated by ego-velocity, (4) multiscale spatio-temporal features of the aligned BEV feature maps are extracted and aggregated. Real-world experimental results validate the feasibility of the proposed method and demonstrate its robustness in handling dynamic environments. Source codes are available."
Learning-Based Adaptive Navigation for Scalar Field Mapping and Feature Tracking.,https://doi.org/10.1109/ICRA55743.2025.11127650,"Scalar field features such as extrema, contours, and saddle points are essential for applications in environmental monitoring, search and rescue, and resource exploration. Traditional navigation methods often rely on predefined trajectories, leading to inefficient and resource-intensive mapping. This paper introduces a new adaptive navigation framework that leverages learning techniques to enhance exploration efficiency and effectiveness in scalar fields, even under noisy data and obstacles. The framework employs Partial Differential Equations to model scalar fields and a Gaussian Process Regressor to estimate the fields and their gradients, enabling real-time path adjustments and obstacle avoidance. We provide a theoretical foundation for the approach and address several limitations found in existing methods. The effectiveness of our framework is demonstrated through simulation benchmarks and field experiments with an Autonomous Surface Vehicle, showing improved efficiency and adaptability compared to traditional methods and offering a robust solution for real-time environmental monitoring."
DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning for Underwater Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128041,"This paper presents a learned model to predict the robot-centric velocity of an underwater robot through dynamics-aware proprioception. The method exploits a recurrent neural network using as inputs inertial cues, motor commands, and battery voltage readings alongside the hidden state of the previous time-step to output robust velocity estimates and their associated uncertainty. An ensemble of networks is utilized to enhance the velocity and uncertainty predictions. Fusing the network's outputs into an Extended Kalman Filter, alongside inertial predictions and barometer updates, the method enables long-term underwater odometry without further exteroception. Furthermore, when integrated into visual-inertial odometry, the method assists in enhanced estimation resilience when dealing with an order of magnitude fewer total features tracked (as few as 1) as compared to conventional visual-inertial systems. Tested onboard an underwater robot deployed both in a laboratory pool and the Trondheim Fjord, the method takes less than 5 ms for inference either on the CPU or the GPU of an NVIDIA Orin AGX and demonstrates less than 4% relative position error in novel trajectories during complete visual blackout, and approximately 2% relative error when a maximum of 2 visual features from a monocular camera are available."
Detecting Perception-Based Attacks using Visual Odometry: Inconsistency Modeling and Checking on Robotic States.,https://doi.org/10.1109/ICRA55743.2025.11127598,"Perception systems in robotic vehicles are crucial for safe and efficient operation, providing key state estimates necessary for planning and control. However, these systems are increasingly vulnerable to perception-based attacks, such as odometry spoofing, position spoofing, obstacle hiding, and object misclassification, which can lead to catastrophic failures. In this paper, we propose a novel approach to detect perception-based attacks by modeling inconsistencies between the physical and estimated states of the robot. Our approach offers a unified methodology for detecting different types of attacks with high accuracy and minimal computational overhead. We validate our method through extensive simulations and real-world scenarios, achieving a 99.5% success rate in detecting attacks, while maintaining a low latency (within 100ms)."
KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems.,https://doi.org/10.1109/ICRA55743.2025.11128047,"Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates longterm and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. Karma distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dualmemory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1.3 \times$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$2.3 \times$</tex> in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$3.4 \times$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$62.7 \times$</tex>. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms. Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. Our code is available at https://github.com/WZX0Swarm0Robotics/KARMA/tree/master."
MJPR: Multi-Modal Joint Predictive Representation in Deep Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128137,"Multi-modal reinforcement learning (RL) has been brought into focus due to its ability to provide complementary information from different sensors, enriching observations of agents. However, the introduction of multi-modal highdimensional observations brings challenges to sample efficiency. There is a lack of research on how to efficiently obtain multi-modal latent states while encouraging them to generate complementary information. To address this, we propose a representation learning method, Multi-modal Joint Predictive Representation (MJPR), which utilizes multi-modal interactive information to predict future latent states. The joint prediction method achieves the representation training for modalities and promotes each modality to generate complementary information related to predictions of each other. In addition, we introduce multi-modal loss balancing to prompt training equilibrium and cross-modal contrastive learning (CMCL) to align the modalities for effective modal interaction. We establish the multi-modal environments in the Deepmind Control suite (DMC) and Webots and compare our method with current RL representation methods. Experimental results show that MJPR outperforms state-of-the-art methods by an average of 12.0% on six subtasks in DMC environments. It outperforms advanced methods by 16.7% and 55.4% in simple tasks and complex tasks of Webots environment, respectively. Moreover, ablation experiments are established in the DMC environment to verify the importance of each module to MJPR."
Real-World Automated Vehicle Longitudinal Stability Analysis: Controller Design and Field Test.,https://doi.org/10.1109/ICRA55743.2025.11127447,"Although extensive research has been conducted on modeling the stable longitudinal controller of automated vehicles (AVs) to dampen traffic oscillations, the real-world performance of these controllers in actual vehicles remains uncertain. In the operation of real-world AVs, the delay between actual dynamics and the commands prevents the controller's command from being effectively implemented to dampen traffic oscillations. Thus, this study adapts the designed controllers within an AV test platform to compare the theoretically stable conditions with the actual oscillation dampening performance. Initially, we compute the stable conditions for both the traditional car-following controller, which assumes no delay, and the longitudinal controller that accounts for the dynamic response of the vehicle. Through empirical experiments, we demonstrate that the longitudinal controller predicts vehicle stability more accurately than conventional car-following controller, showing an improvement from an average prediction accuracy rate of 0.59 to 0.91. Also, the experiments uncover specific delays inherent in dynamics systems, with a response delay of 0.34 seconds. Our work makes two principal contributions to the field of AV control systems. First, it empirically validates that the longitudinal model, which accounts for the vehicle's dynamic responses, offers a more precise representation of vehicular behavior. Second, the relatively brief response delay identified expands the stability region, thereby enhancing vehicle control and safety. The longitudinal controller is critical for enhancing AV performance and reliability in dampening traffic oscillations."
MFSeg: Efficient Multi-Frame 3D Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127629,"We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency."
Verifiably Following Complex Robot Instructions with Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11127418,"When instructing robots, users want to flexibly express constraints, refer to arbitrary landmarks, and verify robot behavior, while robots must disambiguate instructions into specifications and ground instruction referents in the real world. To address this problem, we propose Language Instruction grounding for Motion Planning (LIMP), an approach that enables robots to verifiably follow complex, open-ended instructions in real-world environments without prebuilt semantic maps. LIMP constructs a symbolic instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of correct-by-construction robot behaviors. We conduct a large-scale evaluation of LIMP on 150 instructions across five real-world environments, demonstrating its versatility and ease of deployment in diverse, unstructured domains. LIMP performs comparably to state-of-the-art baselines on standard open-vocabulary tasks and additionally achieves a 79% success rate on complex spatiotemporal instructions, significantly outperforming baselines that only reach 38%. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>See supplementary materials and demo videos at robotlimp.github.io"
Tracking Everything in Robotic-Assisted Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128141,"Accurate tracking of tissues and instruments in videos is crucial for Robotic-Assisted Minimally Invasive Surgery (RAMIS), as it enables the robot to comprehend the surgical scene with precise locations and interactions of tissues and tools. Traditional keypoint-based sparse tracking is limited by featured points, while flow-based dense two-view matching suffers from long-term drifts. Recently, the Tracking Any Point (TAP) algorithm was proposed to overcome these limitations and achieve dense accurate long-term tracking. However, its efficacy in surgical scenarios remains untested, largely due to the lack of a comprehensive surgical tracking dataset for evaluation. To address this gap, we introduce a new annotated surgical tracking dataset for benchmarking tracking methods for surgical scenarios, comprising real-world surgical videos with complex tissue and instrument motions. We extensively evaluate state-of-the-art (SOTA) TAP-based algorithms on this dataset and reveal their limitations in challenging surgical scenarios, including fast instrument motion, severe occlusions, and motion blur, etc. Furthermore, we propose a new tracking method, namely SurgMotion, to solve the challenges and further improve the tracking performance. Our proposed method outperforms most TAP-based algorithms in surgical instruments tracking, and especially demonstrates significant improvements over baselines in challenging medical videos. Our code and dataset are available at https://github.com/zhanbh1019/SurgicalMotion."
A Neural Network-Based Framework for Fast and Smooth Posture Reconstruction of a Soft Continuum Arm.,https://doi.org/10.1109/ICRA55743.2025.11128142,"A neural network-based framework is developed and experimentally demonstrated for the problem of estimating the shape of a soft continuum arm (SCA) from noisy measurements of the pose at a finite number of locations along the length of the arm. The neural network takes as input these measurements and produces as output a finitedimensional approximation of the strain, which is further used to reconstruct the infinite-dimensional smooth posture. This problem is important for various soft robotic applications. It is challenging due to the flexible aspects that lead to the infinitedimensional reconstruction problem for the continuous posture and strains. Because of this, past solutions to this problem are computationally intensive. The proposed fast smooth reconstruction method is shown to be five orders of magnitude faster while having comparable accuracy. The framework is evaluated on two testbeds: a simulated octopus muscular arm and a physical BR2 pneumatic soft manipulator."
Optimize and Coordinate Multiple DMPs Under Constraints to Achieve a Collaborative Manipulation Task.,https://doi.org/10.1109/ICRA55743.2025.11127511,"This paper addresses a significant challenge in achieving collaborative tasks; how can a robot or multiple robots, endowed with a library of pre-learned primitive movements, generate multiple simultaneous coordinated robotic movements, adapting and optimizing those in the library, to complete one collaborative task? This work can thus be seen as a follow-up to the work with a motion presented as dynamic movement primitive (DMP) that now considers collaborative tasks and the existence of multiple robots/manipulators. Specifically, we start with a simple task using one DMP and extend it to accommodate the coordinated execution of multiple DMPs in robots with multiple manipulators or-alternatively-multiple robots with a single manipulator. We investigate mechanisms to jointly optimize multiple DMPs to perform one task in a coordinated fashion. The joint trajectory is built from initial DMPs learned for a single manipulator, and its optimization must comply with task-specific constraints. We illustrate the application of our approach both in a simulated environment and in a simulated and real Baxter robot."
Air-FAR: Fast and Adaptable Routing for Aerial Navigation in Large-Scale Complex Unknown Environments.,https://doi.org/10.1109/ICRA55743.2025.11127430,"This paper presents a novel approach for realtime 3D navigation in large-scale complex environments by introducing a hierarchical 3D visibility graph (V-graph) and an efficient path search method. The proposed algorithm addresses the computational challenges of V-graph construction and shortest path search on the graph simultaneously. By introducing hierarchical 3D V-graph construction with heuristic visibility update, the 3D V-graph is constructed in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$O\left(K \cdot n^{2} \log n\right)$</tex> time, which guarantees real-time performance. The proposed iterative divide-and-conquer path search method can achieve near-optimal path solutions within the constraints of realtime operations. The algorithm ensures efficient 3D V-graph construction and path search. Extensive simulated and realworld environments validated that our algorithm reduces the travel time by 42%, achieves up to 24.8% higher trajectory efficiency, and runs faster than most benchmarks by orders of magnitude in complex environments. The code and developed simulator have been open-sourced to facilitate future research."
A Quantum Annealing Approach to Target Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128357,"This paper delves into the fusion of quantum computing and robotics, focusing on motion planning in cluttered environments. Traditional algorithms struggle with complex problems where many constraints need to be satisfied. Hence, optimization-based approaches such as Constrained Quadratic Models (CQM) have become increasingly popular. Our work presents a 3D tracking algorithm based on CQM uniquely adapted for quantum computers to address computational challenges. With their parallel processing capabilities, Quantum computers offer a groundbreaking approach to optimizing complex problems. We formulate the CQM problem for efficient resolution on the D-Wave quantum computer, showcasing its superiority over classical counterparts. Our application centers on real-time planning in a target-chaser tracking scenario, highlighting the quantum advantage in handling the computation complexity of constrained problems. This paper bridges the quantum-robotics gap and sets the stage for future research in quantum-enhanced robotic motion planning."
Kineto-Dynamical Planning and Accurate Execution of Minimum-Time Maneuvers on Three-Dimensional Circuits.,https://doi.org/10.1109/ICRA55743.2025.11127446,"Online planning and execution of minimum-time maneuvers on three-dimensional (3D) circuits is an open challenge in autonomous vehicle racing. In this paper, we present an artificial race driver (ARD) to learn the vehicle dynamics, plan and execute minimum-time maneuvers on a 3D track. ARD integrates a novel kineto-dynamical (KD) vehicle model for trajectory planning with economic nonlinear model predictive control (E-NMPC). We use a high-fidelity vehicle simulator (VS) to compare the closed-loop ARD results with a minimum-lap-time optimal control problem (MLT-VS), solved offline with the same VS. Our ARD sets lap times close to the MLT-VS, and the new KD model outperforms a literature benchmark. Finally, we study the vehicle trajectories, to assess the re-planning capabilities of ARD under execution errors. A video with the main results is available as supplementary material."
TS-DETR: Traffic Sign Detection Based on Positive and Negative Sample Augmentation.,https://doi.org/10.1109/ICRA55743.2025.11127628,"Traffic sign detection plays an essential role in advanced driver assistance system (ADAS) or self-driving vehicles. Typically, deep neural networks are employed to analyze road scene images captured by an onboard camera. However, due to the significant variation in appearance of different traffic signs, the classification of high similarity patterns is still a challenging task. To address these issues, this paper presents an end-to-end traffic sign detection framework based on DETR. The proposed network incorporates data augmentation and negative sample learning to mitigate the problem of data imbalance and enhance the model recognition capability effectively. An UASPP module (Upsample Atrous Pyramid Pooling) is introduced to integrate multi-scale features and global information. In the experiments, the performance evaluation has demonstrated the improvement of mAP by 3.9% on TT100K and 36.3% on GTSDB compared to state-of-the-art methods. The code and datasets are available at https://github.com/chinglun/TS-DETR."
Towards Transparent Multi-Agent Autonomous Systems Through Principled Multi-Source Knowledge Distillation.,https://doi.org/10.1109/ICRA55743.2025.11127443,"Many real-world robotic applications can be formulated as Multi-Agent Path-Finding (MAPF) problems and approximated using Multi-Agent Reinforcement Learning (MARL) algorithms. However, the opaque nature of the blackbox neural network models employed by MARL algorithms has impeded their widespread adoption due to concerns over interpretability, debugging, and user trust. To address these limitations, we propose an interpretable MAPF framework that emulates a group of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$n$</tex> path-finding agents optimized through reinforcement learning (RL) using behavior trees (BTs), where <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$n$</tex> is the number of agents in path-finding scenarios. Expert behavior datasets consisting of state-action trajectories from MARL algorithms are generated, and a knowledge distillation approach is employed to reduce the size of the datasets and extract implicit rules. Additionally, a principled rules factorization technique based on Boolean algebra theory is utilized to prune the behavior rules and create more compact BTs representations. The proposed framework is evaluated on randomly generated MAPF scenarios and demonstrates superior performance compared to conventional BTs generation methods. This paper advances the field of interpretable AI by enabling the extraction of understandable decision-making processes from complex reinforcement learning models in multiagent systems."
ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127452,"Incorporating language comprehension into robotic operations unlocks significant advancements in robotics, but also presents distinct challenges, particularly in executing spatially oriented tasks like pattern formation. This paper introduces ZeroCAP, a novel system that integrates large language models with multi-robot systems for zero-shot context aware pattern formation. Grounded in the principles of language-conditioned robotics, ZeroCAP leverages the interpretative power of language models to translate natural language instructions into actionable robotic configurations. This approach combines the synergy of vision-language models, cutting-edge segmentation techniques and shape descriptors, enabling the realization of complex, context-driven pattern formations in the realm of multi robot coordination. Through extensive experiments, we demonstrate the systems proficiency in executing complex context aware pattern formations across a spectrum of tasks, from surrounding and caging objects to infilling regions. This not only validates the system's capability to interpret and implement intricate context-driven tasks but also underscores its adaptability and effectiveness across varied environments and scenarios. The experimental videos and additional information about this work can be found at https://sites.google.com/view/zerocap/home."
DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems.,https://doi.org/10.1109/ICRA55743.2025.11127510,"Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple agents to work together in mapping unknown environments while simultaneously estimating their own positions. This approach enhances robustness, scalability, and accuracy by sharing information between agents, reducing drift, and enabling collective exploration of larger areas. In this paper, we present Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source decentralized monocular C-SLAM system. By only utilizing low-cost and light-weight monocular vision sensors, our system is well suited for small robots and micro aerial vehicles (MAVs). DVMSLAM's real-world applicability is validated on physical robots with a custom collision avoidance framework, showcasing its potential in real-time multi-agent autonomous navigation scenarios. We also demonstrate comparable accuracy to state-of-the-art centralized monocular C-SLAM systems. We opensource our code and provide supplementary material online <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">${ }^{1}$</tex>."
Qdgset: a Large Scale Grasping Dataset Generated With Quality-Diversity.,https://doi.org/10.1109/ICRA55743.2025.11127427,"Recent advances in AI have led to significant results in robotic learning, but skills like grasping remain partially solved. Many recent works exploit synthetic grasping datasets to learn to grasp unknown objects. However, those datasets were generated using simple grasp sampling methods using priors. Recently, Quality-Diversity (QD) algorithms have been proven to make grasp sampling significantly more efficient. In this work, we extend QDG-6DoF, a QD framework for generating object-centric grasps, to scale up the production of synthetic grasping datasets. We propose a data augmentation method that combines the transformation of object meshes with transfer learning from previous grasping repertoires. The conducted experiments show that this approach reduces the number of required evaluations per discovered robust grasp by up to 20 %. We used this approach to generate QDGset, a dataset of 6 DoF grasp poses that contains about 3.5 and 4.5 times more grasps and objects, respectively, than the previous state-of-the-art. Our method allows anyone to easily generate data, eventually contributing to a large-scale collaborative dataset of synthetic grasps."
QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds.,https://doi.org/10.1109/ICRA55743.2025.11128143,"Grasping has been a long-standing challenge in facilitating the final interface between a robot and the environment. As environments and tasks become complicated, the need to embed higher intelligence to infer from the surroundings and act on them has become necessary. Although most methods utilize techniques to estimate grasp pose by treating the problem via pure sampling-based approaches in the six-degree-of-freedom space or as a learning problem, they usually fail in real-life settings owing to poor generalization across domains. In addition, the time taken to generate the grasp plan and the lack of repeatability, owing to sampling inefficiency and the probabilistic nature of existing grasp planning approaches, severely limits their application in real-world tasks. This paper presents a lightweight analytical approach towards robotic grasp planning, particularly antipodal grasps, with little to no sampling in the six-degree-of-freedom space. The proposed grasp planning algorithm is formulated as an optimization problem towards estimating grasp points on the object surface instead of directly estimating the endeffector pose. To this extent, a soft-region-growing algorithm is presented for effective plane segmentation, even in the case of curved surfaces. An optimization-based quality metric is then used for the evaluation of grasp points to ensure indirect force closure. The proposed grasp framework is compared with the existing state-of-the-art grasp planning approach, Grasp pose detection (GPD), as a baseline over multiple simulated objects. The effectiveness of the proposed approach in comparison to GPD is also evaluated in a real-world setting using image and point-cloud data, with the planned grasps being executed using a ROBOTIQ gripper and UR5 manipulator. The proposed approach shows better performance in terms of higher probability for force closure with complete repeatability."
Stonefish: Supporting Machine Learning Research in Marine Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127421,"Simulations are highly valuable in marine robotics, offering a cost-effective and controlled environment for testing in the challenging conditions of underwater and surface operations. Given the high costs and logistical difficulties of real-world trials, simulators capable of capturing the operational conditions of subsea environments have become key in developing and refining algorithms for remotely-operated and autonomous underwater vehicles. This paper highlights recent enhancements to the Stonefish simulator, an advanced open-source platform supporting development and testing of marine robotics solutions. Key updates include a suite of additional sensors, such as an event-based camera, a thermal camera, and an optical flow camera, as well as, visual light communication, support for tethered operations, improved thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy. These developments and an automated annotation tool significantly bolster Stonefish's role in marine robotics research, especially in the field of machine learning, where training data with a known ground truth is hard or impossible to collect. https://github.com/patrykcieslak/stonefish"
BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using Bilevel Optimization.,https://doi.org/10.1109/ICRA55743.2025.11127930,"Robotic dexterous grasping is important for interacting with the environment. To unleash the potential of data-driven models for dexterous grasping, a large-scale, highquality dataset is essential. While gradient-based optimization offers a promising way for constructing such datasets, previous works suffer from limitations, such as inefficiency, strong assumptions in the grasp quality energy, or limited object sets for experiments. Moreover, the lack of a standard benchmark for comparing different methods and datasets hinders progress in this field. To address these challenges, we develop a highly efficient synthesis system and a comprehensive benchmark with MuJoCo for dexterous grasping. We formulate grasp synthesis as a bilevel optimization problem, combining a novel lowerlevel quadratic programming (QP) with an upper-level gradient descent process. By leveraging recent advances in CUDAaccelerated robotic libraries and GPU-based QP solvers, our system can parallelize thousands of grasps and synthesize over 49 grasps per second on a single 3090 GPU. Our synthesized grasps for Shadow, Allegro, and Leap hands all achieve a success rate above 75 % in simulation, with a penetration depth under 1 mm, outperforming existing baselines on nearly all metrics. Compared to the previous large-scale dataset, DexGraspNet, our dataset significantly improves the performance of learning models, with a success rate from around 40 % to 80 % in simulation. Real-world testing of the trained model on the Shadow Hand achieves an 81 % success rate across 20 diverse objects. The codes and datasets are released on our project page: https://pku-epic.github.io/BODex."
Dark-DENet: A Lightweight Enhancement Network for Low-Light Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11127459,"Deep learning-based object detection methods have shown significant success, particularly in robotic vision tasks like autonomous navigation and object manipulation. However, their performance drops sharply in low-light conditions, challenging robots in poorly lit environments. To address this, we propose Dark-DENet, a lightweight detection-driven enhancement network specifically designed for low-light conditions. Dark-DENet introduces an Improved Global Enhancement Module for low-frequency components to capture multiscale features, and a multi-layer convolutional structure in the Detail Enhancement Module to enhance high-frequency components. Additionally, the Scale-Aware Pooling Fusion Module enriches the semantic information of HF components. Dark-DENet is a plug-and-play network that can be easily integrated into the backbone of various detectors for joint training. Integrated with YOLOv5 as DD-YOLO, and combined with other models like YOLO series, RT-DETR, RetinaNet, and Faster R-CNN, experimental results show Dark-DENet consistently improves detection performance across all models. It effectively enhances latent features under limited runtime, making it a robust solution for robotic vision in low-light environments."
HGSLoc: 3DGS-Based Heuristic Camera Pose Refinement.,https://doi.org/10.1109/ICRA55743.2025.11127431,"Visual localization refers to the process of determining camera poses and orientation within a known scene representation. This task is often complicated by factors such as changes in illumination and variations in viewing angles. In this paper, we propose HGSLoc, a novel lightweight plug-and-play pose optimization framework, which integrates 3D reconstruction with a heuristic refinement strategy to achieve higher pose estimation accuracy. Specifically, we introduce an explicit geometric map for 3D representation and high-fidelity rendering, allowing the generation of high-quality synthesized views to support accurate visual localization. Our method demonstrates higher localization accuracy compared to NeRFbased neural rendering localization approaches. We introduce a heuristic refinement strategy, its efficient optimization capability can quickly locate the target node, while we set the steplevel optimization step to enhance the pose accuracy in the scenarios with small errors. With carefully designed heuristic functions, it offers efficient optimization capabilities, enabling rapid error reduction in rough localization estimations. Our method mitigates the dependence on complex neural network models while demonstrating improved robustness against noise and higher localization accuracy in challenging environments, as compared to neural network joint optimization strategies. The optimization framework proposed in this paper introduces novel approaches to visual localization by integrating the advantages of 3D reconstruction and the heuristic refinement strategy, which demonstrates strong performance across multiple benchmark datasets, including 7Scenes and Deep Blending dataset. The implementation of our method has been released at https://github.com/anchang699/HGSLoc."
Ergodic Trajectory Optimization on Generalized Domains Using Maximum Mean Discrepancy.,https://doi.org/10.1109/ICRA55743.2025.11127573,"We present a novel formulation of ergodic trajectory optimization that can be specified over general domains using kernel maximum mean discrepancy. Ergodic trajectory optimization is an effective approach that generates coverage paths for problems related to robotic inspection, information gathering problems, and search and rescue. These optimization schemes compel the robot to spend time in a region proportional to the expected utility of visiting that region. Current methods for ergodic trajectory optimization rely on domain-specific knowledge, e.g., a defined utility map, and well-defined spatial basis functions to produce ergodic trajectories. Here, we present a generalization of ergodic trajectory optimization based on maximum mean discrepancy that requires only samples from the search domain. We demonstrate the ability of our approach to produce coverage trajectories on a variety of problem domains including robotic inspection of objects with differential kinematics constraints and on Lie groups without having access to domain specific knowledge. Furthermore, we show favorable computational scaling compared to existing state-of-the-art methods for ergodic trajectory optimization with a trade-off between domain specific knowledge and computational scaling, thus extending the versatility of ergodic coverage on a wider application domain."
OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128049,"Autonomous driving has the potential to significantly enhance productivity and provide numerous societal benefits. Ensuring robustness in these safety-critical systems is essential, particularly when vehicles must navigate adverse weather conditions and sensor corruptions that may not have been encountered during training. Current methods often overlook uncertainties arising from adversarial conditions or distributional shifts, limiting their real-world applicability. We propose an efficient adaptation of an uncertainty estimation technique for 3D occupancy prediction. Our method dynamically calibrates model confidence using epistemic uncertainty estimates. Our evaluation under various camera corruption scenarios, such as fog or missing cameras, demonstrates that our approach effectively quantifies epistemic uncertainty by assigning higher uncertainty values to unseen data. We introduce region-specific corruptions to simulate defects affecting only a single camera and validate our findings through both scene-level and region-level assessments. Our results show superior performance in Out-of-Distribution (OoD) detection and confidence calibration compared to common baselines such as Deep Ensembles and MC-Dropout. Our approach consistently demonstrates reliable uncertainty measures, indicating its potential for enhancing the robustness of autonomous driving systems in real-world scenarios. Code and dataset are available at https://github.com/ika-rwth-aachen/OCCUQ."
UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128050,"Terrain awareness is an essential milestone to enable truly autonomous off-road navigation. Accurately predicting terrain characteristics allows optimizing a vehicle's path against potential hazards. Recent methods use deep neural networks to predict terrain properties in a self-supervised manner, relying on proprioception as a training signal. However, onboard cameras are inherently limited by their point-ofview relative to the ground, suffering from occlusions and vanishing pixel density with distance. This paper introduces a novel approach for self-supervised terrain characterization using an aerial perspective from a hovering drone. We capture terrain-aligned images while sampling the environment with a ground vehicle, effectively training a simple predictor for vibrations, bumpiness, and energy consumption. Our dataset includes 2.8 km of off-road data collected in forest environment, comprising 13484 ground-based images and 12935 aerial images. Our findings show that drone imagery improves terrain property prediction by 21.37% on the whole dataset and 37.35% in high vegetation, compared to ground robot images. We conduct ablation studies to identify the main causes of these performance improvements. We also demonstrate the realworld applicability of our approach by scouting an unseen area with a drone, planning and executing an optimized path on the ground."
Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance.,https://doi.org/10.1109/ICRA55743.2025.11128042,"We demonstrate the capabilities of an attentionbased end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m/s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control."
Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical Tweezer (OT)-Driven Microrobotics Simulation Platform.,https://doi.org/10.1109/ICRA55743.2025.11127419,"Optical tweezers (OT) offer unparalleled capabilities for micromanipulation with submicron precision in biomedical applications. However, controlling conventional multi-trap OT to achieve cooperative manipulation of multiple complexshaped microrobots in dynamic environments poses a significant challenge. To address this, we introduce Interactive OT Gym, a reinforcement learning (RL)-based simulation platform designed for OT-driven microrobotics. Our platform supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks. This integration allows for an adaptive blend of manual and autonomous control, enabling seamless transitions between human input and autonomous operation. We evaluated the effectiveness of our platform using a cell manipulation task. Experimental results show that our shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate. With its high fidelity, interactivity, low cost, and high-speed simulation capabilities, Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms. For more details on the project, please see our website https://sites.google.com/view/otgym"
Simplifying Reward Design in Complex Robotics: Average-Reward Maximum Entropy Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128043,"This paper presents a novel approach to addressing the control challenges of underactuated systems, focusing on the swing-up and stabilisation tasks on the double pendulum system. We propose the Average-Reward Entropy Advantage Policy Optimisation (AR-EAPO), a model-free reinforcement learning (RL) algorithm that integrates the strengths of the average-reward RL and the maximum entropy RL (MaxEnt RL). The average reward criterion allows the use of a simple reward function by naturally promoting the longterm goals, at the same time MaxEnt RL encourages the robustness of the policy. We validate our approach through simulations, consistently outperforming standard RL baselines and traditional control methods. Also, we provide preliminary test results on real double pendulum hardware. Additional experiments on MuJoCo environments further demonstrate AR-EAPO's efficacy on general continuous control tasks. This work underscores the potential of the average-reward criterion in simplifying control design while achieving superior results."
Narrow Passage Path Planning Using Collision Constraint Interpolation.,https://doi.org/10.1109/ICRA55743.2025.11127444,"Narrow passage path planning is a prevalent problem from industrial to household sites, often facing difficulties in finding feasible paths or requiring excessive computational resources. Given that deep penetration into the environment can cause optimization failure, we propose a framework to ensure feasibility throughout the process using a series of subproblems tailored for narrow passage problem. We begin by decomposing the environment into convex objects and initializing collision constraints with a subset of these objects. By continuously interpolating the collision constraints through the process of sequentially introducing remaining objects, our proposed framework generates subproblems that guide the optimization toward solving the narrow passage problem. Several examples are presented to demonstrate how the proposed framework addresses narrow passage path planning problems."
AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments.,https://doi.org/10.1109/ICRA55743.2025.11128046,"In robotic navigation, maintaining precise pose estimation and navigation in complex and dynamic environments is crucial. However, environmental challenges such as smoke, tunnels, and adverse weather can significantly degrade the performance of single-sensor systems like LiDAR or GPS, compromising the overall stability and safety of autonomous robots. To address these challenges, we propose AF-RLIO: an adaptive fusion approach that integrates 4D millimeterwave radar, LiDAR, inertial measurement unit (IMU), and GPS to leverage the complementary strengths of these sensors for robust odometry estimation in complex environments. Our method consists of three key modules. Firstly, the pre-processing module utilizes radar data to assist LiDAR in removing dynamic points and determining when environmental conditions are degraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects appropriate point cloud data for scan-tomap matching and tightly couples it with the IMU using the Iterative Error State Kalman Filter. Lastly, the factor graph optimization module balances weights between odometry and GPS data, constructing a pose graph for optimization. The proposed approach has been evaluated on datasets and tested in real-world robotic environments, demonstrating its effectiveness and advantages over existing methods in challenging conditions such as smoke and tunnels. Furthermore, we open source our code at https://github.com/NeSC-IV/AF-RLIO.git to benefit the research community."
A Modified Resistance Model for Magnetic Honeycomb Robots to Navigate in Low Reynolds Number Fluids.,https://doi.org/10.1109/ICRA55743.2025.11127438,"In recent years, magnetically controlled microrobots have garnered significant attention. This paper presents the H-robot, a self-designed microrobot featuring an innovative structure. The H-robot features a honeycomb porous spherical design specifically engineered to enhance cargo capacity. A new dynamic model for this structure has been developed for low Reynolds number fluid environments, along with a robust backstepping sliding mode control (RBSMC) strategy. Experiments were conducted in a calibrated magnetic field generated by a magnetic field generator to achieve precise motion control. The results demonstrate that the H-robot accurately tracks standard trajectories, with root mean square errors (RMSE) of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$9.09 \times 10^{-4} \mathbf{~ m}$</tex> for the Number-8 path and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$8.29 \times 10^{-4} \mathbf{~ m}$</tex> for the S-shaped path. Additionally, the proposed resistance model enhances tracking accuracy by 73.61% compared to traditional models, effectively adjusting the dynamic behavior of the H-robot in low Reynolds number fluids and significantly improving its motion performance. Finally, path planning experiments in a maze demonstrate the H-robot's ability to navigate and avoid obstacles."
Bayesian Optimal Experimental Design for Robot Kinematic Calibration.,https://doi.org/10.1109/ICRA55743.2025.11127428,"This paper develops a Bayesian optimal experimental design for robot kinematic calibration on <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbb{S}^{3} \times \mathbb{R}^{3}$</tex>. Our method builds upon a Gaussian process approach that incorporates a geometry-aware kernel based on Riemannian Matrn kernels over <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbb{S}^{3}$</tex>. To learn the forward kinematics errors via Bayesian optimization with a Gaussian process, we define a geodesic distance-based objective function. Pointwise values of this function are sampled via noisy measurements taken using fiducial markers on the end-effector using a camera and computed pose with the nominal kinematics. The corrected Denavit-Hartenberg parameters are obtained using an efficient quadratic program that operates on the collected data sets. The effectiveness of the proposed method is demonstrated via simulations and calibration experiments on NASA's ocean world lander autonomy testbed (OWLAT)."
Universal Online Temporal Calibration for Optimization-Based Visual-Inertial Navigation Systems.,https://doi.org/10.1109/ICRA55743.2025.11127433,"6-Degree of Freedom (6DoF) motion estimation with a combination of visual and inertial sensors is a growing area with numerous real-world applications. However, precise calibration of the time offset between these two sensor types is a prerequisite for accurate and robust tracking. To address this, we propose a universal online temporal calibration strategy for optimization-based visual-inertial navigation systems. Technically, we incorporate the time offset <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$t_{d}$</tex> as a state parameter in the optimization residual model to align the IMU state to the corresponding image timestamp using <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$t_{d}$</tex>, angular velocity and translational velocity. This allows the temporal misalignment <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$t_{d}$</tex> to be optimized alongside other tracking states during the process. As our method only modifies the structure of the residual model, it can be applied to various optimization-based frameworks with different tracking frontends. We evaluate our calibration method with both EuRoC [1] and simulation data and extensive experiments demonstrate that our approach provides more accurate time offset estimation and faster convergence, particularly in the presence of noisy sensor data. The experimental code is available at https://github.com/bytedance/Ts_Online_Optimization."
Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies.,https://doi.org/10.1109/ICRA55743.2025.11127509,"With increasing efficiency and reliability, autonomous systems are becoming valuable assistants to humans in various tasks. In the context of robot-assisted delivery, we investigate how robot performance and trust repair strategies impact human trust. In this task, while handling a secondary task, humans can choose to either send the robot to deliver autonomously or manually control it. The trust repair strategies examined include short and long explanations, apology and promise, and denial. Using data from human participants, we model human behavior using an Input-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and human action probabilities. Our findings indicate that humans are more likely to deploy the robot autonomously when their trust is high. Furthermore, state transition estimates show that long explanations are the most effective at repairing trust following a failure, while denial is most effective at preventing trust loss. We also demonstrate that the trust estimates generated by our model are isomorphic to self-reported trust values, making them interpretable. This model lays the groundwork for developing optimal policies that facilitate real-time adjustment of human trust in autonomous systems."
"RTAGrasp: Learning Task-Oriented Grasping from Human Videos via Retrieval, Transfer, and Alignment.",https://doi.org/10.1109/ICRA55743.2025.11128140,"Task-oriented grasping (TOG) is crucial for robots to accomplish manipulation tasks, requiring the determination of TOG positions and directions. Existing methods either rely on costly manual TOG annotations or only extract coarse grasping positions or regions from human demonstrations, limiting their practicality in real-world applications. To address these limitations, we introduce RTAGrasp, a Retrieval, Transfer, and Alignment framework inspired by human grasping strategies. Specifically, our approach first effortlessly constructs a robot memory from human grasping demonstration videos, extracting both TOG position and direction constraints. Then, given a task instruction and a visual observation of the target object, RTAGrasp retrieves the most similar human grasping experience from its memory and leverages semantic matching capabilities of vision foundation models to transfer the TOG constraints to the target object in a training-free manner. Finally, RTAGrasp aligns the transferred TOG constraints with the robot's action for execution. Evaluations on the public TOG benchmark, TaskGrasp dataset, show the competitive performance of RTAGrasp on both seen and unseen object categories compared to existing baseline methods. Real-world experiments further validate its effectiveness on a robotic arm. Our code, appendix, and video are available at https://sites.google.com/view/rtagrasp/home."
CLIMB: Language-Guided Continual Learning for Task Planning with Iterative Model Building.,https://doi.org/10.1109/ICRA55743.2025.11127439,"Intelligent and reliable task planning is a core capability for generalized robotics, which requires a descriptive domain representation that sufficiently models all object and state information for the scene. We present CLIMB, a continual learning framework for robot task planning that leverages foundation models and feedback from execution to guide the construction of domain models. CLIMB can build a model from a natural language description, learn non-obvious predicates while solving tasks, and store that information for future problems. We demonstrate the ability of CLIMB to improve performance in common planning environments compared to baseline methods. We also developed the BlocksWorld++ domain, a simulated environment with an easily usable real counterpart, together with a curriculum of tasks with progressing difficulty to evaluate continual learning. Code and additional details for this system can be found at https://plan-with-climb.github.io/."
AI-Enhanced Automatic Design of Efficient Underwater Gliders.,https://doi.org/10.1109/ICRA55743.2025.11127645,"The development of novel autonomous underwater gliders has been hindered by limited shape diversity, primarily due to the reliance on traditional design tools that depend heavily on manual trial and error. Building an automated design framework is challenging due to the complexities of representing glider shapes and the high computational costs associated with modeling complex solid-fluid interactions. In this work, we introduce an AI-enhanced automated computational framework designed to overcome these limitations by enabling the creation of underwater robots with non-trivial hull shapes. Our approach involves an algorithm that cooptimizes both shape and control signals, utilizing a reducedorder geometry representation and a differentiable neural-network-based fluid surrogate model. This end-to-end design workflow facilitates rapid iteration and evaluation of hydrodynamic performance, leading to the discovery of optimal and complex hull shapes across various control settings. We validate our method through wind tunnel experiments and swimming pool gliding tests, demonstrating that our computationally designed gliders surpass manually designed counterparts in terms of energy efficiency. By addressing challenges in efficient shape representation and neural fluid surrogate models, our work paves the way for the development of highly efficient underwater gliders, with implications for long-range ocean exploration and environmental monitoring."
A Visual Servo System for Robotic on-Orbit Servicing Based on 3D Perception of Non-Cooperative Satellite.,https://doi.org/10.1109/ICRA55743.2025.11127498,"The 3D perception of satellites, including both their shape and pose, is a key foundation for robotic on-orbit servicing. However, the demanding space environment-such as intense and dim illumination-presents significant challenges. Previous non-cooperative methods focus on specific geometric features like solar panel brackets or docking rings, overlooking the satellite's overall shape and increasing the risk of collisions during grasping. Additionally, satellites are often weakly textured, limiting the accuracy of 3D perception. To address these issues, we propose, for the first time, a 3D perceptionbased visual servo system of non-cooperative satellites. This system combines reconstruction and tracking to enhance shape perception and pose estimation accuracy in orbital conditions. Specifically, we employ an alternating iterative strategy to simultaneously reconstruct and track the satellite and introduce a novel constraint to fuse different cues under extreme conditions. Further, we develop a simulation environment platform, a dualarm microgravity grasping system, and an online monitoring module to enhance system capabilities for on-orbit servicing. Synthetic and real-world datasets from the simulation environment are also created for experimental validation. Results show that each module of our system achieves state-of-the-art performance."
Cocube: a Tabletop Modular Multi-Robot Platform for Education and Research.,https://doi.org/10.1109/ICRA55743.2025.11127500,This paper presents CoCube a tabletop modular robotics platform designed for robotics education and multirobot algorithm research. CoCube is characterized by its low cost low floors high ceilings and wide walls offering flexibility and broad applicability across various use cases. The platform comprises four key components: CoCube robots which integrate wireless communication movement and interaction; CoModules which provide versatile external functionality; CoMaps which enable high-precision localization via microdot patterns on regular printed paper; and CoTags for interaction. CoCube operates on MicroBlocks a blocks programming language for physical computing inspired by MIT Scratch a widely-used coding language with a simple visual interface that makes programming accessible to young learners. It offers users both flexibility and ease of use with advanced API support for more complex applications. This paper details the design of the CoCube platform and demonstrates its potential in both educational and research contexts.
Soft Robotic Dynamic in-Hand Pen Spinning.,https://doi.org/10.1109/ICRA55743.2025.11127449,"Dynamic in-hand manipulation remains challenging for soft robotic systems, which have demonstrated advantages in safe, compliant interactions but struggle with highspeed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasistatic actions, and precise object models, SWIFT learns to spin a pen through trial and error using only real-world data and without requiring explicit knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, SWIFT discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen reliably. After 130 sampled actions per object, SWIFT achieves 10/10 success rate across three pens with different weights and weight distributions, demonstrating generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks. We also demonstrate generalization to different shapes and weights, such as a brush and a screwdriver, with 10/10 and 5/10 success rates, respectively. Videos, data, and code are available at https://soft-spin.github.io."
Inference Based Multi-Object Reactive Search in a Partially Known Environment With Temporal Logic Specifications.,https://doi.org/10.1109/ICRA55743.2025.11127453,"Efficiently searching for multiple objects in a partially known environment, where only the names and locations of landmarks are available, presents significant challenges. Existing search algorithms in the literature fail to fully utilize prior knowledge to improve search efficiency, and exhibit significantly diminished efficiency when extended to multiobject search. To address these limitations, we propose an inference-based multi-object reactive search framework. This framework utilizes the COMET inference model to reason about co-occurrence values between the target objects and known landmarks, thereby enhancing search efficiency. These co-occurrence values are integrated into a reactive temporal logic motion planning strategy, which allows the robot search for multiple objects with temporal logic constraints specified by LTL and adapt dynamically if the inferred reasoning differs from the actual object arrangement encountered during the search. Extensive simulations were conducted to evaluate the feasibility and efficiency of the proposed motion planning algorithm. Results demonstrate that the integration of commonsense reasoning with reactive temporal logic planning significantly improves multi-object search efficiency. Project website: https://sites.google.com/view/imors."
Residual Policy Learning for Perceptive Quadruped Control Using Differentiable Simulation.,https://doi.org/10.1109/ICRA55743.2025.11127448,"First-order Policy Gradient (FoPG) algorithms such as Backpropagation through Time and Analytical Policy Gradients leverage local simulation physics to accelerate policy search, significantly improving sample efficiency in robot control compared to standard model-free reinforcement learning. However, FoPG algorithms can exhibit poor learning dynamics in contact-rich tasks like locomotion. Previous approaches address this issue by alleviating contact dynamics via algorithmic or simulation innovations. In contrast, we propose guiding the policy search by learning a residual over a simple baseline policy. For quadruped locomotion, we find that the role of residual policy learning in FoPG-based training (FoPG RPL) is primarily to improve asymptotic rewards, compared to improving sample efficiency for model-free RL. Additionally, we provide insights on applying FoPG's to pixel-based local navigation, training a point-mass robot to convergence within seconds. Finally, we showcase the versatility of FoPG RPL by using it to train locomotion and perceptive navigation end-toend on a quadruped in minutes."
FedEFM: Federated Endovascular Foundation Model with Unseen Data.,https://doi.org/10.1109/ICRA55743.2025.11127787,"In endovascular surgery, the precise identification of catheters and guidewires in X-ray images is essential for reducing intervention risks. However, accurately segmenting catheter and guidewire structures is challenging due to the limited availability of labeled data. Foundation models offer a promising solution by enabling the collection of similar-domain data to train models whose weights can be fine-tuned for downstream tasks. Nonetheless, large-scale data collection for training is constrained by the necessity of maintaining patient privacy. This paper proposes a new method to train a foundation model in a decentralized federated learning setting for endovascular intervention. To ensure the feasibility of the training, we tackle the unseen data issue using differentiable Earth Mover's Distance within a knowledge distillation frame-work. Once trained, our foundation model's weights provide valuable initialization for downstream tasks, thereby enhancing task-specific performance. Intensive experiments show that our approach achieves new state-of-the-art results, contributing to advancements in endovascular intervention and robotic-assisted endovascular surgery, while addressing the critical issue of data sharing in the medical domain."
Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus.,https://doi.org/10.1109/ICRA55743.2025.11127429,"Depth estimation is a fundamental task in 3D geometry. While stereo depth estimation can be achieved through triangulation methods, it is not as straightforward for monocular methods, which require the integration of global and local information. The Depth from Defocus (DFD) method utilizes camera lens models and parameters to recover depth information from blurred images and has been proven to perform well. However, these methods rely on All-In-Focus (AIF) images for depth estimation, which is nearly impossible to obtain in real-world applications. To address this issue, we propose a self-supervised framework based on 3D Gaussian splatting and Siamese networks. By learning the blur levels at different focal distances of the same scene in the focal stack, the framework predicts the defocus map and Circle of Confusion (CoC) from a single defocused image, using the defocus map as input to DepthNet for monocular depth estimation. The 3D Gaussian splatting model renders defocused images using the predicted CoC, and the differences between these and the real defocused images provide additional supervision signals for the Siamese Defocus self-supervised network. This framework has been validated on both artificially synthesized and real blurred datasets. Subsequent quantitative and visualization experiments demonstrate that our proposed framework is highly effective as a DFD method."
Robust Robotic Breast Ultrasound Scanning and Real-Time Lesion Localization.,https://doi.org/10.1109/ICRA55743.2025.11127435,"The inherent flexibility and real-time deformation of breast tissue pose significant challenges for achieving full coverage and accurate lesion localization in autonomous breast ultrasound scanning. This paper introduces a robust finite state machine-based framework that mimics the decision-making process of an experienced physician, dynamically transitioning between the global breast scan and the fine lesion scan. An autonomous radial and anti-radial global scan pattern ensures comprehensive breast coverage. To avoid lesion misidentification caused by soft tissue movement, a real-time lesion fine scan method is proposed for lesion detection and localization. Experimental results demonstrate that the system in full coverage tests achieves 7 identified lesions out of 7 existing lesions and maintains a robust localization accuracy of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{3. 2 3 ~ m m}$</tex> across phantoms with varying stiffnesses."
Teleoperating a 6 DoF Robotic Manipulator from Head Movements.,https://doi.org/10.1109/ICRA55743.2025.11127494,"This article presents an interactive control approach allowing a human user to teleoperate a robotic manipulator located nearby. With this approach, the user keeps his/her hands free, as only head movements are exploited to control the robot. The controller maps the 6 Degrees of Freedom (DoF) user's head position and orientation into the 6 DoF robot endeffector position and orientation. The robot can reach a large workspace thanks to the combination of two features. Firstly, a virtual wand between the user's head and the robot end-effector converts user's head pantilt rotations into large displacements of the robot end-effector center perpendicularly to the wand axis (2 DoF). Secondly, for the remaining 4 DoF (robot end-effector center displacement along the wand axis and robot en-effector orientation), realtime deformation of the virtual wand is triggered when the user reaches uncomfortable configurations due to his/her head workspace limitations. Additionally, the user gets, through an Augmented Reality (AR) Headset, a non-delayed visual feedback of the current virtual wand geometry and location. The paper includes a description of the setup and the proposed controller, detailing how the robot position/orientation is coupled to the user's head position/orientation. A set of elementary experiments with a constant-geometry wand is first presented, showing workspace limitations for some DoF. Then the wand reconfiguration is introduced in the experiments, leading to full control of 6 DoF manipulation tasks throughout a large workspace."
Panoptic-Depth Forecasting.,https://doi.org/10.1109/ICRA55743.2025.11127571,"Forecasting the semantics and 3D structure of scenes is essential for robots to navigate and plan actions safely. Recent methods have explored semantic and panoptic scene forecasting; however, they do not consider the geometry of the scene. In this work, we propose the panoptic-depth forecasting task for jointly predicting the panoptic segmentation and depth maps of unobserved future frames, from monocular camera images. To facilitate this work, we extend the popular KITTI-360 and Cityscapes benchmarks by computing depth maps from LiDAR point clouds and leveraging sequential labeled data. We also introduce a suitable evaluation metric that quantifies both the panoptic quality and depth estimation accuracy of forecasts in a coherent manner. Furthermore, we present two baselines and propose the novel PDcast architecture that learns rich spatio-temporal representations by incorporating a transformer-based encoder, a forecasting module, and task-specific decoders to predict future panoptic-depth outputs. Extensive evaluations demonstrate the effectiveness of PDcast across two datasets and three forecasting tasks, consistently addressing the primary challenges. We make the code publicly available at https://pdcast.cs.uni-freiburg.de"
Routing Manipulation of Deformable Linear Object Using Reinforcement Learning and Diffusion Policy.,https://doi.org/10.1109/ICRA55743.2025.11127451,"Tasks involving deformable linear objects (DLOs) are prevalent in daily life but pose significant challenges due to their infinite degrees of freedom and underactuated nature. Frequent contact between DLOs and surrounding objects with unknown physical parameters, such as friction, further complicates their manipulation. Performing tasks like routing ropes through a hole requires gentle yet robust manipulation, making it particularly challenging. Previous research has not adequately addressed general DLO manipulation tasks that involve intensive contact, especially in environments with rough surfaces. This paper presents a robust and delicate manipulation learning approach for the DLO routing task, leveraging reinforcement learning (RL) and diffusion policy. First, reinforcement learning agents are trained separately for rope insertion and pulling. During training, the agents are encouraged to minimize rope tension throughout task execution in environments with randomized friction to achieve delicate motion. Next, the rollouts from these agents are collected as expert demonstrations to train a diffusion policy. Our approach generates delicate motions to prevent the rope from being damaged or getting stuck on rough surfaces while remaining robust against environmental disturbances. Please refer to our project page: https://lmeee.github.io/DLOPull/"
Cycloidal Quasi-Direct Drive Actuator Designs with Learning-Based Torque Estimation for Legged Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127436,"This paper presents a novel approach through the design and implementation of Cycloidal Quasi-Direct Drive actuators for legged robotics. The cycloidal gear mechanism, with its inherent high torque density and mechanical robustness, offers significant advantages over conventional designs. By integrating cycloidal gears into the Quasi-Direct Drive framework, we aim to enhance the performance of legged robots, particularly in tasks demanding high torque and dynamic loads, while still keeping them lightweight. Additionally, we develop a torque estimation framework for the actuator using an Actuator Network, which effectively reduces the sim-toreal gap introduced by the cycloidal drive's complex dynamics. This integration is crucial for capturing the complex dynamics of a cycloidal drive, which contributes to improved learning efficiency, agility, and adaptability for reinforcement learning."
Generalizable Imitation Learning Through Pre-Trained Representations.,https://doi.org/10.1109/ICRA55743.2025.11127800,"In this paper, we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce DVK, an imitation learning algorithm that leverages rich pre-trained Visual Transformer patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into groups associated with semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We demonstrate how this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. To facilitate further study of generalization in Imitation Learning, all of our code for the method and evaluation, as well as the dataset, is made available."
Robot Planning Under Uncertainty for Object Assembly and Troubleshooting Using Human Causal Models.,https://doi.org/10.1109/ICRA55743.2025.11128040,"In this paper we explore if human mental models of objects, even when flawed, can be integrated with a collaborative robot's decision making framework to allow it to make smarter choices under partial observability for different object-related tasks such as assembly and troubleshooting. We demonstrate how (1) these informative causal models can be extracted from humans through crowdsourcing, (2) object assembly and troubleshooting can be formulated as Partially Observable Markov Decision Processes (POMDPs) and (3) our extracted causal models can be incorporated into those models in the form of approximate priors. Finally, (4) we use systematic experimentation in simulation to demonstrate the success of this approach, with 2 X average improvement in reward observed for object assembly tasks, and 1.4 X average improvement in reward observed for troubleshooting tasks."
Point and Go: Intuitive Reference Frame Reallocation in Mode Switching for Assistive Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127426,"Operating high degree of freedom robots can be difficult for users of wheelchair mounted robotic manipulators. Mode switching in Cartesian space has several drawbacks such as unintuitive control reference frames, separate translation and orientation control, and limited movement capabilities that hinder performance. We propose Point and Go mode switching, which reallocates the Cartesian mode switching reference frames into a more intuitive action space comprised of new translation and rotation modes. We use a novel sweeping motion to point the gripper, which defines the new translation axis along the robot base frame's horizontal plane. This creates an intuitive point and go translation mode that allows the user to easily perform complex, human-like movements without switching control modes. The system's rotation mode combines position control with a refined endeffector oriented frame that provides precise and consistent robot actions in various end-effector poses. We verified its effectiveness through initial experiments, followed by a three-task user study that compared our method to Cartesian mode switching and a state of the art learning method. Results show that Point and Go mode switching reduced completion times by 31%, pauses by 41%, and mode switches by 33%, while receiving significantly favorable responses in user surveys."
Motion-Guided Dual-Camera Tracker for Endoscope Tracking and Motion Analysis in a Mechanical Gastric Simulator.,https://doi.org/10.1109/ICRA55743.2025.11127574,"Flexible endoscope motion tracking and analysis in mechanical simulators have proven useful for endoscopy training. Common motion tracking methods based on electromagnetic tracker are however limited by their high cost and material susceptibility. In this work, the motion-guided dual-camera vision tracker is proposed to provide robust and accurate tracking of the endoscope tip's 3D position. The tracker addresses several unique challenges of tracking flexible endoscope tip inside a dynamic, life-sized mechanical simulator. To address the appearance variation and keep dualcamera tracking consistency, the cross-camera mutual template strategy (CMT) is proposed by introducing dynamic transient mutual templates. To alleviate large occlusion and light-induced distortion, the Mamba-based motion-guided prediction head (MMH) is presented to aggregate historical motion with visual tracking. The proposed tracker achieves superior performance against state-of-the-art vision trackers, achieving 42% and 72% improvements against the second-best method in average error and maximum error. Further motion analysis involving novice and expert endoscopists also shows that the tip 3D motion provided by the proposed tracker enables more reliable motion analysis and more substantial differentiation between different expertise levels, compared with other trackers. Project page: https://github.com/PieceZhang/MotionDCTrack"
Shape-Space Deformer: Unified Visuo-Tactile Representations for Robotic Manipulation of Deformable Objects.,https://doi.org/10.1109/ICRA55743.2025.11127499,"Accurate modelling of object deformations is crucial for a wide range of robotic manipulation tasks, where interacting with soft or deformable objects is essential. Current methods struggle to generalise to unseen forces or adapt to new objects, limiting their utility in real-world applications. We propose Shape-Space Deformer, a unified representation for encoding a diverse range of object deformations using template augmentation to achieve robust, fine-grained reconstructions that are resilient to outliers and unwanted artefacts. Our method improves generalization to unseen forces and can rapidly adapt to novel objects, significantly outperforming existing approaches. We perform extensive experiments to test a range of force generalisation settings and evaluate our method's ability to reconstruct unseen deformations. Our results demonstrate significant improvements in reconstruction accuracy and robustness. Our approach is suitable for real-time performance, making it ready for downstream manipulation applications."
Robust Optical Transceiver Manipulation in Cluttered Cable Environments Using 3D Scene Understanding and Planning.,https://doi.org/10.1109/ICRA55743.2025.11127450,"Robotic manipulation in cluttered environments presents significant challenges, particularly when the clutter includes thin, deformable objects like cables, which complicate perception and decision-making processes. In the context of datacenters, the automation of networking tasks often involves the manipulation of optical transceivers within densely packed cable configurations. Such environments are characterized by an abundance of delicate, overlapping, and intersecting cables, leading to frequent occlusions. This paper introduces an innovative system designed for the manipulation of optical transceivers in environments cluttered by cables. Our integrated approach combines advanced 3D scene understanding with a heuristic-based pushing policy to effectively manipulate optical transceivers amidst clutter. The system's perception component utilizes image segmentation and 3D reconstruction to accurately model the transceivers and surrounding cables. Meanwhile, the planning aspect employs a search algorithm with task-specific heuristics, to navigate the gripper, displace obstructing cables, and safely achieve a precise pre-grasp position in front of the target transceiver. We have conducted extensive evaluations of our methodology in both simulated and real-world settings, demonstrating its high success rates, robustness, and proficiency in addressing the unique challenges posed by cable-occluded environments within datacenters."
Patch Tree: Exploiting the Gauss Map and Principal Component Analysis for Robotic Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127440,"Grasp planning must consider an object's local geometry (at the finger contacts), for the range of applicable wrenches under friction, and its global geometry, for force closure and grasp quality. Most everyday objects have curved surfaces unamenable to a pure combinatorial approach but treatable with tools from differential geometry. Our idea is to discretize such a surface in a top-down fashion into elementary patches (e-patches), each consisting of points that would yield close enough wrenches. Preprocessing based on Gaussian curvature decomposes the surface into strictly convex, strictly concave, ruled, and saddle patches. The Gauss map guides the subdivision of any patch with a large variation in the contact force direction, with the aid of a Platonic solid. The principal component analysis (PCA) further subdivides any patch that has a large variation in torque. The final structure is called a patch tree, which stores e-patches at its leaves, and force or torque ranges at its internal nodes. Grasp synthesis and optimization operates on the patch tree with a stack to efficiently prune away non-promising finger placements. Simulation and experiment with a Shadow Hand have been conducted over everyday items. The patch tree exhibits different levels of surface granularity. It has a good promise for efficient planning of finger gaits to carry out grasping and tool manipulation."
"Strain-Coordinated Formation, Migration, and Encapsulation Behaviors in a Tethered Robot Collective.",https://doi.org/10.1109/ICRA55743.2025.11127437,"Tethers are an underutilized tool in multi-robot systems: tethers can provide power, facilitate retrieval and sensing, and be used to manipulate and gather objects. Starting with the simplest possible configuration, our work explores how agents linked in series by flexible, passive, fixed-length tethers, can use those tethers as sensors to achieve distributed formation control. In this study, we extend upon previous work to show the applicability of strain-coordinated formation control for encapsulation and migration along a global gradient as well as the trade-offs between formation control and taxis in an obstacle-laden environment. Our results indicate significant potential for tethered robot collectives: versatile behaviors that can work on simple, resource-constrained robots or serve as a fallback mechanism in case more sophisticated means of coordination fail."
Adaptive Task Allocation in Multi-Human Multi-Robot Teams Under Team Heterogeneity and Dynamic Information Uncertainty.,https://doi.org/10.1109/ICRA55743.2025.11127646,"Task allocation in multi-human multi-robot (MHMR) teams presents significant challenges due to the inherent heterogeneity of team members, the dynamics of task execution, and the information uncertainty of operational states. Existing approaches often fail to address these challenges simultaneously, resulting in suboptimal performance. To tackle this, we propose ATA-HRL, an adaptive task allocation framework using hierarchical reinforcement learning (HRL), which incorporates initial task allocation (ITA) that leverages team heterogeneity and conditional task reallocation in response to dynamic operational states. Additionally, we introduce an auxiliary state representation learning task to manage information uncertainty and enhance task execution. Through an extensive case study in large-scale environmental monitoring tasks, we demonstrate the benefits of our approach. More details are available on our website: https://sites.google.com/view/ata-hrl."
Deliberative Control-Aware Motion Planning for Kinematic-Constrained UAVs in a Dynamic Environment.,https://doi.org/10.1109/ICRA55743.2025.11127441,"This paper introduces a motion planning approach for navigating in a dynamic environment. The path is represented using a Non-Uniform Rational B-Spline (NURBS) to ensure smoothness, curvature continuity, and proper orientation by adjusting its parameters. A Differential Evolution algorithm optimizes the curve parameters and traversal speed at each replanning interval, taking into account speed limits, maximum curvature, and obstacles in the environment. A constraintbased on Velocity Obstacle (VO) ensures collision-free motion, considering bounds provided by lower-level controllers. The feasibility of the approach is validated through simulations and real-world experiments with the Crazyflie 2.1 micro quadcopter."
An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals.,https://doi.org/10.1109/ICRA55743.2025.11128052,"This paper presents an iterative approach for heterogeneous multi-agent route planning in environments with unknown resource distributions. We focus on a team of robots with diverse capabilities tasked with executing missions specified using Capability Temporal Logic (CaTL), a formal framework built on Signal Temporal Logic to handle spatial, temporal, capability, and resource constraints. The key challenge arises from the uncertainty in the initial distribution and quantity of resources in the environment. To address this, we introduce an iterative algorithm that dynamically balances exploration and task fulfillment. Robots are guided to explore the environment, identifying resource locations and quantities while progressively refining their understanding of the resource landscape. At the same time, they aim to maximally satisfy the mission objectives based on the current information, adapting their strategies as new data is uncovered. This approach provides a robust solution for planning in dynamic, resource-constrained environments, enabling efficient coordination of heterogeneous teams even under conditions of uncertainty. Our method's effectiveness and performance are demonstrated through simulated case studies."
New Graph Distance Measures and Matching of Topological Maps for Robotic Exploration.,https://doi.org/10.1109/ICRA55743.2025.11127512,"Comparing graph-structured maps is a task of paramount importance in robotic exploration and cartography, but unfortunately the computational cost of the existing similarity measures, such as the graph edit distance (GED), is prohibitive for large graphs. In this paper, we introduce and characterize three new graph distance measures which satisfy the requirements for a metric. The first one, LogEig, computes the square root of the sum of the squared logarithms of the generalized eigenvalues of the shifted Laplacian matrices associated with the two graphs, while the second calculates the Bures distance between these positive definite matrices. The third distance, Rank, computes the rank of the difference of the graph shift operators associated with the two graphs, e.g. the adjacency or the Laplacian matrix. Examples and numerical experiments with graphs from a publicly-available dataset, show the accuracy and computational efficiency of the new metrics for 2D topological-map matching, compared to the GED. The effect of spectral sparsification on the new graph distance measures is examined as well."
MDC-Seg: Multi-Directional Convolution-Based Semantic Segmentation for LiDAR Point Clouds.,https://doi.org/10.1109/ICRA55743.2025.11127422,"LiDAR point clouds 3D semantic segmentation enables efficient and accurate environmental sensing for intelligent vehicles and autonomous robots, greatly advancing these domains. Existing advanced methods that use 3D sparse convolutional often suffer from a small Effective Receptive Field (ERF), which limits context sensing and challenging highperformance segmentation. Building on this observation, we propose MDC-Seg for efficient ERF enlargement. We design Multi-directional Convolution (MDConv), which simultaneously performs sparse feature encoding on the Bird's Eye View (BEV) and Range View (RV) planes to enlarge the ERF of 3D sparse convolution. To enhance feature fusion in MDConv, we introduce an attention mechanism and design an efficient multifeature fusion (EMFF) module suitable for both 3D and 2D sparse features. To improve segmentation accuracy, we design a point-voxel constraint (PVC) module to handle edge voxels containing multiple point cloud categories, optimizing the final inference results. These modules add minimal memory and inference time but significantly improve performance compared to the baseline. Extensive experiments on the SemanticKITTI benchmark demonstrate MDC-Seg's excellent performance, with supplementary tests on nuScenes further confirming its superiority by yielding good results. The source code is available at https://github.com/OYgreat-river/MDC-Seg."
Human-Robot Collaboration for the Remote Control of Mobile Humanoid Robots With Torso-Arm Coordination.,https://doi.org/10.1109/ICRA55743.2025.11128048,"Recently, many humanoid robots have been increasingly deployed in various facilities, including hospitals and assisted living environments, where they are often remotely controlled by human operators. Their kinematic redundancy enhances reachability and manipulability, enabling them to navigate complex, cluttered environments and perform a wide range of tasks. However, this redundancy also presents significant control challenges, particularly in coordinating the movements of the robot's macro-micro structure (torso and arms). Therefore, we propose various human-robot collaborative (HRC) methods for coordinating the torso and arm of remotely controlled mobile humanoid robots, aiming to balance autonomy and human input to enhance system efficiency and task execution. The proposed methods include human-initiated approaches, where users manually control torso movements, and robot-initiated approaches, which autonomously coordinate torso and arm based on factors such as reachability, task goal, or inferred human intent. We conducted a user study with <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{N} \boldsymbol{=} \mathbf{1 7}$</tex> participants to compare the proposed approaches in terms of task performance, manipulability, and energy efficiency, and analyzed which methods were preferred by participants."
Maintaining Strong $r$-Robustness in Reconfigurable Multi-Robot Networks Using Control Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11127420,"In leader-follower consensus, strong <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$r$</tex>-robustness of the communication graph provides a sufficient condition for followers to achieve consensus in the presence of misbehaving agents. Previous studies have assumed that robots can form and/or switch between predetermined network topologies with known robustness properties. However, robots with distancebased communication models may not be able to achieve these topologies while moving through spatially constrained environments, such as narrow corridors, to complete their objectives. This paper introduces a Control Barrier Function (CBF) that ensures robots maintain strong <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$r$</tex>-robustness of their communication graph above a certain threshold without maintaining any fixed topologies. Our CBF directly addresses robustness, allowing robots to have flexible reconfigurable network structure while navigating to achieve their objectives. The efficacy of our method is tested through various simulation and hardware experiments [code] https://github.com/joonlee16/Resilient-Leader-Follower-CBF-QP."
From Imitation to Refinement - Residual Rl for Precise Assembly.,https://doi.org/10.1109/ICRA55743.2025.11127442,"Recent advances in Behavior Cloning (BC) have made it easy to teach robots new tasks. However, we find that the ease of teaching comes at the cost of unreliable performance that saturates with increasing data for tasks requiring precision. The performance saturation can be attributed to two critical factors: (a) distribution shift resulting from the use of offline data and (b) the lack of closed-loop corrective control caused by action chucking (predicting a set of future actions executed open-loop) critical for BC performance. Our key insight is that by predicting action chunks, BC policies function more like trajectory planners than closedloop controllers necessary for reliable execution. To address these challenges, we devise a simple yet effective method, Resip (Residual for Precise Manipulation), that overcomes the reliability problem while retaining BC's ease of teaching and long-horizon capabilities. Resip augments a frozen, chunked BC model with a fully closed-loop residual policy trained with reinforcement learning (RL) that addresses distribution shifts and introduces closed-loop corrections over open-loop execution of action chunks predicted by the BC trajectory planner. Videos, code, and data: residual-assembly.github.io."
Learning Adversarial Policies for Swarm Leader Identification Using a Probing Agent.,https://doi.org/10.1109/ICRA55743.2025.11128045,"This study introduces a novel approach to swarm leader identification (SLI) in multi-agent robot systems by employing a physical adversary interacting with the swarm in the same environment. We develop a new simulation environment to study the SLI problem and train an adversary, which we term the prober, to solve the SLI problem using forceful interactions with the swarm as its guiding information source. The prober's policy is modeled using the simplified structure state space sequence (S5) model and trained with the Proximal Policy Optimization (PPO) algorithm. The prober only has access to the information on the relative positions of the other agents. We evaluate our approach through extensive simulations using two performance metrics and validate the sim-to-real transfer through robot experiments. Results on evaluating the performance in 10,000 different testing scenarios demonstrate that our method finds the leader's identity in the vast majority (95.7%) of the cases, regardless of the initial leader selection during training. The proposed system represents the first instance of learning-based automatic identification of leader agents in a swarm. This capability is crucial for enabling efficient and robust human-swarm interaction, understanding artificial swarm behaviors, and analyzing latent behaviors in biological swarms in nature."
Loopy Movements: Emergence of Rotation in a Multicellular Robot.,https://doi.org/10.1109/ICRA55743.2025.11128053,"Unlike most human-engineered systems, many biological systems rely on emergent behaviors from low-level interactions, enabling greater diversity and superior adaptation to complex, dynamic environments. This study explores emergent decentralized rotation in the Loopy multicellular robot, composed of homogeneous, physically linked, 1-degree-of-freedom cells. Inspired by biological systems like sunflowers, Loopy uses simple local interactions-diffusion, reaction, and active transport of simulated chemicals, called morphogens-without centralized control or knowledge of its global morphology. Through these interactions, the robot self-organizes to achieve coordinated rotational motion and forms lobes-local protrusions created by clusters of motor cells. This study investigates how these interactions drive Loopy's rotation, the impact of its morphology, and its resilience to actuator failures. Our findings reveal two distinct behaviors: 1) inner valleys between lobes rotate faster than the outer peaks, contrasting with rigid body dynamics, and 2) cells rotate in the opposite direction of the overall morphology. The experiments show that while Loopy's morphology does not affect its angular velocity relative to its cells, larger lobes increase cellular rotation and decrease morphology rotation relative to the environment. Even with up to one-third of its actuators disabled and significant morphological changes, Loopy maintains its rotational abilities, highlighting the potential of decentralized, bio-inspired strategies for resilient and adaptable robotic systems."
Assembly Order Planning for Modular Structures by Autonomous Multi-Robot Systems.,https://doi.org/10.1109/ICRA55743.2025.11127644,"Coordinated multi-agent robotic construction provides a means to build infrastructure in extreme environments and improve efficiency in high performance applications. Planning methods are key to understanding and achieving the scope of such applications, and are typically tailored to specific models of construction material and a consideration of passivity or activity thereof. Here, we focus on the NASA Automated Reconfigurable Mission Adaptive Digital Assembly Systems (ARMADAS) model, which includes passive lightweight structural modules and small robots that traverse the structure. We present an algorithm for calculating a build plan for robots under the constraints of this type of system. We then evaluate the quality of this plan experimentally. Many of the techniques we use can be applied to any robotic assembly system whose robots perform locomotion over the structure that they are building."
Achieving Human Level Competitive Robot Table Tennis.,https://doi.org/10.1109/ICRA55743.2025.11127501,"Achieving human-level performance on real world tasks is a north star for the robotics community. We present the first learned robot agent that reaches amateur humanlevel performance in competitive table tennis. Table tennis is a physically demanding sport that takes humans years to master. We contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their skill descriptors that model their capabilities and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real and curriculum building, including an iterative approach (train in sim, deploy in real), and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45 % (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100 % matches vs. beginners and 55 % matches vs. intermediate players, demonstrating solidly amateur humanlevel performance. Videos of the matches can be viewed here<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1.</sup>See sites https://google.com/view/competitive-robot-table-tennis."
Robo-DM: Data Management for Large Robot Datasets.,https://doi.org/10.1109/ICRA55743.2025.11128693,"Recent results suggest that very large datasets of teleoperated robot demonstrations can be used to train transformer-based models that have the potential to generalize to new scenes, robots, and tasks. However, curating, distributing, and loading large datasets of robot trajectories, which typically consist of video, textual, and numerical modalities - including streams from multiple cameras - remains challenging. We propose Robo-DM, an efficient open-source cloud-based data management toolkit for collecting, sharing, and learning with robot data. With Robo-DM, robot datasets are stored in a self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can significantly reduce the size of robot trajectory data, transfer costs, and data load time during training. Compared to the RLDS format used in OXE datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates data retrieval by load-balancing video decoding with memory-mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x faster when decoding sequentially. We physically evaluate a model trained by Robo-DM with lossy compression, a pick-and-place task, and In-Context Robot Transformer. Robo-DM uses 75x compression of the original dataset and does not suffer reduction in downstream task accuracy. Code and evaluation scripts can be found on website https://github.com/BerkeleyAutomation/fog_x."
No Plan but Everything Under Control: Robustly Solving Sequential Tasks with Dynamically Composed Gradient Descent.,https://doi.org/10.1109/ICRA55743.2025.11127552,"We introduce a novel gradient-based approach for solving sequential tasks by dynamically adjusting the underlying myopic potential field in response to feedback and the world's regularities. This adjustment implicitly considers subgoals encoded in these regularities, enabling the solution of long sequential tasks, as demonstrated by solving the traditional planning domain of Blocks Worldwithout any planning. Unlike conventional planning methods, our feedbackdriven approach adapts to uncertain and dynamic environments, as demonstrated by one hundred real-world trials involving drawer manipulation. These experiments highlight the robustness of our method compared to planning and show how interactive perception and error recovery naturally emerge from gradient descent without explicitly implementing them. This offers a computationally efficient alternative to planning for a variety of sequential tasks, while aligning with observations on biological problem-solving strategies."
MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation.,https://doi.org/10.1109/ICRA55743.2025.11127306,"In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12 % of the teacher model's parameter count."
PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-Rich Manipulation Using Tactile-Diffusion Policies.,https://doi.org/10.1109/ICRA55743.2025.11128816,"Achieving robust dexterous manipulation in un-structured domestic environments remains a significant challenge in robotics. Even with state-of-the-art robot learning methods, haptic-oblivious control strategies (i.e. those relying only on external vision and/or proprioception) often fall short due to occlusions, visual complexities, and the need for precise contact interaction control. To address these limitations, we introduce PolyTouch, a novel robot finger that integrates camera-based tactile sensing, acoustic sensing, and peripheral visual sensing into a single design that is compact and durable. PolyTouch provides high-resolution tactile feedback across multiple temporal scales, which is essential for efficiently learning complex manipulation tasks. Experiments demonstrate an at least 20-fold increase in lifespan over commercial tactile sensors, with a design that is both easy to manufacture and scalable. We then use this multimodal tactile feedback along with visuo-proprioceptive observations to synthesize a tactile-diffusion policy from human demonstrations; the resulting contact-aware control policy significantly outperforms haptic-oblivious policies in multiple contact-aware manipulation policies. This paper highlights how effectively integrating multimodal contact sensing can hasten the development of effective contact-aware manipulation policies, paving the way for more reliable and versatile domestic robots. More information can be found at https://polytouch.alanz.info/."
A New Stereo Fisheye Event Camera for Fast Drone Detection and Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128164,"In this paper, we present a new compact vision sensor consisting of two fisheye event cameras mounted back-to-back, which offers a full 360-degree view of the surrounding environment. We describe the optical design, projection model and practical calibration using the incoming stream of events, of the novel stereo camera, called SFERA. The potential of SFERA for real-time target tracking is evaluated using a Bayesian estimator adapted to the geometry of the sphere. Real-world experiments with a prototype of SFERA, including two synchronized Prophesee EVK4 cameras and a DJI Mavic Air 2 quadrotor, show the effectiveness of the proposed system for aerial surveillance."
VSS-SLAM: Voxelized Surfel Splatting for Geometally Accurate SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128492,"[1] Visual Simultaneous Localization and Mapping (SLAM) helps robots estimate their poses and perceive the environment in unknown settings. Recent work has demonstrated that implicit neural radiance fields and 3D Gaussian Splatting (3DGS) offer higher fidelity scene representation than traditional map representations. We propose VSS-SLAM, which utilizes voxelized surfels as the map representation for incremental mapping in unknown environments. This representation effectively addresses the issue of redundant and disordered primitives encountered in previous methods, thereby enhancing geometric accuracy during reconstruction. Specifically, our approach divides the scene using voxels and stores geometric and appearance information in feature vectors at the voxel vertices. Before rendering, these feature vectors are decoded to generate the corresponding surfels. Additionally, we align camera poses through image and depth rendering. Extensive experiments on the Replica and TUM-RGBD datasets demonstrate that VSS-SLAM delivers high-fidelity reconstruction and accurate pose estimation in both simulated and real-world environments. Source code will soon be available."
EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial Awareness and Semantic Reasoning in Heterogeneous Environments.,https://doi.org/10.1109/ICRA55743.2025.11127594,"To ensure the efficiency of robot autonomy under diverse real-world conditions, a high-quality heterogeneous dataset is essential to benchmark the operating algorithms' performance and robustness. Current benchmarks predominantly focus on urban terrains, specifically for on-road autonomous driving, leaving multi-degraded, densely vegetated, dynamic and feature-sparse environments, such as underground tunnels, natural fields, and modern indoor spaces underrepresented. To fill this gap, we introduce EnvoDat, a large-scale, multi-modal dataset collected in diverse environments and conditions, including high illumination, fog, rain, and zero visibility at different times of the day. Overall, EnvoDat contains 26 sequences from 13 scenes, 10 sensing modalities, over 1.9TB of data, and over 89 K fine-grained polygon-based annotations for more than 82 object and terrain classes. We post-processed EnvoDat in different formats that support benchmarking SLAM and supervised learning algorithms, and fine-tuning multimodal vision models. With EnvoDat, we contribute to environment-resilient robotic autonomy in areas where the conditions are extremely challenging. The datasets and other relevant resources can be accessed through https://linusnep.github.io/EnvoDat/."
Weathergs: 3D Scene Reconstruction in Adverse Weather Conditions Via Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11128699,"D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGSbased framework for reconstructing clear scenes from multiview images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods. See project: https://jumponthemoon.github.io/weather-gs."
RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128103,"Sim-to-Real refers to the process of transferring policies learned in simulation to the real world, which is crucial for achieving practical robotics applications. However, recent Sim2real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks. In recent years, with the emergence of radiance field reconstruction methods, especially 3D Gaussian splatting, it has become possible to construct realistic real-world scenes. To this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim-to-real transfer for vision-based deep reinforcement learning. We introduce a mesh-based 3D GS method with soft binding constraints, enhancing the rendering quality of mesh models. Then utilizing a GS editing approach to synchronize the rendering with the physics simulator, RL-GSBridge could reflect the visual interactions of the physical robot accurately. Through a series of sim-to-real experiments, including grasping and pick-and-place tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in real-world task completion during sim-to-real transfer. Furthermore, a series of rendering metrics and visualization results indicate that our proposed mesh-based 3D GS reduces artifacts in unstructured objects, demonstrating more realistic rendering performance."
High-Quality 3D Creation From a Single Image Using Subject-Specific Knowledge Prior.,https://doi.org/10.1109/ICRA55743.2025.11127728,"In this paper, we address the critical bottleneck in robotics caused by the scarcity of diverse 3D data by presenting a novel two-stage approach for generating high-quality 3D models from a single image. This method is motivated by the need to efficiently expand 3D asset creation, particularly for robotics datasets, where the variety of object types is currently limited compared to general image datasets. Unlike previous methods that primarily rely on general diffusion priors, which often struggle to align with the reference image, our approach leverages subject-specific prior knowledge. By incorporating subject-specific priors in both geometry and texture, we ensure precise alignment between the generated 3D content and the reference object. Specifically, we introduce a shading modeaware prior into the NeRF optimization process, enhancing the geometry and refining texture in the coarse outputs to achieve superior quality. Extensive experiments demonstrate that our method significantly outperforms prior approaches."
DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127313,"Novel-view synthesis approaches play a critical role in vast scene reconstruction. However, these methods rely heavily on dense image inputs and prolonged training times, making them unsuitable where computational resources are limited. Additionally, few-shot methods often struggle with poor reconstruction quality in vast environments. This paper presents DGTR, a novel distributed framework for efficient Gaussian reconstruction for sparse-view vast scenes. Our approach divides the scene into regions, processed independently by drones with sparse image inputs. Using a feed-forward Gaussian model, we predict high-quality Gaussian primitives, followed by a global alignment algorithm to ensure geometric consistency. Depth priors is incorporated to further enhance training, while a distillation-based model aggregation mechanism enables efficient reconstruction. Our method achieves high-quality large-scale scene reconstruction and novel-view synthesis in significantly reduced training times, outperforming existing approaches in both speed and scalability. We demonstrate the effectiveness of our framework on vast aerial scenes, achieving high-quality results within minutes. Code will released on our project page https://3d-aigc.github.io/DGTR."
LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127587,"We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks. Project website with open-sourced code: https://sites.google.com/view/lidar-edit"
TransForce: Transferable Force Prediction for Vision-Based Tactile Sensors with Sequential Image Translation.,https://doi.org/10.1109/ICRA55743.2025.11127381,"Vision-based tactile sensors (VBTSs) provide highresolution tactile images crucial for robot in-hand manipulation. However, force sensing in VBTSs is underutilized due to the costly and time-intensive process of acquiring paired tactile images and force labels. In this study, we introduce a transferable force prediction model, TransForce, designed to leverage collected image-force paired data for new sensors under varying illumination colors and marker patterns while improving the accuracy of predicted forces, especially in the shear direction. Our model effectively achieves translation of tactile images from the source domain to the target domain, ensuring that the generated tactile images reflect the illumination colors and marker patterns of the new sensors while accurately aligning the elastomer deformation observed in existing sensors, which is beneficial to force prediction of new sensors. As such, a recurrent force prediction model trained with generated sequential tactile images and existing force labels is employed to estimate higher-accuracy forces for new sensors with lowest average errors of 0.69 N (5.8 % in full work range) in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$x$</tex>-axis, 0.70 N(5.8%) in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$y$</tex>-axis, and 1.11 N(6.9%) in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$z$</tex>-axis compared with models trained with single images. The experimental results also reveal that pure marker modality is more helpful than the RGB modality in improving the accuracy of force in the shear direction, while the RGB modality show better performance in the normal direction."
HumanFT: A Human-Like Fingertip Multimodal Visuo-Tactile Sensor.,https://doi.org/10.1109/ICRA55743.2025.11128300,"Tactile sensors play a crucial role in enabling robots to interact effectively and safely with objects in everyday tasks. In particular, visuotactile sensors have seen increasing usage in two and three-fingered grippers due to their high-quality feedback. However, a significant gap remains in the development of sensors suitable for humanoid robots, especially five-fingered dexterous hands. One reason is because of the challenges in designing and manufacturing sensors that are compact in size. In this paper, we propose HumanFT, a multimodal visuotactile sensor that replicates the shape and functionality of a human fingertip. To bridge the gap between human and robotic tactile sensing, our sensor features real-time force measurements, high-frequency vibration detection, and overtemperature alerts. To achieve this, we developed a suite of fabrication techniques for a new type of elastomer optimized for force propagation and temperature sensing. Besides, our sensor integrates circuits capable of sensing pressure and vibration. These capabilities have been validated through experiments. The proposed design is simple and cost-effective to fabricate. We believe HumanFT can enhance humanoid robots' perception by capturing and interpreting multimodal tactile information."
FeelAnyForce: Estimating Contact Force Feedback from Tactile Sensation for Vision-Based Tactile Sensors.,https://doi.org/10.1109/ICRA55743.2025.11127723,"In this paper, we tackle the problem of estimating 3D contact forces using vision-based tactile sensors. In particular, our goal is to estimate contact forces over a large range (up to 15 N) on any objects while generalizing across different vision-based tactile sensors. Thus, we collected a dataset of over 200K indentations using a robotic arm that pressed various indenters onto a GelSight Mini sensor mounted on a force sensor and then used the data to train a multi-head transformer for force regression. Strong generalization is achieved via accurate data collection and multi-objective optimization that leverages depth contact images. Despite being trained only on primitive shapes and textures, the regressor achieves a mean absolute error of 4% on a dataset of unseen real-world objects. We further evaluate our approach's generalization capability to other GelSight mini and DIGIT sensors, and propose a reproducible calibration procedure for other sensors. Finally, the method was evaluated on real-world tasks, including weighing objects and controlling the deformation of delicate objects. Supplementary material and demo are available at http://prg.cs.umd.edu/FeelAnyForce."
VITaL Pretraining: Visuo-Tactile Pretraining for Tactile and Non-Tactile Manipulation Policies.,https://doi.org/10.1109/ICRA55743.2025.11128336,"Tactile information is a critical tool for dexterous manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on manipulation tasks. We show that incorporating visuo-tactile pretraining improves imitation learning performance, not only for tactile agents (policies that use tactile information at inference), but also for non-tactile agents (policies that do not use tactile information at inference). For these non-tactile agents, pretraining with tactile information significantly improved performance (for example, improving the accuracy on USB plugging from 20% to 85%), reaching a level on par with visuo-tactile agents, and even surpassing them in some cases. For demonstration videos and access to our codebase, see the project website: https://sites.google.com/andrew.cmu.edu/visuo-tactile-pretraining"
Nezha-Mb: Design and Implementation of a Morphing Hybrid Aerial-Underwater Vehicle.,https://doi.org/10.1109/ICRA55743.2025.11127620,"Hybrid aerial underwater vehicles (HAUVs) exhibit significant potential due to their ability to operate seamlessly in the air and water domains. However, balancing rapid maneuverability in both media and achieving stability during the cross-domain phase remains a significant challenge. Inspired by the retractable limbs of a tortoise, this paper presented a novel morphing HAUV, Nezha-MB. Nezha-MB utilizes linear actuators combined with a gear and rack system for arm transformation during the transition phase, replacing conventional servos. The transformation mechanism accounts for 11 % of the total weight. In aerial mode, Nezha-MB exhibits flight performance comparable to a quadrotor configuration. In underwater mode, NezhaMB retracts its quadrotor arms into a bullet-shaped shell, significantly reducing drag and energy consumption, while enabling passage through narrow gaps with diameters as small as 134 mm. Simulations and field tests conducted in both aerial and underwater domains demonstrate that NezhaMB combines the swift maneuverability of a streamlined underwater vehicle with the stability of a traditional multirotor vehicle, highlighting its robust and rapid cross-domain capabilities."
From Ceilings to Walls: Universal Dynamic Perching of Quadrotors on Surfaces with Variable Orientations.,https://doi.org/10.1109/ICRA55743.2025.11128577,"This work demonstrates universal dynamic perching capabilities for quadrotors of various sizes and on surfaces with different orientations. By employing a non-dimensionalization framework and deep reinforcement learning, we systematically assessed how robot size and surface orientation affect landing capabilities. We hypothesized that maintaining geometric proportions across different robot scales ensures consistent perching behavior, which was validated in both simulation and experimental tests. Additionally, we investigated the effects of joint stiffness and damping in the landing gear on perching behaviors and performance. While joint stiffness had minimal impact, joint damping ratios influenced landing success under vertical approaching conditions. The study also identified a critical velocity threshold necessary for successful perching, determined by the robot's maneuverability and leg geometry. Overall, this research advances robotic perching capabilities, offering insights into the role of mechanical design and scaling effects, and lays the groundwork for future drone autonomy and operational efficiency in unstructured environments."
Towards Perpetually-Deployable Ubiquitous Aerial Robotics: An Amphibious Self-Sustainable Solar Small-UAS.,https://doi.org/10.1109/ICRA55743.2025.11127506,"This work deals with the problem of unlocking perpetual deployment capabilities for small-UAS robotics across the diverse settings of the real world and their challenges, encompassing considerations for marine environments alongside the more common terrestrial ones. Via the progress made within this scope, a step towards truly ubiquitous and selfsustainable aerial robotics is accomplished. The work consists of the development of the Gannet Solar-VTOL, a waterproof small-UAS that is capable of resting on the surface of water for prolonged periods of time and over varying temperature ranges, while harvesting solar power to recharge itself. Equally importantly, it integrates a field-proven Self-Sustainable Autonomous System architecture that allows it to hibernate and sustain its battery charge overnight or during periods of solar illumination scarcity, as well as to assess mission-critical parameters (e.g., water surface turbulence, ambient temperature of battery compartment) on the low-power side of the Power Management Stack, and react appropriately. Finally, the robot is equipped with an onboard camera and a Neural Processing Unit that allows it to perform in-field environmental monitoring operations (e.g., wildfire detection). This paper experimentally demonstrates the aforementioned capabilities, and concludes with a presentation of the amphibious small-UAS' long-term deployment within a marine environment in the N. Nevada region, spanning over 3 consecutive days."
Autonomous Drone for Dynamic Smoke Plume Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128144,"This paper presents a novel autonomous drone-based smoke plume tracking system capable of navigating and tracking plumes in highly unsteady atmospheric conditions. The system integrates advanced hardware and software and a comprehensive simulation environment to ensure robust performance in controlled and real-world settings. The quadrotor, equipped with a high-resolution imaging system and an advanced onboard computing unit, performs precise maneuvers while accurately detecting and tracking dynamic smoke plumes under fluctuating conditions. Our software implements a two-phase flight operation: descending into the smoke plume upon detection and continuously monitoring the smoke's movement during in-plume tracking. Leveraging Proportional Integral-Derivative (PID) control and a Proximal Policy Optimization (PPO) based Deep Reinforcement Learning (DRL) controller enables adaptation to plume dynamics. Unreal Engine simulation evaluates performance under various smoke-wind scenarios, from steady flow to complex, unsteady fluctuations, showing that while the PID controller performs adequately in simpler scenarios, the DRL-based controller excels in more challenging environments. Field tests corroborate these findings. This system opens new possibilities for drone-based monitoring in areas like wildfire management and air quality assessment. The successful integration of DRL for real-time decision-making advances autonomous drone control for dynamic environments."
EvMAPPER: High-Altitude Orthomapping with Event Cameras.,https://doi.org/10.1109/ICRA55743.2025.11128544,"Traditionally, unmanned aerial vehicles (UAVs) rely on CMOS-based cameras to collect images about the world below. One of the most successful applications of UAVs is to generate orthomosaics or orthomaps, in which a series of images are integrated to develop a larger map. However, using CMOS-based cameras with global or rolling shutters means that orthomaps are vulnerable to challenging light conditions, motion blur, and high-speed motion of independently moving objects (IMOs) under the camera. Event cameras are less sensitive to these issues, as their pixels trigger asynchronously on brightness changes. This work introduces the first orthomosaic approach using event cameras. We focus on addressing high-dynamic range and low-light problems in orthomosaics. In contrast to existing methods relying only on CMOS cameras, our approach enables map generation even in challenging light conditions, including direct sunlight and after sunset. The source code for EvMAPPER, the high-altitude hardware, and the dataset collected in this paper are available open source<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>https://evmapper.fcladera.com."
CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus.,https://doi.org/10.1109/ICRA55743.2025.11127779,"Collaborative perception, fusing information from multiple agents, can extend perception range so as to improve perception performance. However, temporal asynchrony in real-world environments, caused by communication delays, clock misalignment, or sampling configuration differences, can lead to information mismatches. If this is not well handled, then the collaborative performance is patchy, and what's worse safety accidents may occur. To tackle this challenge, we propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception framework that is robust to the information mismatches caused by temporal asynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each region of interest by modeling aleatoric and epistemic uncertainty as well as selectively suppressing or retaining single-vehicle features, thereby mitigating information mismatches. We then design a multi-scale fusion module to handle multi-scale feature maps processed by DFTM. Compared to existing works that also consider asynchronous collaborative perception, CoDynTrust combats various low-quality information in temporally asynchronous scenarios and allows uncertainty to be propagated to downstream tasks such as planning and control. Experimental results demonstrate that CoDynTrust significantly reduces performance degradation caused by temporal asynchrony across multiple datasets, achieving state-of-the-art detection performance even with temporal asynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust."
The Devil is in the Quality: Exploring Informative Samples for Semi-Supervised Monocular 3D Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11128376,"This paper tackles the challenging problem of semi-supervised monocular 3D object detection with a general framework. In specific, having observed that the bottleneck of this task lies in lacking reliable and informative samples from unlabeled data for detector learning, we introduce a novel simple yet effective Augment and Criticize pipeline that mines abundant informative samples for robust detection. To be more specific, in the Augment stage, we present the Augmentation-based Prediction aGgregation (APG), which applies automatically learned transformations to unlabeled images and aggregates detections from various augmented views as pseudo labels. Since not all the pseudo labels from APG are beneficially informative, the subsequent Criticize phase is introduced. Particularly, we present the Critical Retraining Strategy (CRS) that, unlike simply filtering pseudo labels using a fixed threshold, employs a learnable network to evaluate the contribution of unlabeled images at different training timestamps. This way, the noisy samples prohibitive to model evolution can be effectively suppressed. In order to validate Augment-Criticize, we apply it to MonoDLE [1] and MonoFlex [2], and the two new detectors, dubbed 3DSeMo<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">DLE</inf> and 3DSeMo<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">FLEX</inf>, achieve state-of-the-art results with consistent improvements, evidencing its effectiveness and generality."
MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent Teacher Models.,https://doi.org/10.1109/ICRA55743.2025.11127874,"We tackle the problem of monocular 3D object detection across different sensors, environments, and camera setups. In this paper, we introduce a novel unsupervised domain adaptation approach, MonoCT, that generates highly accurate pseudo labels for self-supervision. Inspired by our observation that accurate depth estimation is critical to mitigating domain shifts, MonoCT introduces a novel Generalized Depth Enhancement (GDE) module with an ensemble concept to improve depth estimation accuracy. Moreover, we introduce a novel Pseudo Label Scoring (PLS) module by exploring inner-model consistency measurement and a Diversity Maximization (DM) strategy to further generate high-quality pseudo labels for self-training. Extensive experiments on six benchmarks show that MonoCT outperforms existing SOTA domain adaptation methods by large margins (~21% minimum for AP Mod.) and generalizes well to car, traffic camera and drone views."
LiDAR Inertial Odometry and Mapping Using Learned Registration-Relevant Features.,https://doi.org/10.1109/ICRA55743.2025.11127666,"SLAM is an important capability for many autonomous systems, and modern LiDAR-based methods offer promising performance. However, for long duration missions, existing works that either take directly the full pointclouds or extracted features face key tradeoffs in accuracy and computational efficiency (e.g., memory consumption). To address these issues, this paper presents DFLIOM with several key innovations. Unlike previous methods that rely on handcrafted heuristics and hand-tuned parameters for feature extraction, we propose a learning-based approach that select points relevant to LiDAR SLAM pointcloud registration. Furthermore, we extend our prior work DLIOM with the learned feature extractor and observe our method enables similar or even better localization performance using only about 20% of the points in the dense point clouds. We demonstrate that DFLIOM performs well on multiple public benchmarks, achieving a 2.4% decrease in localization error and 57.5% decrease in localization error and 57.5 % decrease in memory usage compared to state-of-the-art methods (DLIOM). Although extracting features with the proposed network requires extra time, it is offset by the faster processing time downstream, thus maintaining real-time performance using 20 Hz LiDAR on our hardware setup. The effectiveness of our learning-based feature extraction module is further demonstrated through comparison with several handcrafted feature extractors."
DreamDrive: Generative 4D Scene Modeling from Street View Images.,https://doi.org/10.1109/ICRA55743.2025.11127463,"Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and in-the-wild driving data demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving."
"Key-Scan-Based Mobile Robot Navigation: Integrated Mapping, Planning, and Control Using Graphs of Scan Regions.",https://doi.org/10.1109/ICRA55743.2025.11128158,"Safe autonomous navigation in a priori unknown environments is an essential skill for mobile robots to reliably and adaptively perform diverse tasks (e.g., delivery, inspection, and interaction) in unstructured cluttered environments. Hybrid metric-topological maps, constructed as a pose graph of local submaps, offer a computationally efficient world representation for adaptive mapping, planning, and control at the regional level. In this paper, we consider a pose graph of locally sensed star-convex scan regions as a metric-topological map, with star convexity enabling simple yet effective local navigation strategies. We design a new family of safe local scan navigation policies and present a perception-driven feedback motion planning method through the sequential composition of local scan navigation policies, enabling provably correct and safe robot navigation over the union of local scan regions. We introduce a new concept of bridging and frontier scans for automated key scan selection and exploration for integrated mapping and navigation in unknown environments. We demonstrate the effectiveness of our key-scan-based navigation and mapping framework using a mobile robot equipped with a 360 laser range scanner in 2D cluttered environments through numerical ROS-Gazebo simulations and real hardware experiments."
Angular Divergent Component of Motion: A Step Towards Planning Spatial DCM Objectives for Legged Robots.,https://doi.org/10.1109/ICRA55743.2025.11128476,"In this work, the Divergent Component of Motion (DCM) method is expanded to include angular coordinates for the first time. This work introduces the idea of spatial DCM, which adds an angular objective to the existing linear DCM theory. To incorporate the angular component into the framework, a discussion is provided on extending beyond the linear motion of the Linear Inverted Pendulum model (LIPM) towards the Single Rigid Body model (SRBM) for DCM. This work presents the angular DCM theory for a 1D rotation, simplifying the SRBM rotational dynamics to a flywheel to satisfy necessary linearity constraints. The 1D angular DCM is mathematically identical to the linear DCM and defined as an angle which is ahead of the current body rotation based on the angular velocity. This theory is combined into a 3D linear and 1D angular DCM framework, with discussion on the feasibility of simultaneously achieving both sets of objectives. A simulation in MATLAB and hardware results on the TORO humanoid are presented to validate the framework's performance."
Finite-Step Capturability and Recursive Feasibility for Bipedal Walking in Constrained Regions.,https://doi.org/10.1109/ICRA55743.2025.11128831,"This paper presents a Model Predictive Control (MPC) formulation for bipedal footstep planning based on the Linear Inverted Pendulum (LIP) model, ensuring recursive feasibility when navigating restricted regions. The proposed approach incorporates capturability and introduces a new constraint that forces the Divergent Component of Motion (DCM) into a finite-step capture region, adjusted between consecutive MPC calls. This constraint enables the MPC to anticipate beyond its prediction horizon, preventing collisions with the walking surface boundaries. We validate the approach through high-fidelity simulations with the bipedal robot Digit, demonstrating recursively feasible MPC footstep planning in restricted regions. Future efforts will extend the approach to general polytopic constraints, thereby facilitating footstep planning in cluttered environments while preserving the MPC's recursive feasibility."
Realtime Limb Trajectory Optimization for Humanoid Running Through Centroidal Angular Momentum Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11127382,"One of the essential aspects of humanoid robot running is determining the limb-swinging trajectories. During the flight phases, where the ground reaction forces are not available for regulation, the limb swinging trajectories are significant for the stability of the next stance phase. Due to the conservation of angular momentum, improper leg and arm swinging results in highly tilted and unsustainable body configurations at the next stance phase landing. In such cases, the robotic system fails to maintain locomotion independent of the stability of the center of mass trajectories. This problem is more apparent for fast and high flight time trajectories. This paper proposes a real-time nonlinear limb trajectory optimization problem for humanoid running. The optimization problem is tested on two different humanoid robot models, and the generated trajectories are verified using a running algorithm for both robots in a simulation environment."
Pitching Motion in a Humanoid Robot Using Human-Inspired Shoulder Elastic Energy and Motor Torque Optimization.,https://doi.org/10.1109/ICRA55743.2025.11127290,"Humanoid robots that mimic human movement have garnered significant attention in recent years. This study focuses on mimicking the efficient pitching motion of humans by incorporating two main approaches into a humanoid robot: (1) the use of elastic elements to assist joint torque, and (2) the optimization of motor torque to minimize energy consumption. This robot is intended to emulate human physical characteristics, such as mass, link length, and center of gravity, with a particular focus on utilizing the elastic energy generated during shoulder internal and external rotation. A leaf spring is attached in parallel with the motor at the shoulder pitch joint to release the elastic energy stored during shoulder external rotation, thereby assisting internal rotation in a manner similar to human biomechanics. Additionally, motor torque optimization is addressed by formulating the torque minimization problem as a combinatorial optimization challenge and solving it using Fujitsu's quantum-inspired Digital Annealer. Experiments conducted through simulations and with an actual pitching robot assessed the effectiveness of these technologies in mimicking human-like pitching motion. The results suggest that combining elastic elements with motion optimization techniques enable robots to achieve more efficient human-like movements."
"Single-Stage Optimization of Open-Loop Stable Limit Cycles with Smooth, Symbolic Derivatives.",https://doi.org/10.1109/ICRA55743.2025.11128720,"Open-loop stable limit cycles are foundational to legged robotics, providing inherent self-stabilization that minimizes the need for computationally intensive feedback-based gait correction. While previous methods have primarily targeted specific robotic models, this paper introduces a general framework for rapidly generating limit cycles across various dynamical systems, with the flexibility to impose arbitrarily tight stability bounds. We formulate the problem as a single-stage constrained optimization problem and use Direct Collocation to transcribe it into a nonlinear program with closed-form expressions for constraints, objectives, and their gradients. Our method supports multiple stability formulations. In particular, we tested two popular formulations for limit cycle stability in robotics: (1) based on the spectral radius of a discrete return map, and (2) based on the spectral radius of the monodromy matrix, and tested five different constraintsatisfaction formulations of the eigenvalue problem to bound the spectral radius. We compare the performance and solution quality of the various formulations on a robotic swing-leg model, highlighting the Schur decomposition of the monodromy matrix as a method with broader applicability due to weaker assumptions and stronger numerical convergence properties. As a case study, we apply our method on a hopping robot model, generating open-loop stable gaits in under 2 seconds on an Intel<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""></sup>Core i7-6700K, while simultaneously minimizing energy consumption even under tight stability constraints."
A Robust Deep Reinforcement Learning Framework for Image-Based Autonomous Guidewire Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127741,"Percutaneous coronary intervention (PCI) involves the insertion of a catheter or guidewire into a blood vessel of a patient, which poses a problem as a doctor is exposed to radiation during the procedure. The use of assistive robots has been proposed to address this issue. Furthermore, recent research is progressing toward complete autonomous navigation using deep reinforcement learning (DRL). Nevertheless, existing algorithms face limitations when operating in numerous unseen environments close to real PCI. This study proposes a robust DRL framework for image-based guidewire navigation to overcome the limitation. We introduce a subtasks strategy and domain randomization to improve robustness in various environments. The subtasks strategy consistently addresses complex global tasks by breaking them into subtasks designed using local maps, allowing them to be robustly solved by a single agent. Domain randomization is applied to handle real PCI issues, including variations in vessel geometry, guidewire deformation, and camera settings. By integrating the two novel methods, our DRL algorithm demonstrates superior performance compared to existing methods across various challenging simulation and phantom environments, validating its effectiveness in real-world scenarios. A video of our experiment is available at https://youtu.be/93Q88gESzOY."
CTS: A Consistency-Based Medical Image Segmentation Model.,https://doi.org/10.1109/ICRA55743.2025.11128236,"In medical image segmentation tasks, diffusion models have exhibited significant potential. However, mainstream diffusion models show drawbacks including multiple sampling times and slow prediction results. Recently, as a standalone generative network, consistency models have resolved the existing issue. Compared to diffusion models, consistency models can lower the sampling times to once, not only achieving similar generative effects but also significantly accelerating training and prediction. However, they are not suitable for image segmentation tasks. Meanwhile, their application in the medical imaging field has not yet been investigated. Therefore, this study employs the consistency model to perform medical image segmentation tasks, designing multi-scale feature signal supervision modes and loss function guidance to realize model convergence. Experiments have demonstrated that the CTS model is capable of obtaining better medical image segmentation results with a single sampling during the test phase."
An Adversarial Learning Framework for Reliable Myoelectric Force Estimation Under Fatigue.,https://doi.org/10.1109/ICRA55743.2025.11128839,"Electromyography (EMG) signals are widely used as control inputs for myoelectric exoskeletons. However, muscle fatigue, which can result from prolonged use or heavy loads, significantly affects muscle activation patterns, leading to reduced estimation accuracy. To address this challenge, we propose an adversarial learning framework to enhance grip force estimation under fatigue conditions. The framework consists of three key components: a domain-invariant feature extractor to mitigate domain shifts between non-fatigue and fatigue states, a force estimator to predict grip forces from these domain-invariant features, and a domain discriminator to distinguish between the two domains. The proposed method was evaluated on a dataset collected from eight participants performing gripping tasks under both non-fatigue and fatigue conditions, during which high-density EMG signals and grip forces were recorded simultaneously. Experimental results demonstrated that our method significantly reduced the root mean square error (RMSE) from 0.264 to 0.127, outperforming a baseline model consisting of only the feature extractor and force estimator <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(p &lt; 0.01)$</tex>. Additionally, the proposed approach exhibited consistent performance across all participants, highlighting its robustness and generalizability. These findings suggest that the proposed adversarial learning framework effectively enhances grip force estimation accuracy under muscle fatigue, offering a promising solution for improving the reliability and usability of myoelectric exoskeletons."
RACE: A Fast and Lightweight Urban Exploration and Search Strategy for Multi-Robot Systems.,https://doi.org/10.1109/ICRA55743.2025.11128833,"Multi-Robot Systems (MRS) are increasingly de-ployed for hazardous tasks in urban environments. Among many tasks, search and rescue remains challenging as it deals with exploration in an unknown indoor constrained environ-ment. For example, without global knowledge of the map of a building floor, it is not advantageous to choose one path over another at a corridor junction. Also, if the assigned frontiers are far from the robot, backtracking along a corridor will cost more than moving forward. Since exploration along corridors is similar to solving a maze, this paper examines classical maze-solving algorithms that are known to be computationally fast and lightweight, such as the Right Hand Rule (RHR), Random Mouse (RM), and more. The authors have identified two gaps that need to be addressed before these algorithms can be applied to physical MRS. Firstly, these algorithms are not designed for the cooperation of multiple agents in exploration. Secondly, they are often applied to only a low-fidelity simulation environment, which requires some work to make these algorithms transferable to work in the commonly used occupancy grid map environment. In this paper, the authors introduced RACE, a fast and lightweight collective urban exploration and search algorithm based on a modified and condensed version of the Ant Colony Optimization (ACO) algorithm. The proposed solution is successfully verified in a low-fidelity simulation, evaluated against other exploration and search algorithms like RHR and RM. An innovative approach of RACE Simulation to Physical implementation is presented and a physical system evaluation is performed to evaluate RACE against a Rapidly-Exploring Random Tree algorithm. Finally, the proposed solution is further verified with a physical experiment, in which a quadrupedal robot is assigned to explore part of a floor of SUTD, spanning approximately <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(55m \times 40m)$</tex>. RACE also showed potential in handling challenging closed-loop and dead-end environments."
Path Planning Using Instruction-Guided Probabilistic Roadmaps.,https://doi.org/10.1109/ICRA55743.2025.11127823,"This work presents a novel data-driven path planning algorithm named Instruction-Guided Probabilistic Roadmap (IG-PRM). Despite the recent development and widespread use of mobile robot navigation, the safe and effective travels of mobile robots still require significant engineering effort to take into account the constraints of robots and their tasks. With IG-PRM, we aim to address this problem by allowing robot operators to specify such constraints through natural language instructions, such as aim for wider paths or mind small gaps. The key idea is to convert such instructions into embedding vectors using large-language models (LLMs) and use the vectors as a condition to predict instruction-guided cost maps from occupancy maps. By constructing a roadmap based on the predicted costs, we can find instruction-guided paths via the standard shortest path search. Experimental results demonstrate the effectiveness of our approach on both synthetic and real-world indoor navigation environments."
Pushing Through Clutter with Movability Awareness of Blocking Obstacles.,https://doi.org/10.1109/ICRA55743.2025.11127788,"Navigation Among Movable Obstacles (NAMO) poses a challenge for traditional path-planning methods when obstacles block the path, requiring push actions to reach the goal. We propose a framework that enables movability-aware planning to overcome this challenge without relying on explicit obstacle placement. Our framework integrates a global Semantic Visibility Graph and a local Model Predictive Path Integral (SVG-MPPI) approach to efficiently sample rollouts, taking into account the continuous range of obstacle movability. A physics engine is adopted to simulate the interaction result of the rollouts with the environment, and generate trajectories that minimize contact force. In qualitative and quantitative experiments, SVG-MPPI outperforms the existing paradigm that uses only binary movability for planning, achieving higher success rates with reduced cumulative contact forces. Our code is available at: https://github.com/tud-amrISVG-MPPI"
Improving Efficiency in Path Planning: Tangent Line Decomposition Algorithm.,https://doi.org/10.1109/ICRA55743.2025.11128751,"This paper introduces a tangent line decomposition (TLD) algorithm that efficiently finds collision-free paths close to optimal in both 2D and 3D environments. Compared with the existing visibility line-based algorithms, the proposed algorithm innovatively proposed the concept of tangent line decomposition, which decomposes complicated planning into many simple steps. For each step, only one key obstacle is taken into consideration. Besides, instead of constructing a complete graph, a best-first search algorithm is used to avoid searching redundant edges. The path planned by the algorithm is not the optimal path. However, following the idea of the informed RRT* algorithm, the path length planned by TLD can be used as a precondition for other optimal algorithms. In this way, the overall efficiency can be significantly improved. The simulations show that the proposed methods outperform existing methods regarding planning efficiency and solution quality."
Gradient Guided Search for Aircraft Contingency Landing Planning.,https://doi.org/10.1109/ICRA55743.2025.11127796,"This paper presents a three-dimensional discrete search path planner for fixed-wing aircraft emergency landing planning that manages state-space complexity by incorporating cost gradients to assure descent flight path angle and runway heading alignment constraints are met. Our approach incorporates steady wind and maximizes margin from flight envelope boundaries to accommodate wind variation in a manner commensurate with a loss of thrust condition. A novel multi-objective cost function that combines gradient-based path guidance and population risk metrics is implemented to efficiently enable discrete search to find a robust solution. The proposed method is demonstrated through use cases with population data for a region of Long Island, New York that highlight our algorithm's effectiveness."
Search-Based Path Planning in Interactive Environments Among Movable Obstacles.,https://doi.org/10.1109/ICRA55743.2025.11128242,"This paper investigates Path planning Among Movable Obstacles (PAMO), which seeks a minimum cost collision-free path among static obstacles from start to goal while allowing the robot to push away movable obstacles (i.e., objects) along its path when needed. To develop planners that are complete and optimal for PAMO, the planner has to search a giant state space involving both the location of the robot as well as the locations of the objects, which grows exponentially with respect to the number of objects. This paper leverages a simple yet under-explored idea that, only a small fraction of this giant state space needs to be searched during planning as guided by a heuristic, and most of the objects far away from the robot are intact, which thus leads to runtime efficient algorithms. Based on this idea, this paper introduces two PAMO formulations, i.e., bi-objective and resource constrained problems in an occupancy grid, and develops PAMO*, a planning method with completeness and solution optimality guarantees, to solve the two problems. We then further extend PAMO* to hybrid-state PAMO* to plan in continuous spaces with high-fidelity interaction between the robot and the objects. Our results show that, PAMO* can often find optimal solutions within a second in cluttered maps with up to 400 objects."
Neural Encodings for Energy-Efficient Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127969,"Neural motion planners can increase motion planning quality and, by reducing collision detection computations, improve runtime. However, when profiled on an accelerator-rich hardware system, neural planning contributes to more than 50% of the runtime, and 33% of the computation energy consumption, motivating the design of compute- and energy-efficient neural planners. In this work, we propose a neural planner using Binary Encoded Labels (BEL), where a set of binary classifiers are used instead of a typical regression network. Compared to conventional regression-based neural planners, the proposed BEL neural planner reduces neural planning (inference) computation and collision detection checks while maintaining equal or higher motion planning success rate across various motion planning benchmarks. This computation reduction can improve the computation energy efficiency of neural planning by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1.4 \times-21.4 \times$</tex>. Finally, we demonstrate the trade-offs between collision detection and neural planning computation to maximize energy efficiency for different hardware configurations."
Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater Environments with a Swarm of Micro-Robots.,https://doi.org/10.1109/ICRA55743.2025.11128089,"Long-term monitoring and exploration of extreme environments, such as underwater storage facilities, is costly, labor-intensive, and hazardous. Automating this process with low-cost, collaborative robots can greatly improve efficiency. These robots capture images from different positions, which must be processed simultaneously to create a spatio-temporal model of the facility. In this paper, we propose a novel approach that integrates data simulation, a multi-modal deep learning network for coordinate prediction, and image reassembly to address the challenges posed by environmental disturbances causing drift and rotation in the robots' positions and orientations. Our approach enhances the precision of alignment in noisy environments by integrating visual information from snapshots, global positional context from masks, and noisy coordinates. We validate our method through extensive experiments using synthetic data that simulate real-world robotic operations in underwater settings. The results demonstrate very high coordinate prediction accuracy and plausible image assembly, indicating the real-world applicability of our approach. The assembled images provide clear and coherent views of the underwater environment for effective monitoring and inspection, showcasing the potential for broader use in extreme settings, further contributing to improved safety, efficiency, and cost reduction in hazardous field monitoring."
CapBot: Enabling Battery-Free Swarm Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127301,"Swarm robotics focuses on designing and coordinating large groups of relatively simple robots to perform tasks in a decentralised and collective manner. The swarm provides a resilient and flexible solution for many applications. However, contemporary swarm robots have a significant power problem in that secondary (i.e. rechargeable) batteries are slow to charge and offer lifetimes of only a few years, increasing maintenance costs and pollution due to battery replacement. We imagine a different future, wherein battery-free robots powered by supercapacitors can be recharged in seconds, offer long-life autonomous operation and can rapidly pass charge between one another using trophallaxis. In pursuit of this vision, we contribute the CapBot, a battery-free swarm robot equipped with Mecanum wheels, a Cortex M4F application processor and Bluetooth Low Energy networking. The CapBot fully recharges in 16 s, offers 51 min of autonomous operation at top speed, and can transfer up to 50 % of its available charge to a peer via trophallaxis in under 20 s. The CapBot is fully open source and all software and hardware source is available online."
Express Yourself: Enabling Large-Scale Public Events Involving Multi-Human-Swarm Interaction for Social Applications with MOSAIX.,https://doi.org/10.1109/ICRA55743.2025.11128216,"Robot swarms have the potential to help groups of people with social tasks, given their ability to scale to large numbers of robots and users. Developing multi-human-swarm interaction is therefore crucial to support multiple people interacting with the swarm simultaneously - which is an area that is scarcely researched, unlike single-human, single-robot or single-human, multi-robot interaction. Moreover, most robots are still confined to laboratory settings. In this paper, we present our work with MOSAIX, a swarm of robot Tiles, that facilitated ideation at a science museum. 63 robots were used as a swarm of smart sticky notes, collecting input from the public and aggregating it based on themes, providing an evolving visualization tool that engaged visitors and fostered their participation. Our contribution lies in creating a large-scale (63 robots and 294 attendees) public event, with a completely decentralized swarm system in real-life settings. We also discuss learnings we obtained that might help future researchers create multi-human-swarm interaction with the public."
MochiSwarm: A Testbed for Robotic Micro-Blimps in Realistic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127424,"Efficient energy management and scalability are critical for aerial robots in tasks such as pickup-and-delivery and surveillance. This paper introduces MochiSwarm, an open-source testbed of light-weight micro robotic blimps designed for multi-robot operation without external localization. We propose a modular system architecture that integrates adaptable hardware, a flexible software framework, and a detachable perception module. The hardware is designed to allow for rapid modifications and sensor integration, while the software supports multiple actuation models and robust communication between a base station and multiple blimps. We showcase a differential-drive module as an example, in which autonomy is enabled by visual servoing using the perception module. A case study of pickup-and-delivery tasks with up to 12 blimps highlights the autonomy of the MochiSwarm without relying on external infrastructures."
KFCalibNet: A KansFormer-Based Self-Calibration Network for Camera and LiDAR.,https://doi.org/10.1109/ICRA55743.2025.11127566,"In autonomous driving and robotic navigation, multi-sensor fusion technology has become increasingly mainstream, with precise sensor calibration as its foundation. Traditional calibration methods rely on manual effort or specific targets, limiting adaptability to complex environments. Learning-based calibration methods still face challenges, such as insufficient overlap between the fields of view (FoV) of multiple sensors and suboptimal cross-modal feature association, which hinder accurate parameter regression. Unlike traditional CNN-based networks, we propose a KansFormer-based self-Calibration Network for camera and LiDAR (KFCalibNet) that replaces fixed activation functions and linear transformations with learnable nonlinear activation functions. This enables the extraction of more fine-grained features from both image and point cloud, significantly enhancing the network's robustness in scenarios with limited FoV overlap. We also employ a multihead attention (MHA) module to compute correlations between image and point cloud features, significantly enhancing cross-modal feature association. To reduce learning complexity, we designed KansFormer with FastKAN as the feedforward network, enabling deep fusion and regression of fine-grained cross-modal features for accurate extrinsic calibration. KFCalibNet achieves an absolute average calibration error of 0.0965 cm in translation and 0.0234 in rotation on the KITTI Odometry dataset, outperforming existing state-of-the-art calibration methods. Moreover, its accuracy and generalization capability have been validated across multiple real-world railway lines."
Inducing Matrix Sparsity Bias for Improved Dynamic Identification of Parallel Kinematic Manipulators using Deep Learning.,https://doi.org/10.1109/ICRA55743.2025.11128257,"Among the many challenges of parallel kinematic manipulators, achieving high-speed and accurate control remains crucial. Estimating their dynamic properties is essential for designing precise and efficient control schemes. Conventional methods for dynamic model identification have been effective, though deep learning approaches have historically faced limitations due to data inefficiencies. However, recent advancements in physics-informed neural networks (PINNs) offer a way to improve both control and the extraction of interpretable physical properties from these robots. In this work, we propose and validate a PINN-based dynamic model for a Delta parallel robot, specifically the ABB IRB 360-6/1600. Our approach incorporates known physical properties, such as mass matrix sparsity, to improve accuracy and computational efficiency in dynamic model identification. To the best of our knowledge, this is the first study applying PINNs to model parallel robots. The method is validated experimentally, and its performance is compared to a validated identification technique for physically consistent identification, demonstrating the effectiveness of this approach for real-world applications in parallel robots."
Infield Self-Calibration of Intrinsic Parameters for Two Rigidly Connected IMUs.,https://doi.org/10.1109/ICRA55743.2025.11128439,"This paper presents a study on the infield self-calibration of two rigidly connected IMUs' intrinsic parameters, without the aid of any external sensors, equipment, or specialized procedures. Specifically, we consider the calibration of gyroscope biases, gyroscope scale factors, and accelerometer biases, using only IMU data and known extrinsics between the two IMUs. We focus on the observability analysis of this system, and show that all gyroscope intrinsic parameters and a portion of accelerometer biases are observable, with information from both IMUs and sufficient motion. Moreover, we identify the additional unobservable directions in the intrinsic parameters that arise from various degenerate motions. Finally, we validate our observability findings through numerical simulations, and assess our system's calibration accuracy using real-world data."
PlaneHEC: Efficient Hand-Eye Calibration for Multi-View Robotic Arm via Any Point Cloud Plane Detection.,https://doi.org/10.1109/ICRA55743.2025.11127326,"Hand-eye calibration is an important task in vision-guided robotic systems and is crucial for determining the transformation matrix between the camera coordinate system and the robot end-effector. Existing methods, for multi-view robotic systems, usually rely on accurate geometric models or manual assistance, generalize poorly, and can be very complicated and inefficient. Therefore, in this study, we propose PlaneHEC, a generalized hand-eye calibration method that does not require complex models and can be accomplished using only depth cameras, which achieves the optimal and fastest calibration results using arbitrary planar surfaces like walls and tables. PlaneHEC introduces hand-eye calibration equations based on planar constraints, which makes it strongly interpretable and generalizable. PlaneHEC also uses a comprehensive solution that starts with a closed-form solution and improves it with iterative optimization, which greatly improves accuracy. We comprehensively evaluated the performance of PlaneHEC in both simulated and real-world environments and compared the results with other point-cloud-based calibration methods, proving its superiority. Our approach achieves universal and fast calibration with an innovative design of computational models, providing a strong contribution to the development of multi-agent systems and embodied intelligence."
Legged Robot State Estimation with Invariant Extended Kalman Filter Using Neural Measurement Network.,https://doi.org/10.1109/ICRA55743.2025.11127971,"This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters with deep neural networks. In environments where vision systems are not reliable, proprioceptive state estimators become indispensable. Traditionally, proprioceptive state estimators are based on model-based approaches, which rely solely on contact foot kinematics as measurements. In contrast, learning-based approaches have obtained new measurements, such as displacement and covariance, by leveraging real-world data in a supervised manner. In this work, we develop a state estimation framework that trains a neural measurement network (NMN) to estimate the base's linear velocity and foot contact probability, which are then employed as measurements in an invariant extended Kalman filter. Our approach relies solely on simulation data for training, as it allows us to obtain extensive data easily. We address the sim-to-real gap by adapting existing learning techniques and regularization. To validate our proposed method, we conduct hardware experiments using a quadruped robot on four types of terrain: flat, debris, soft, and slippery. In our experiments, the proposed method demonstrates significant improvements over the model-based state estimator, achieving an average reduction in Absolute Trajectory Error (ATE) by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">${6 1. 8 \%}$</tex> for position and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">${8. 5 \%}$</tex> for velocity."
Physically-Consistent Parameter Identification of Robots in Contact.,https://doi.org/10.1109/ICRA55743.2025.11128710,"Accurate inertial parameter identification is crucial for the simulation and control of robots encountering intermittent contacts with the environment. Classically, robots' inertial parameters are obtained from CAD models that are not precise (and sometimes not available, e.g., Spot from Boston Dynamics), hence requiring identification. To do that, existing methods require access to contact force measurement, a modality not present in modern quadruped and humanoid robots. This paper presents an alternative technique that utilizes joint current/torque measurements -a standard sensing modality in modern robots- to identify inertial parameters without requiring direct contact force measurements. By projecting the whole-body dynamics into the null space of contact constraints, we eliminate the dependency on contact forces and reformulate the identification problem as a linear matrix inequality that can handle physical and geometrical constraints. We compare our proposed method against a common black-box identification method using a deep neural network and show that incorporating physical consistency significantly improves the sample efficiency and generalizability of the model. Finally, we validate our method on the Spot quadruped robot across various locomotion tasks, showcasing its accuracy and generalizability in real-world scenarios over different gaits."
Contact Force Estimation for a Leg-Wheel Transformable Robot With Varying Contact Points.,https://doi.org/10.1109/ICRA55743.2025.11127416,"Accurate estimation of contact forces is crucial for effective control of quadrupedal robots, especially in complex locomotion scenarios. In this paper, we introduce a novel force estimation technique for robots equipped with transformable leg-wheels. Unlike conventional methods that focus on forces at specific contact points, our approach expresses varying contact points through a simplified kinematic model and derives the corresponding Jacobian matrices. This allows us to apply the virtual work method to evaluate contact forces across the entire surface of the leg-wheel, including the tips, sides, and other contact regions. This adaptability is particularly advantageous in hybrid locomotion modes, where different parts of the leg-wheel interact with the terrain. The proposed method is highly efficient, relying solely on motor current and position feedback without the need for additional sensors. We validate our approach through simulations and real-world experiments, demonstrating its accuracy, robustness, and applicability under diverse operational conditions."
Simultaneous Collision Detection and Force Estimation for Dynamic Quadrupedal Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128205,"In this paper we address the simultaneous collision detection and force estimation problem for quadrupedal locomotion using joint encoder information and the robot dynamics only. We design an interacting multiple-model Kalman filter (IMM-KF) that estimates the external force exerted on the robot and multiple possible contact modes. The method is invariant to any gait pattern design. Our approach leverages pseudo-measurement information of the external forces based on the robot dynamics and encoder information. Based on the estimated contact mode and external force, we design a reflex motion and an admittance controller for the swing leg to avoid collisions by adjusting the leg's reference motion. Additionally, we implement a force-adaptive model predictive controller to enhance balancing. Simulation ablatation studies and experiments show the efficacy of the approach."
PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter.,https://doi.org/10.1109/ICRA55743.2025.11128475,"In critical applications, including search-and-rescue in degraded environments, blockages can be prevalent and prevent the effective deployment of certain sensing modalities, particularly vision, due to occlusion and the constrained range of view of onboard camera sensors. To enable robots to tackle these challenges, we propose a new approach, Proprioceptive Obstacle Detection and Estimation while navigating in clutter (PROBE), which instead relies only on the robot's proprioception to infer the presence or absence of occluded rectangular obstacles while predicting their dimensions and poses in SE (2). The proposed approach is a Transformer neural network that receives as input a history of applied torques and sensed whole-body movements of the robot and returns a parameterized representation of the obstacles in the environment. The effectiveness of PROBE is evaluated on simulated environments in Isaac Gym and with a real Unitree Go1 quadruped robot. The project webpage can be found at https://dhruvmetha.github.io/legged-probe/."
"Elderly Bodily Assistance Robot (E-BAR): A Robot System for Body-Weight Support, Ambulation Assistance, and Fall Catching, Without the Use of a Harness.",https://doi.org/10.1109/ICRA55743.2025.11127403,"As over 11,000 people turn 65 each day in the U.S., our country, like many others, is facing growing challenges in caring for elderly persons, further exacerbated by a major shortfall of care workers. To address this, we introduce an elder-care robot (E-BAR) capable of lifting a human body, assisting with postural changes/ambulation, and catching a user during a fall, all without the use of any wearable device or harness. Our robot is the first to integrate these 3 tasks, and is capable of lifting the full weight of a human outside of the robot's base of support (across gaps and obstacles). In developing E-BAR, we interviewed nurses and care professionals and conducted userexperience tests with elderly persons. Based on their functional requirements, the design parameters were optimized using a computational model and trade-off analysis. We developed a novel 18-bar linkage to lift a person from a floor to a standing position along a natural trajectory, while providing maximal mechanical advantage at key points. An omnidirectional, nonholonomic drive base, in which the wheels could be oriented to passively maximize floor grip, enabled the robot to resist lateral forces without active compensation. With a minimum width of 38 cm, the robot's small footprint allowed it to navigate the typical home environment. Four airbags were used to catch and stabilize a user during a fall in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\leq \mathbf{2 5 0 ~ m s}$</tex>. We demonstrate E-BAR's utility in multiple typical home scenarios, including getting into/out of a bathtub, bending to reach for objects, sit-to-stand transitions, and ambulation."
A Cane-Mounted System for Dynamic Orientation Prediction for Correcting Incorrect Cane-Tapping by Visually Challenged Persons.,https://doi.org/10.1109/ICRA55743.2025.11127542,"People with visual impairments rely on Electronic Travel Aids (ETAs), such as sensor-equipped guide canes, for safe and effective navigation. Misalignment or improper handling of these devices can reduce their effectiveness, increasing the risk of collisions and injuries. This paper presents an AIbased embedded system designed to predict and correct the orientation of a guide cane in real time. By integrating an Inertial Measurement Unit (IMU) with a neural network, the system continuously monitors the cane's lateral angle and orientation while providing feedback to help the user self-correct. The feedback is proportional to the degree of error, guiding users to maintain proper cane positioning during mobility. The device logs data that can be visualized remotely, offering mobility trainers valuable insights into the user's navigation patterns. Evaluation by visually impaired users demonstrated that the system effectively aided in real-time orientation correction. This effort contributes towards safe use of ETAs for navigation by visually challenged persons."
SRL-Gym: A Morphology and Controller Co-Optimization Framework for Supernumerary Robotic Limbs in Load-Bearing Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128597,"Supernumerary Robotic Limbs (SRLs) can assist human motions by providing extra degrees of freedom (DoFs) and body support. The extra DoFs lead to larger design space in structure and control policies, which is complex and time-consuming with the traditional manual design process. In this pilot study, we proposed a novel morphology-controller co-optimization framework to automatically generate and optimize the SRL structure based on the locomotion task input. There are two layers, with the inner layer optimizing the controller to achieve human-robot synchronization, and the outer layer optimizing the morphology parameters for performance enhancement. We validated the proposed framework through simulations using SRLs in a load-bearing locomotion task. The results demonstrate that the controller optimization can automatically generate realistic gait patterns and stable human-robot synchronization, while the SRLs significantly improve the user's load-bearing capability. Additionally, the co-optimization process reduces both the manufacturing cost of the SRL and the torque on the joints. This approach shows potential for exhaustive exploration of the design space and acceleration of the design process. Future works will be done in a more realistic SRL generative design model and achieve Sim2Real for practical uses."
Adaptive Walker: User Intention and Terrain Aware Intelligent Walker with High-Resolution Tactile and IMU Sensor.,https://doi.org/10.1109/ICRA55743.2025.11127691,"In this paper, we present an adaptive walker system designed to address limitations in current intelligent walker technologies. While recent advancements have been made in this field, existing systems often struggle to seamlessly interpret user intent for speed control and lack adaptability across diverse scenarios and terrain. Our proposed solution incorporates high-resolution tactile sensors, deep learning algorithms, IMU sensors, and linear motors to dynamically adjust to the user's intentions and terrain changes. The system is capable of predicting the user's desired speed with an error margin of only 20.99%, relying solely on tactile input from hand and arm contact points. Additionally, it maintains the walker's horizontal stability with an error of less than 1 degree by adjusting leg lengths in response to variations in ground angle. This adaptive walker enhances user safety and comfort, particularly for individuals with reduced strength or cognitive abilities, and offers reliable assistance on uneven terrain such as uphill and downhill paths."
"IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition.",https://doi.org/10.1109/ICRA55743.2025.11128229,"Robotic assistive feeding holds significant promise for improving the quality of life for individuals with eating disabilities. However, acquiring diverse food items under varying conditions and generalizing to unseen food presents unique challenges. Existing methods that rely on surface-level geometric information (e.g., bounding box and pose) derived from visual cues (e.g., color, shape, and texture) often lacks adaptability and robustness, especially when foods share similar physical properties but differ in visual appearance. We employ imitation learning (IL) to learn a policy for food acquisition. Existing methods employ IL or Reinforcement Learning (RL) to learn a policy based on off-the-shelf image encoders such as ResNet-50. However, such representations are not robust and struggle to generalize across diverse acquisition scenarios. To address these limitations, we propose a novel approach, IMRL (Integrated Multi-Dimensional Representation Learning), which integrates visual, physical, temporal, and geometric representations to enhance the robustness and generalizability of IL for food acquisition. Our approach captures food types and physical properties (e.g., solid, semi-solid, granular, liquid, and mixture), models temporal dynamics of acquisition actions, and introduces geometric information to determine optimal scooping points and assess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies based on context, improving the robot's capability to handle diverse food acquisition scenarios. Experiments on a real robot demonstrate our approach's robustness and adaptability across various foods and bowl configurations, including zero-shot generalization to unseen settings. Our approach achieves an improvement up to 35 % in success rate compared with the best-performing baseline. More details can be found on our website https://ruiiu.github.io/imrl."
An Interactive Hands-Free Controller for a Riding Ballbot to Enable Simple Shared Control Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127992,"Our team developed a riding ballbot (called PURE) that is dynamically stable, omnidirectional, and driven by lean-to-steer control. A hands-free admittance control scheme (HACS) was previously integrated to allow riders with different torso functions to control the robot's movements via torso leaning and twisting. Such an interface requires motor coordination skills and could result in collisions with obstacles due to low proficiency. Hence, a shared controller (SC) that limits the speed of PURE could be helpful to ensure the safety of riders. However, the self-balancing dynamics of PURE could result in a weak control authority of its motion, in which the torso motion of the rider could easily disturb the tracking of the command speed dictated by the shared controller. Thus, we proposed an interactive hands-free admittance control scheme (iHACS), with the following features to improve the speed-tracking performance of PURE: control gain personalization module and interaction compensation module. Human riding tests of simple tasks, idle-keeping and speed-limiting, were conducted to compare the performance of HACS and iHACS. Two manual wheelchair users and two able-bodied individuals participated in this study. They were instructed to use adver-sarial torso motions that would tax the PURE's ability to keep the ballbot idling or below a set speed, i.e., competing objectives between rider and robot. In the idle-keeping tasks, iHACS demonstrated minimal translational motion and low command speed tracking RMSE, even with significant torso lean angles. During the speed-limiting task, where the commanded speed was saturated at 0.5 m/s, the system achieved an average maximum speed of 1.1 m/s with iHACS, compared with that of over 1.9 m/s with HACS. These results suggest that iHACS can enhance PURE's control authority over the rider, which enables PURE to provide physical interactions back to the rider and results in a collaborative rider-robot synergy."
Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD.,https://doi.org/10.1109/ICRA55743.2025.11128395,"With the rapid advancements in autonomous driving, accurately predicting pedestrian behavior has become essential for ensuring safety in complex and unpredictable traffic conditions. The growing interest in this challenge highlights the need for comprehensive datasets that capture unstructured environments, enabling the development of more robust prediction models to enhance pedestrian safety and vehicle navigation. In this paper, we introduce an Indian driving pedestrian dataset designed to address the complexities of modeling pedestrian behavior in unstructured environments, such as illumination changes, occlusion of pedestrians, unsignalized scene types and vehicle-pedestrian interactions. The dataset provides high-level and detailed low-level comprehensive annotations focused on pedestrians requiring the ego-vehicle's attention. Evaluation of the state-of-the-art intention prediction methods on our dataset shows a significant performance drop of up to 15 %, while trajectory prediction methods underperform with an increase of up to 1208 MSE, defeating standard pedes-trian datasets. Additionally, we present exhaustive quantitative and qualitative analysis of intention and trajectory baselines. We believe that our dataset will open new challenges for the pedestrian behavior research community to build robust models. Project Page: https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped"
Visual-Linguistic Reasoning for Pedestrian Trajectory Prediction.,https://doi.org/10.1109/ICRA55743.2025.11127538,"Accurate prediction of pedestrian trajectories is crucial as autonomous vehicles become more prevalent on roads. The dynamic nature of urban environments and the less predictable behavior of pedestrians present significant challenges in developing reliable prediction models. Earlier methods relying on recurrent neural networks (RNNs) and long-shortterm memory (LSTM) networks have shown promise, but often fail to fully take advantage of the rich visual and contextual information available in real-world scenarios. Recent advances in vision-language models (VLMs) offer new opportunities to improve pedestrian trajectory prediction by incorporating multimodal reasoning capabilities. This paper introduces a novel approach that uses a powerful pre-trained VLM to improve the estimation of pedestrian trajectories. Specifically, we first enable learning of semantically useful scene context and high-level reasoning features via vision-language model finetuning on specific prompts using road scenes with pedestrians. Next, with the learned VLM features and the pedestrian's past trajectory history, we predict future trajectories using an encoder-decoder head. Through experiments with first-person datasets JAAD and PIE, we show that utilizing visual-linguistic semantics via a pretrained vision-language model outperforms previous methods in both deterministic and stochastic trajectory prediction setups."
Curb Your Attention: Causal Attention Gating for Robust Trajectory Prediction in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128367,"Trajectory prediction models in autonomous driving are vulnerable to perturbations from non-causal agents whose actions should not affect the ego-agent's behavior. Such perturbations can lead to incorrect predictions of other agents' trajectories, potentially compromising the safety and efficiency of the ego-vehicle's decision-making process. Motivated by this challenge, we propose Causal tRajecTory predICtion (CRiTIC), a novel model that utilizes a causal discovery network to identify inter-agent causal relations over a window of past time steps. To incorporate discovered causal relationships, we propose a novel Causal Attention Gating mechanism to selectively filter information in the proposed Transformer- based architecture. We conduct extensive experiments on two autonomous driving benchmark datasets to evaluate the robustness of our model against non-causal perturbations and its generalization capacity. Our results indicate that the robustness of predictions can be improved by up to 54% without a significant detriment to prediction accuracy. Lastly, we demonstrate the superior domain generalizability of the proposed model, which achieves up to 29% improvement in cross-domain performance. These results underscore the potential of our model to enhance both robustness and generalization capacity for trajectory prediction in diverse autonomous driving domairis.<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup>"
Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128112,"3D multi-object tracking plays a critical role in autonomous driving by enabling the real-time monitoring and prediction of multiple objects' movements. Traditional 3D tracking systems are typically constrained by predefined object categories, limiting their adaptability to novel, unseen objects in dynamic environments. To address this limitation, we introduce open-vocabulary 3D tracking, which extends the scope of 3D tracking to include objects beyond predefined categories. We formulate the problem of open-vocabulary 3D tracking and introduce dataset splits designed to represent various open-vocabulary scenarios. We propose a novel approach that integrates open-vocabulary capabilities into a 3D tracking framework, allowing for generalization to unseen object classes. Our method effectively reduces the performance gap between tracking known and novel objects through strategic adaptation. Experimental results demonstrate the robustness and adaptability of our method in diverse outdoor driving scenarios. To the best of our knowledge, this work is the first to address open-vocabulary 3D tracking, presenting a significant advancement for autonomous systems in real-world settings. Code, trained models, and dataset splits are available at https://github.com/ayesha-ishaq/Open3DTrack."
Asynchronous Multi-Object Tracking with an Event Camera.,https://doi.org/10.1109/ICRA55743.2025.11127984,"Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>https://github.com/angus-apps/AEMOT"
Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127303,"Vehicle-to-everything technologies (V2X) have become an ideal paradigm to extend the perception range and see through the occlusion. Exiting efforts focus on single-frame cooperative perception, however, how to capture the temporal cue between frames with V2X to facilitate the prediction task even the planning task is still underexplored. In this paper, we introduce the Co-MTP, a general cooperative trajectory prediction framework with multi-temporal fusion for autonomous driving, which leverages the V2X system to fully capture the interaction among agents in both history and future domains to benefit the planning. In the history domain, V2X can complement the incomplete history trajectory in single-vehicle perception, and we design a heterogeneous graph transformer to learn the fusion of the history feature from multiple agents and capture the history interaction. Moreover, the goal of prediction is to support future planning. Thus, in the future domain, V2X can provide the prediction results of surrounding objects, and we further extend the graph transformer to capture the future interaction among the ego planning and the other vehicles' intentions and obtain the final future scenario state under a certain planning action. We evaluate the Co-MTP framework on the real-world dataset V2X-Seq, and the results show that Co-MTP achieves state-of-the-art performance and that both history and future fusion can greatly benefit prediction. Our code is available on our project website: https://xiaomiaozhang.github.io/Co-MTP/"
Sim4EndoR: A Reinforcement Learning Centered Simulation Platform for Task Automation of Endovascular Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127627,"Robotic-assisted percutaneous coronary intervention (PCI) holds considerable promise for elevating precision and safety in cardiovascular procedures. Nevertheless, current systems heavily depend on human operators, resulting in variability and the potential for human error. To tackle these challenges, Sim4EndoR, an innovative reinforcement learning (RL) based simulation environment, is first introduced to bolster task-level autonomy in PCI. This platform offers a comprehensive and risk-free environment for the development, evaluation, and refinement of potential autonomous systems, enhancing data collection efficiency and minimizing the need for costly hardware trials. A notable aspect of the groundbreaking Sim4EndoR is its reward function, which takes into account the anatomical constraints of the vascular environment, utilizing the geometric characteristics of vessels to steer the learning process. By seamlessly integrating advanced physical simulations with neural network-driven policy learning, Sim4EndoR fosters efficient sim-to-real translation, paving the way for safer, more consistent robotic interventions in clinical practice, ultimately improving patient outcomes."
Design and Implementation of a Snake Robot for Cranial Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128494,"Craniosynostosis involves premature fusion of the cranial sutures resulting in abnormal skull morphology and elevated intracranial pressure. Surgical intervention is necessary to correct the skull shape and to allow for unrestricted brain growth. This study presents a novel snake robot designed for minimally invasive cranial osteotomies featuring two articulating bending segments. The end-effector comprises a bone-punch for bone-cutting, a dural and scalp retractor, as well as channels for an endoscope and an instrument. The robot's bending mechanism is driven by tendons and utilizes geared linkages to facilitate a smooth curved shape. Pre-tensioned antagonistic tendons allow the robot to modulate its stiffness to adapt to external loads. A follow-the-leader algorithm was implemented to guide the robot along a skull cutting path. Experimental results demonstrated that at maximum bending of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$60^{\circ}$</tex> for segment 1 and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$90^{\circ}$</tex> for segment 2 there was a <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$15.9^{\circ}$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$11.5^\circ$</tex> error, respectively. Position errors ranged from 2.5 to 21.5 mm when tracing a curved path. The tool increased stiffness with tendon pre-tensioning from 20100 N during bent configurations <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$q_{1}$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$q_{2}$</tex> for segments 1 and 2, respectively, at <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$[q_{1},q_{2}]=[0^{\mathrm{o}},30^{\mathrm{o}}]$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$[30^{\circ},60^{\circ}]$</tex>. Tip deflection reduced from 0.42 to 0.03 cm and 0.37 to 0.10 cm during axial loading and from 11.40 to 3.88 cm and 3.62 to 0.48 cm during radial loading for each configuration, respectively. Ex vitro trials demonstrated the robots ability to perform simulated osteotomies on skull models to 6873% of desired path lengths with a maximum deviation of 8 mm."
Single-Fiber Optical Frequency Domain Reflectometry (Ofdr) Shape Sensing of Continuum Manipulators With Planar Bending.,https://doi.org/10.1109/ICRA55743.2025.11128185,"To address the challenges associated with shape sensing of continuum manipulators (CMs) using Fiber Bragg Grating (FBG) optical fibers, we present a unique shape sensing assembly utilizing solely a single Optical Frequency Domain Reflectometry (OFDR) fiber attached to a flat nitinol wire (NiTi). Integrating this easy-to-manufacture unique sensor with a long and soft CM with 170 mm length, we performed different experiments to evaluate its C -, J -, and S-shape reconstruction ability. Results demonstrate phenomenal shape reconstruction accuracy for the performed C-shape (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;3.14 ~\text{mm}$</tex> tip error, < 2.54 mm shape error), J-shape (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;1.91 ~\text{mm}$</tex> tip error, <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;1.11 \mathbf{m m}$</tex> shape error), and S-shape (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;\mathbf{1. 7 4 ~ m m}$</tex> tip error, <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;\mathbf{1. 4 0 ~ m m}$</tex> shape error) experiments."
Learning-Based Tip Contact Force Estimation for FBG-Embedded Continuum Robots.,https://doi.org/10.1109/ICRA55743.2025.11128662,"Knowledge of the tip contact force in continuum robots, which are often used as medical instruments, is critical for clinical applications. It enhances the interventionalist's decision-making, navigation efficiency, and procedural safety. However, accurately determining the tip contact force in conventionally sized instruments remains challenging. This study introduces a learning-based method for estimating the external contact force at the tip of a continuum robot. By leveraging curvature and bending angle data from a multi-core fiber equipped with fiber Bragg gratings (FBGs) embedded inside the Nitinol tube, the method maps these inputs to the corresponding tip force in 3D. Experiments conducted on an FBG-embedded Nitinol rod validate the feasibility of the proposed method, yielding Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) values of 20.9 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\left(m N^{2}\right), 2.7(m N)$</tex>, and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$4.6(m N)$</tex>, respectively, which represent a 26 % improvement compared to the learning-based vision methodology."
Three-Dimension Tip Force Perception and Axial Contact Location Identification for Flexible Endoscopy Using Tissue-Compliant Soft Distal Attachment Cap Sensors.,https://doi.org/10.1109/ICRA55743.2025.11128801,"In endoluminal surgeries, inserting a flexible endo-scope is one of the fundamental procedures. During this process, vision remains the primary feedback, while the perception of tactile magnitude and location is insufficient. This limitation can hinder the clinician's efficiency when navigating the endoscope through various segments of the natural lumens. To address this issue, we propose a fiber Bragg grating (FBG)-based tissue-compliant sensor cap with multi-mode sensing capabilities, including contact location identification at the terminal surface and the three-dimensional contact force perception at the tip. The soft sensor cap can be affixed to the standard endoscope tip, like a distal attachment cap, for easy installation. Utilizing the relative contact location information, operators can adjust the steerable segment of the endoscope when transitioning from one segment of a natural orifice to a narrower segment, which may be obstructed by constricted lumens. A finite element analysis simulation and the corresponding calibration process based on learning-based approaches have been carried out. The FBG-based sensor can perceive the tip contact force and identify the axial contact location with high precision, where the force perception error is less than 3%, and the contact location identification accuracy is 98.8%. The experimental results demonstrate the potential of the proposed sensing mechanism to be applied in surgeries requiring endoscope insertions."
MeshDMP: Motion Planning on Discrete Manifolds Using Dynamic Movement Primitives.,https://doi.org/10.1109/ICRA55743.2025.11128556,"An open problem in industrial automation is to reliably perform tasks requiring in-contact movements with complex workpieces, as current solutions lack the ability to seamlessly adapt to the workpiece geometry. In this paper, we propose a Learning from Demonstration approach that allows a robot manipulator to learn and generalise motions across complex surfaces by leveraging differential mathematical operators on discrete manifolds to embed information on the geometry of the workpiece extracted from triangular meshes, and extend the Dynamic Movement Primitives (DMPs) framework to generate motions on the mesh surfaces. We also propose an effective strategy to adapt the motion to different surfaces, by introducing an isometric transformation of the learned forcing term. The resulting approach, namely MeshDMP, is evaluated both in simulation and real experiments, showing promising results in typical industrial automation tasks like car surface polishing."
Robotic Sim-to-Real Transfer for Long-Horizon Pick-and-Place Tasks in the Robotic Sim2Real Competition.,https://doi.org/10.1109/ICRA55743.2025.11128837,"This paper presents a fully autonomous robotic system that performs sim-to-real transfer in complex longhorizon tasks involving navigation, recognition, grasping, and stacking in an environment with multiple obstacles. The key feature of the system is the ability to overcome typical sensing and actuation discrepancies during sim-to-real transfer and to achieve consistent performance without any algorithmic modifications. To accomplish this, a lightweight noise-resistant visual perception system and a nonlinearityrobust servo system are adopted. We conduct a series of tests in both simulated and realworld environments. The visual perception system achieves the speed of 11 ms per frame due to its lightweight nature, and the servo system achieves sub-centimeter accuracy with the proposed controller. Both exhibit high consistency during sim-to-real transfer. Benefiting from these, our robotic system took first place in the mineral searching task of the Robotic Sim2Real Challenge hosted at ICRA 2024."
Towards Autonomous Data Annotation and System-Agnostic Robotic Grasping Benchmarking with 3D-Printed Fixtures.,https://doi.org/10.1109/ICRA55743.2025.11127593,"The interaction of robots with their environment requires robust object-centric perception capabilities, typically achieved using learning-based methods trained on synthetic data. However, real-world deployment demands evaluating these capabilities in relevant environments, often involving extensive manual annotation for a quantitative analysis. Additionally, standardized evaluations for robotic tasks, such as grasping, need reproducible object scene configurations and performance benchmarks. We propose a solution to both problems by temporarily employing 3D-printed components, socalled fixtures, which can be designed for any rigid object. Once the scene is set up and object poses are extracted, the fixtures are removed, leaving the natural scene without any artificial distractions. The presented approach is seemingly applicable for pre-determined configurations of multiple objects, which enables precise re-building of scenes with consistent object-toobject relations. Our suggested annotation procedure achieves strong pose accuracy solely on RGB images without any manual involvement. We evaluate and show the usability of the proposed fixtures for automated real-world data annotation to fine-tune a detector and for benchmarking object pose estimation algorithms for robotic grasping. Code and fixture meshes for 3D printing are available at https://github.com/DLRRM/fixture_generation."
Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control.,https://doi.org/10.1109/ICRA55743.2025.11128022,"Partial hand amputations significantly affect the physical and psychosocial well-being of individuals, yet intuitive control of externally powered prostheses remains an open challenge. To address this gap, we developed a force-controlled prosthetic finger activated by electromyography (EMG) signals. The prototype, constructed around a wrist brace, functions as a supernumerary finger placed near the index, allowing for early-stage evaluation on unimpaired subjects. A neural network-based model was then implemented to estimate fingertip forces from EMG inputs, allowing for online adjustment of the prosthetic finger grip strength. The force estimation model was validated through experiments with ten participants, demonstrating its effectiveness in predicting forces. Additionally, online trials with four users wearing the prosthesis exhibited precise control over the device. Our findings highlight the potential of using EMG-based force estimation to enhance the functionality of prosthetic fingers."
Integrated Motion State Prediction for Sit-to-Stand and Stand-to-Sit Motions Toward Effective Power Assist Control.,https://doi.org/10.1109/ICRA55743.2025.11128744,"Sit-to-stand and stand-to-sit motions are important in daily activities. However, elderly individuals often find these motions difficult to perform with declining lower limb strength, which causes a considerable reduction to their quality of life. In this study, a sensing method for controlling robotic assistive devices was proposed. This method utilizes electromyographic measurements and a deep neural network to predict motion initiation, and it estimates the timing of triggering assistive devices. Experimental results indicate that four muscle synergy patterns are required to represent the sit-to-stand and stand-to-sit motions together, with two of them being shared between both movements. Subsequently, a long short-term memory network was designed to forecast these two motions, and the result indicates that the prediction accuracy reached 92.95%  0.83% with forecasting time of 300 ms."
"On Chain Driven, Adaptive, Underactuated Fingers for the Development of Affordable, Robust Humanlike Prosthetic Hands.",https://doi.org/10.1109/ICRA55743.2025.11128644,"Amputations and limb loss can have detrimental effects on personal well-being. Although prosthetic devices can offer significant benefits helping amputees regain some of the lost dexterity, they often lack the required affordability and durability. Current affordable prosthetic designs have trended towards underactuation, which contributes to stable grasping but is often characterized by low durability. In this paper, a new chain-driven, adaptive, underactuated finger design has been proposed for the development of affordable and highly durable prosthetic hands. The transmission mechanism used is composed of a steel roller chain and several routing sprockets. The finger phalanges are constructed of 3D printed PLA, and finger flexion is produced by pulling the internally routed roller chain. In total, six 3D printed PLA sprockets are used for chain routing, with a design emphasis on high force transmission. The performance of the proposed chaindriven finger was experimentally validated and compared with an analogous tendon-driven version. The metrics employed for this comparison were longevity, pinch grasp efficiency, force response, and maximum force capability. The chain-driven finger was shown to have a higher maximum transmissible force, better long term durability, and no issues related to elongation (such as tendon elongation). The cost to manufacture the chain-driven robotic finger is only 91 USD, making it an excellent solution for affordable prostheses."
Force Myography Based Torque Estimation in Human Knee and Ankle Joints.,https://doi.org/10.1109/ICRA55743.2025.11128774,"The online adaptation of exoskeleton control based on muscle activity sensing offers a promising approach to personalizing exoskeleton behavior based on the user's biosignals. While electromyography (EMG)-based methods have demonstrated improvements in joint torque estimation, EMG sensors require direct skin contact and extensive post-processing. In contrast, force myography (FMG) measures normal forces resulting from changes in muscle volume due to muscle activity. We propose an FMG-based method to estimate knee and ankle joint torques by integrating joint angles and velocities with muscle activity data. We learn a model for joint torque estimation using Gaussian process regression (GPR). The effectiveness of the proposed FMG-based method is validated on isokinetic motions performed by ten participants. The model is compared to a baseline model that uses only joint angle and velocity as well as a model augmented by EMG data. The results indicate that incorporating FMG into exoskeleton control can improve the estimation of joint torque for the ankle and knee joints in novel task characteristics within a single participant. Although the findings suggest that this approach may not improve the generalizability of estimates between multiple participants, they highlight the need for further research into its potential applications in exoskeleton control."
Adaptive Ankle-Foot Prosthesis with Passive Agonist-Antagonist Design.,https://doi.org/10.1109/ICRA55743.2025.11127721,"The development of prosthetic feet that closely replicate the natural biomechanics of the human foot remains a significant challenge in prosthetics engineering. This paper presents the design and testing of a novel agonist-antagonist architecture for the ankle joint of a passive prosthetic foot featuring an adaptive sole. The ankle mechanism, inspired by the dynamics of the human leg-ankle-foot complex, utilizes compliant elements in an agonist-antagonist configuration to passively achieve an ankle torque close to that of a sound ankle without the need for external actuation. Concurrently, the adaptive sole adjusts its shape in response to different terrains, potentially improving stability and comfort for the user. The theoretical model underlying the proposed design is presented, followed by a preliminary validation through simulations. Finally, a prototype based on the new architecture is tested by a healthy subject using customized walking boots, demonstrating its potential to improve the functional performance of prosthetic feet in diverse environments."
V2X-DGW: Domain Generalization for Multi-Agent Perception Under Adverse Weather Conditions.,https://doi.org/10.1109/ICRA55743.2025.11127945,"Current LiDAR-based Vehicle-to-Everything (V2X) multi-agent perception systems have shown the significant success on 3D object detection. While these models perform well in the trained clean weather, they struggle in unseen adverse weather conditions with the domain gap. In this paper, we propose a Domain Generalization based approach, named V2X-DGW, for LiDAR-based 3D object detection on multi-agent perception system under adverse weather conditions. Our research aims to not only maintain favorable multi-agent performance in the clean weather but also promote the performance in the unseen adverse weather conditions by learning only on the clean weather data. To realize the Domain Generalization, we first introduce the Adaptive Weather Augmentation (AWA) to mimic the unseen adverse weather conditions, and then propose two alignments for generalizable representation learning: Trust-region Weatherinvariant Alignment (TWA) and Agent-aware Contrastive Alignment (ACA). To evaluate this research, we add Fog, Rain, Snow conditions on two publicized multi-agent datasets based on physics-based models, resulting in two new datasets: OPV2V-w and V2XSet-w. Extensive experiments demonstrate that our V2X-DGW achieved significant improvements in the unseen adverse weathers. The code is available at https://github.com/Baolu1998/V2X-DGW."
The Automation of Uncrewed Aircraft Systems Traffic Management Calibration Based on Experimental Platform Data.,https://doi.org/10.1109/ICRA55743.2025.11128464,"Many countries are developing an Urban Air Mobility (UAM) capability defining an Uncrewed Aircraft Systems (UAS) Traffic Management (UTM) architecture to allow safe UAS services in urban environments (e.g., delivery, inspection, air taxis, etc.). The main considerations are air worthiness, operator certification, air traffic management, C2 Link, detect and avoid (DAA), safety management, and security. In addition, if thousands of simultaneous UAS flights are to be achieved, it is not possible for them to be controlled individually by human operators. This makes it necessary to have a rigorous and safe automation methodology to handle such a number of flights. A lane-based airspace structure has been proposed which reduces the complexity of strategic deconfliction by providing UAS agents with a set of pre-defined airway corridors called lanes [1], [2]. This yields collateral benefits including UAS information privacy, robust contingency handling exploiting the lane structure, as well as improved observability and control of the air space. A robust set of UTM parameters and policies must be determined based on the performance characteristics of the deployed UAS platforms, and a methodology which constitutes a first step toward this end is proposed and demonstrated here. In order to realize this approach, a set of initial experiments have been performed to determine the constraints imposed by the UTM on UAS platform capabilities and vice versa. Initial implementation parameters and policies are defined. The major contribution here is a methodology to calibrate UTM safety parameters (e.g., headway, platform speed) in terms of specific platform models' operational characteristics. That is, UTM parameters are a function of platform and not some arbitrarily imposed values. Safety uncertainty is then characterized by the calibration method."
Non-Parametric GNSS Integer Ambiguity Estimation via Positional Likelihood Field Marginalization.,https://doi.org/10.1109/ICRA55743.2025.11128778,"In this paper, we propose a non-parametric method for estimating the posterior distribution of global positioning satellite systems (GNSS) integer ambiguity. It is difficult to estimate the posterior probability of discrete integer ambiguities directly from carrier phase observations due to the unclear domain definition. We thus introduce a positional likelihood field that accumulates the ambiguity function method values in the position space and then estimate the integer ambiguity distributions by marginalizing the likelihood over the entire position. Defining the positional likelihood field in the position space facilitates carrier phase likelihood accumulation. To correctly estimate the posterior distribution, however, a sufficient density of samples is required, which results in a large computational cost. The proposed method enables large-scale sampling by taking advantage of GPU parallel processing. Experimental results demonstrate that the proposed method enables accurate and robust estimation of integer ambiguity distributions, contributing to improved centimeter-level position estimation accuracy. In addition, the histograms provide quantitative evidence of events in urban environments where integer ambiguity is not uniquely determined."
"Whenever, Wherever: Towards Orchestrating Crowd Simulations with Spatio-Temporal Spawn Dynamics.",https://doi.org/10.1109/ICRA55743.2025.11128302,"Realistic crowd simulations are essential for immersive virtual environments, relying on both individual behaviors (microscopic dynamics) and overall crowd patterns (macroscopic characteristics). While recent data-driven methods like deep reinforcement learning improve microscopic realism, they often overlook critical macroscopic features such as crowd density and flow, which are governed by spatio-temporal spawn dynamics, namely, when and where agents enter a scene. Traditional methods, like random spawn rates, stochastic processes, or fixed schedules, are not guaranteed to capture the underlying complexity or lack diversity and realism. To address this issue, we propose a novel approach called nTPP-GMM that models spatio-temporal spawn dynamics using Neural Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate our approach by orchestrating crowd simulations of three diverse real-world datasets with nTPP-GMM. Our experiments demonstrate the orchestration with nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios and allow crowd analysis."
RMP-YOLO: A Robust Motion Predictor for Partially Observable Scenarios Even if You Only Look Once.,https://doi.org/10.1109/ICRA55743.2025.11127743,"We introduce RMP-YOLO, a unified framework designed to provide robust motion predictions even with incomplete input data. Our key insight stems from the observation that complete and reliable historical trajectory data plays a pivotal role in ensuring accurate motion prediction. Therefore, we propose a new paradigm that prioritizes the reconstruction of intact historical trajectories before feeding them into the prediction modules. Our approach introduces a novel scene tokenization module to enhance the extraction and fusion of spatial and temporal features. Following this, our proposed recovery module reconstructs agents' incomplete historical trajectories by leveraging local map topology and interactions with nearby agents. The reconstructed, clean historical data is then integrated into the downstream prediction modules. Our framework is able to effectively handle missing data of varying lengths and remains robust against observation noise while maintaining high prediction accuracy. Furthermore, our recovery module is compatible with existing prediction models, ensuring seamless integration. Extensive experiments validate the effectiveness of our approach, and deployment in real-world autonomous vehicles confirms its practical utility. In the 2024 Waymo Motion Prediction Competition, our method, RMP-YOLO, achieves state-of-the-art performance, securing third place. Our code is open-source at https://github.com/ggosjw/RMP-YOLO."
Leg Exoskeleton Odometry using a Limited FOV Depth Sensor.,https://doi.org/10.1109/ICRA55743.2025.11128659,"For leg exoskeletons to operate effectively in real-world environments, they must be able to perceive and understand the terrain around them. However, unlike other legged robots, exoskeletons face specific constraints on where depth sensors can be mounted due to the presence of a human user. These constraints lead to a limited Field Of View (FOV) and greater sensor motion, making odometry particularly challenging. To address this, we propose a novel odometry algorithm that integrates proprioceptive data from the exoskeleton with point clouds from a depth camera to produce accurate elevation maps despite these limitations. Our method builds on an extended Kalman filter (EKF) to fuse kinematic and inertial measurements, while incorporating a tailored iterative closest point (ICP) algorithm to register new point clouds with the elevation map. Experimental validation with a leg exoskeleton demonstrates that our approach reduces drift and enhances the quality of elevation maps compared to a purely proprioceptive baseline, while also outperforming a more traditional point cloud map-based variant."
Improving Monocular Visual-Inertial Initialization with Structureless Visual-Inertial Bundle Adjustment.,https://doi.org/10.1109/ICRA55743.2025.11127304,"Monocular visual inertial odometry (VIO) has facilitated a wide range of real-time motion tracking applications, thanks to the small size of the sensor suite and low power consumption. To successfully bootstrap VIO algorithms, the initialization module is extremely important. Most initialization methods rely on the reconstruction of 3D visual point clouds. These methods suffer from high computational cost as state vector contains both motion states and 3D feature points. To address this issue, some researchers recently proposed a structureless initialization method, which can solve the initial state without recovering 3D structure. However, this method potentially compromises performance due to the decoupled estimation of rotation and translation, as well as linear constraints. To improve its accuracy, we propose novel structureless visual-inertial bundle adjustment to further refine previous structureless solution. Extensive experiments on real-world datasets show our method significantly improves the VIO initialization accuracy, while maintaining real-time performance."
ORB-SfMLearner: ORB-Guided Self-supervised Visual Odometry with Selective Online Adaptation.,https://doi.org/10.1109/ICRA55743.2025.11127848,"Deep visual odometry, despite extensive research, still faces limitations in accuracy and generalizability that prevent its broader application. To address these challenges, we propose an Oriented FAST and Rotated BRIEF (ORB)-guided visual odometry with selective online adaptation named ORB-SfMLearner. We present a novel use of ORB features for learning-based ego-motion estimation, leading to more robust and accurate results. We also introduce the cross-attention mechanism to enhance the explainability of PoseNet and have revealed that driving direction of the vehicle can be explained through the attention weights. To improve generalizability, our selective online adaptation allows the network to rapidly and selectively adjust to the optimal parameters across different domains. Experimental results on KITTI and vKITTI datasets show that our method outperforms previous state-of-the-art deep visual odometry methods in terms of ego-motion accuracy and generalizability."
QVIO2: Quantized Map-Based Visual-Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11127636,"Energy-efficient visual-inertial motion tracking on SWAP-constrained edge devices (e.g., drones and AR glasses) is essential but challenging. Our previous work [1] introduced the first-of-its-kind quantized visual-inertial odometry (QVIO), utilizing either raw measurement quantization (zQVIO) or single-bit residual quantization (rQVIO). While QVIO has demonstrated significant data transfer reduction with competitive performance, it has limitations. Specifically, zQVIO directly quantizes raw measurements into multi-bit values, while requiring the ad-hoc inflation of measurement noise to account for quantization errors. On the other hand, rQVIO is limited to single-bit measurement with certain accuracy loss. This work introduces QVIO2 to address these issues. The proposed QVIO2 improves data quantization strategies and derives a Maximum A Posteriori (MAP) quantized estimator that rigorously handles both multi-bit and single-bit, raw and residual quantized measurements in a unified manner. These improvements lead to more communication-efficient and accurate systems. Additionally, we optimize the communication protocol to further reduce data transfer by eliminating unnecessary transmissions. Extensive numerical and experimental results demonstrate reduced communication requirements and improved accuracy. Compared to the previous QVIO system, zQVIO2 achieves the same accuracy with a 30 % reduction in data transfer, while rQVIO2 improves accuracy without increasing data communication. In real-world scenarios, our new zQVIO2 and rQVIO2 have demonstrated nearly no accuracy loss with only 4.6 bits and 3.5 bits of data communication, achieving compression rates of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$7 \times$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$9.1 \times$</tex>."
Is Iteration Worth It? Revisit its Impact in Sliding-Window VIO.,https://doi.org/10.1109/ICRA55743.2025.11127352,"Visual-inertial odometry (VIO), which fuses noisy inertial readings and camera measurements to provide 3D motion tracking, is a foundational component in many autonomous applications. With the increasing use of next-generation edge devices (e.g., AR/VR devices, nano drones, and mobile robotics) that are constrained by limited power, resources, and multitasking demands, balancing computational efficiency and accuracy in VIO estimators has become more critical than ever. Historically, state estimation algorithms have been developed using either optimization or filtering-based methods, with the key distinction being the ability to relinearize measurements and correct state estimates iteratively. It has been widely claimed that iterative methods improve accuracy by allowing for the reduction of error through relinearization at a higher computational demand. Conversely, filtering methods are more efficient but may suffer from significant linearization errors. However, these trade-offs have not been thoroughly examined in the context of visual-inertial motion tracking. In this paper, we conduct the first comprehensive study on the impact of iterative algorithms in sliding-window VIO. We analyze the relinearization of IMU and camera measurements separately, providing insights into how each affects system performance. By considering key factors such as system observability and measurement processes, we offer a deeper understanding of VIO estimator behavior. Our findings, backed by real-world tests, offer practical guidelines for balancing accuracy and efficiency, helping practitioners determine when to prioritize iterative methods or simpler filtering approaches while encouraging researchers and engineers to rethink VIO design for optimal resource allocation."
EAR-SLAM: Environment-Aware Robust Localization System for Terrestrial-Aerial Bimodal Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128310,"Terrestrial-aerial bimodal vehicles (TABVs) can fly to avoid obstacles and move safely on the ground to save energy, offering enhanced adaptability and flexibility in various challenging environments. However, a robust localization approach becomes a bottleneck to stably applying the TABVs in real-world tasks. Besides the general limitations of visual SLAM methods, large FoV differences between the two modes, abrupt motion strikes in mode transitions, and unstable attitude in ground mode pose great challenges. In this paper, we present an environment-aware robust localization system specifically designed for passive-wheel-based TABVs, which feature two passive wheels alongside a standard quadrotor. The localization system tightly integrates data from multiple sensors, including a stereo camera, Inertial Measurement Units (IMUs), encoders, and single-point laser distance sensors. First, we introduce a terrain-aware odometer model that accurately estimates terrain slope and vehicle's velocity. Then, we propose an anomaly-aware method that senses anomalous sensors and dynamically adjusts the optimization weights accordingly. By explicitly estimating the environmental conditions, such as ground terrain slopes and visual information qualities, the robot can achieve accurate and robust localization results on the ground. To validate our localization approach, we conducted extensive experiments across various challenging scenarios, demonstrating the effectiveness and reliability of our system for real-world applications."
3D Whole-Body Pose Estimation Using Graph High-Resolution Network for Humanoid Robot Teleoperation.,https://doi.org/10.1109/ICRA55743.2025.11128446,"In the realm of robotics, teleoperation plays a pivotal role in performing high-risk or intricate tasks, and obtaining precise 3D whole-body pose is crucial for this purpose. Traditional two-stage methods have limitations in estimating different body parts, leading to complex systems and higher estimation errors. In order to address these issues,the paper introduces a novel framework called Graph High-Resolution Network (GraphHRNet) for accurate 3D whole-body pose estimation, which is essential for the teleoperation of humanoid robots. GraphHRNet effectively captures global structural information and local details by integrating a High-Resolution Module and a Multi-branch Regression Module. The High-Resolution Module utilizes an enhanced graph convolution kernel to fuse multi-scale features, capturing global information, while the Multi-branch Regression Module focuses on refining and predicting accurate 3D coordinates for intricate body parts such as hands and face. Experimental results on the H3WB dataset demonstrate that GraphHRNet surpasses state-of-the-art (SOTA) methods in 3D whole-body pose estimation, significantly improving performance. Furthermore, the paper explores the potential application of this approach in a tele-operation system for humanoid robots, providing an intuitive and high-fidelity solution for remotely executing complex tasks. The code have been publicly available at https://github.com/Z-mingyu/GraphHRNet.git"
Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation.,https://doi.org/10.1109/ICRA55743.2025.11128424,"Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, ours is one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Resulting videos and code are provided at https://sites.google.com/illinois.edu/comp-teleop."
ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128061,"In most contact-rich manipulation tasks, humans apply time-varying forces to the target object, compensating for inaccuracies in the vision-guided hand trajectory. However, current robot learning algorithms primarily focus on trajectory-based policy, with limited attention given to learning force-related skills. To address this limitation, we introduce ForceMimic, a force-centric robot learning system, providing a natural, force-aware and robot-free robotic demonstration collection system, along with a hybrid force-motion imitation learning algorithm for robust contact-rich manipulation. Using the proposed ForceCapture system, an operator can peel a zucchini in 5 minutes, while force-feedback teleoperation takes over 13 minutes and struggles with task completion. With the collected data, we propose HybridIL to train a force-centric imitation learning model, equipped with hybrid force-position control primitive to fit the predicted wrench-position parameters during robot execution. Experiments demonstrate that our approach enables the model to learn a more robust policy under the contact-rich task of vegetable peeling, increasing the success rates by 54.5% relatively compared to state-of-the-art pure-vision-based imitation learning. Hardware, code, data and more results can be found on the project website at https://forcemimic.github.io."
How to Train Your Robots? The Impact of Demonstration Modality on Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128520,"Imitation learning is a promising approach for learning robot policies with user-provided data. The way demonstrations are provided, i.e., demonstration modality, influences the quality of the data. While existing research shows that kinesthetic teaching (physically guiding the robot) is preferred by users for the intuitiveness and ease of use, the majority of existing manipulation datasets were collected through teleoperation via a VR controller or spacemouse. In this work, we investigate how different demonstration modalities impact downstream learning performance as well as user experience. Specifically, we compare low-cost demonstration modalities including kinesthetic teaching, teleoperation with a VR controller, and teleoperation with a spacemouse controller. We experiment with three table-top manipulation tasks with different motion constraints. We evaluate and compare imitation learning performance using data from different demonstration modalities, and collected subjective feedback on user experience. Our results show that kinesthetic teaching is rated the most intuitive for controlling the robot and provides cleanest data for best downstream learning performance. However, it is not preferred as the way for large-scale data collection due to the physical load. Based on such insight, we propose a simple data collection scheme that relies on a small number of kinesthetic demonstrations mixed with data collected through teleoperation to achieve the best overall learning performance while maintaining low data-collection effort."
Hierarchical Visual Policy Learning for Long-Horizon Robot Manipulation in Densely Cluttered Scenes.,https://doi.org/10.1109/ICRA55743.2025.11128752,"In this work, we focus on addressing the long-horizon packing tasks in densely cluttered scenes. Such tasks require policies to effectively manage severe occlusions among objects and continually produce precise actions based on visual observations. We propose a vision-based Hierarchical policy for Cluttered-scene Long-horizon Manipulation (HCLM). It employs a high-level policy and three options to select and instantiate three parameterized action primitives: push, pick, and place. We first train the two-stream pick and place options by behavior cloning (BC). Subsequently, we use hierarchical reinforcement learning (HRL) to train the high-level policy and push option. During HRL, we propose a Spatially Extended Q-update (SEQ) to augment the updates for the push option and a Two-Stage Update Scheme (TSUS) to alleviate the non-stationary transition problem in updating the high-level policy. We demonstrate that HCLM significantly outperforms baseline methods in terms of success rate and efficiency in diverse tasks both in simulation and real world. The ablation studies also validate the key roles of SEQ and TSUS in HRL."
AERAS: Adaptive Experience Replay with Attention-Based Sequence Embedding for Improved Multi-Agent Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128572,"Multi-agent systems in non-stationary environments face challenges due to rapidly changing dynamics, leading to quick obsolescence of experiences in the replay buffer. To address this, we propose the Adaptive Experience Replay with Attention-Based Sequence Embedding (AERAS) framework, which integrates sequence embedding with an attention mechanism to prioritize experiences based on their relevance. By assigning adaptive weights, AERAS emphasizes relevant experiences while diminishing the impact of outdated ones, enhancing efficiency and learning performance in multi-agent reinforcement learning. Evaluations on the StarCraft II Multi-Agent Challenge and Google Research Football environments show that AERAS consistently outperforms state-of-the-art methods, achieving faster convergence and higher win rates. Ablation studies confirm the essential roles of sequence embedding and attention mechanisms in boosting AERAS's robustness and adaptability, underscoring its effectiveness in managing non-stationary environments within multi-agent systems."
Multi-Type Preference Learning: Empowering Preference-Based Reinforcement Learning with Equal Preferences.,https://doi.org/10.1109/ICRA55743.2025.11127694,"Preference-Based reinforcement learning (PBRL) learns directly from the preferences of human teachers regarding agent behaviors without needing meticulously designed reward functions. However, existing PBRL methods often learn primarily from explicit preferences, neglecting the possibility that teachers may choose equal preferences. This neglect may hinder the understanding of the agent regarding the task perspective of the teacher, leading to the loss of important information. To address this issue, we introduce the Equal Preference Learning Task, which optimizes the neural network by promoting similar reward predictions when the behaviors of two agents are labeled as equal preferences. Building on this task, we propose a novel PBRL method, Multi-Type Preference Learning (MTPL), which allows simultaneous learning from equal preferences while leveraging existing methods for learning from explicit preferences. To validate our approach, we design experiments applying MTPL to four existing state-of-the-art baselines across ten locomotion and robotic manipulation tasks in the DeepMind Control Suite. The experimental results indicate that simultaneous learning from both equal and explicit preferences enables the PBRL method to more comprehensively understand the feedback from teachers, thereby enhancing feedback efficiency. Project page: https://github.com/FeiCuiLengMMbb/paper_MTPL"
Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128069,"Control Lyapunov functions are traditionally used to design a controller which ensures convergence to a desired state, yet deriving these functions for nonlinear systems remains a complex challenge. This paper presents a novel, sample-efficient method for neural approximation of nonlinear Lyapunov functions, leveraging self-supervised Reinforcement Learning (RL) to enhance training data generation, particularly for inaccurately represented regions of the state space. The proposed approach employs a data-driven World Model to train Lyapunov functions from off-policy trajectories. The method is validated on both standard and goal-conditioned robotic tasks, demonstrating faster convergence and higher approximation accuracy compared to the state-of-the-art neural Lyapunov approximation baseline. The code is available at: https://github.com/CAV-Research-Lab/SACLA.git"
SuPLE: Robot Learning with Lyapunov Rewards.,https://doi.org/10.1109/ICRA55743.2025.11128350,"The reward function is an essential component in robot learning. Reward directly affects the sample and computational complexity of learning, and the quality of a solution. The design of informative rewards requires domain knowledge, which is not always available. We use the properties of the dynamics to produce system-appropriate reward without adding external assumptions. Specifically, we explore an approach to utilize the Lyapunov exponents of the system dynamics to generate a system-immanent reward. We demonstrate that the Sum of the Positive Lyapunov Exponents (SuPLE) is a strong candidate for the design of such a reward. We develop a computational framework for the derivation of this reward, and demonstrate its effectiveness on classical benchmarks for sample-based stabilization of various dynamical systems. It eliminates the need to start the training trajectories at arbitrary states, also known as auxiliary exploration. While the latter is a common practice in simulated robot learning, it is unpractical to consider to use it in real robotic systems, since they typically start from natural rest states such as a pendulum at the bottom, a robot on the ground, etc. and can not be easily initialized at arbitrary states. Comparing the performance of SuPLE to commonly-used reward functions, we observe that the latter fail to find a solution without auxiliary exploration, even for the task of swinging up the double pendulum and keeping it stable at the upright position, a prototypical scenario for multi-linked robots. SuPLE-induced rewards for robot learning offer a novel route for effective robot learning in typical as opposed to highly specialized or fine-tuned scenarios. Our code is publicly available for reproducibility and further research."
Speedtuning: Speeding Up Policy Execution with Lightweight Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128753,"While learned robotic policies hold promise for advancing generalizable manipulation, their practical deployment is often hindered by suboptimal execution speeds. Imitation learning policies are inherently limited by hardware constraints and the speed of the operator during data collection. In addition, there are no established methods for accelerating policies learned via imitation, and the empirical relationship between execution speed and task success remains underexplored. To address these issues, we introduce Speed Tuning, a reinforcement learning framework specifically designed to enhance the speed of manipulation policies. SPEEDTUNING learns to predict the optimal execution speed for actions, thereby complementing a base policy without necessitating additional data collection. We provide empirical evidence that SPEEDTUNING achieves substantial improvements in execution speed, exceeding 2.4x speed-up, while preserving an adequate success rate compared to both the original task policy and straightforward speed-up methods such as linear interpolation at a fixed speed. We evaluate our approach across a diverse set of dynamic and precise tasks, including pouring, throwing, and picking, demonstrating its effectiveness and robustness in enhancing real-world robotic manipulation. Videos and code are available at https://daivdyuan.github.io/speed-tuning/"
Points2Plans: From Point Clouds to Long-Horizon Plans with Composable Relational Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11127935,"We present Points2Plans, a framework for composable planning with a relational dynamics model that enables robots to solve long-horizon manipulation tasks from partial-view point clouds. Given a language instruction and a point cloud of the scene, our framework initiates a hierarchical planning procedure, whereby a language model generates a high-level plan and a sampling-based planner produces constraint-satisfying continuous parameters for manipulation primitives sequenced according to the high-level plan. Key to our approach is the use of a relational dynamics model as a unifying interface between the continuous and symbolic representations of states and actions, thus facilitating language-driven planning from high-dimensional perceptual input such as point clouds. Whereas previous relational dynamics models require training on datasets of multi-step manipulation scenarios that align with the intended test scenarios, Points2Plans uses only single-step simulated training data while generalizing zero-shot to a variable number of steps during real-world evaluations. We evaluate our approach on tasks involving geometric reasoning, multi-object interactions, and occluded object reasoning in both simulated and real-world settings. Results demonstrate that Points2Plans offers strong generalization to unseen long-horizon tasks in the real world, where it solves over 85% of evaluated tasks while the next best baseline solves only 50%."
Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and Hindsight Modular Reflections for Task Planning with LLMs.,https://doi.org/10.1109/ICRA55743.2025.11128105,"Large Language Models (LLMs) have demonstrated remarkable abilities in various language tasks, making them promising candidates for decision-making in robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose Retrieval-Augmented Hierarchical in-context reinforcement Learning (RAHL), a novel framework that decomposes complex tasks into sub-tasks using an LLM-based high-level policy, in which a complex task is decomposed into sub-tasks by a high-level policy on-the-fly. The sub-tasks, defined by goals, are assigned to the low-level policy to complete. To improve the agent's performance in multi-episode execution, we propose Hindsight Modular Reflection (HMR), where, instead of reflecting on the full trajectory, we let the agent reflect on shorter sub-trajectories using intermediate goals to improve reflection efficiency. We evaluated the decision-making ability of the proposed RAHL in three benchmark environments, ALFWorld, Webshop, and HotpotQA, where the results show that RAHL can achieve an improvement in the performance of, respectively, 9%, 42%, and 10% in 5 execution episodes compared to state-of-the-art baselines. We also implemented RAHL on the Boston Dynamics SPOT robot, which is shown to effectively scan the environment, find entrances, and navigate to new rooms controlled by the LLM policy."
Automatic Behavior Tree Expansion with LLMs for Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127942,"Robotic systems for manipulation tasks are increasingly expected to be easy to configure for new tasks or unpredictable environments, while keeping a transparent policy that is readable and verifiable by humans. We propose the method BEhavior TRee eXPansion with Large Language Models (BETR-XP-LLM) to dynamically and automatically expand and configure Behavior Trees as policies for robot control. The method utilizes an LLM to resolve errors outside the task planner's capabilities, both during planning and execution. We show that the method is able to solve a variety of tasks and failures and permanently update the policy to handle similar problems in the future."
LLM-as-BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning.,https://doi.org/10.1109/ICRA55743.2025.11128454,"Robotic assembly tasks remain an open challenge due to their long horizon nature and complex part relations. Behavior trees (BTs) are increasingly used in robot task planning for their modularity and flexibility, but creating them manually can be effort-intensive. Large language models (LLMs) have recently been applied to robotic task planning for generating action sequences, yet their ability to generate BTs has not been fully investigated. To this end, we propose LLM-as-BT-Planner, a novel framework that leverages LLMs for BT generation in robotic assembly task planning. Four in-context learning methods are introduced to utilize the natural language processing and inference capabilities of LLMs for producing task plans in BT format, reducing manual effort while ensuring robustness and comprehensibility. Additionally, we evaluate the performance of fine-tuned smaller LLMs on the same tasks. Experiments in both simulated and real-world settings demonstrate that our framework enhances LLMs' ability to generate BTs, improving success rate through in-context learning and supervised fine-tuning."
Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-Based Planner and Graph-Based Policy.,https://doi.org/10.1109/ICRA55743.2025.11127486,"Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks."
Annealed Winner-Takes-All for Motion Forecasting.,https://doi.org/10.1109/ICRA55743.2025.11128554,"In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code is made publicly available: https://github.com/valeoai/MF_aWTA."
Causal Contrastive Learning with Data Augmentations for Imitation-Based Planning.,https://doi.org/10.1109/ICRA55743.2025.11128741,"Motion planning is a difficult task, especially when generating feasible future trajectories in complex and interactive scenarios. While recent advancements in imitation-based planning have shown significant progress, this approach often encounters causal confusion in dynamic traffic environments. This confusion will cause the planner to incorrectly associate certain actions with outcomes, leading to suboptimal or unsafe plans. To address this, we introduce a novel framework called <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\overline{C}^{2}L$</tex>, which improves the planner's latent Causal understanding by incorporating Contrastive Learning and counterfactual data augmentation. Additionally, we propose a shortcut eliminator to extract copycat-free features from history states, reducing the impact of temporal spurious correlations. We validate our method on the nuPlan and interPlan benchmarks, with extensive experiments demonstrating that <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$C^{2}L$</tex> delivers highly competitive performance compared to state-of-the-art methods."
Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127996,"The autoregressive world model exhibits robust generalization capabilities in vectorized scene understanding but encounters difficulties in deriving actions due to insufficient uncertainty modeling and self-delusion. In this paper, we explore the feasibility of deriving decisions from an autoregres-sive world model by addressing these challenges through the formulation of multiple probabilistic hypotheses. We propose LatentDriver, a framework models the environment's next states and the ego vehicle's possible actions as a mixture distribution, from which a deterministic control signal is then derived. By incorporating mixture modeling, the stochastic nature of decision-making is captured. Additionally, the self-delusion problem is mitigated by providing intermediate actions sampled from a distribution to the world model. Experimen-tal results on the recently released closed-loop benchmark Waymax demonstrate that LatentDriver surpasses state-of-the-art reinforcement learning and imitation learning methods, achieving expert-level performance. The code and models will be made available at https://github.com/Sephirex-X/LatentDriver."
Autonomous Wheel Loader Navigation Using Goal-Conditioned Actor-Critic MPC.,https://doi.org/10.1109/ICRA55743.2025.11127375,"This paper proposes a novel control method for an autonomous wheel loader, enabling time-efficient navigation to an arbitrary goal pose. Unlike prior works which combine high-level trajectory planners with Model Predictive Control (MPC), we directly enhance the planning capabilities of MPC by incorporating a cost function derived from Actor-Critic Reinforcement Learning (RL). Specifically, we first train an RL agent to solve the pose reaching task in simulation, then transfer the learned planning knowledge to an MPC by incorporating the trained neural network critic as both the stage and terminal cost. We show through comprehensive simulations that the resulting MPC inherits the time-efficient behavior of the RL agent, generating trajectories that compare favorably against those found using trajectory optimization. We also deploy our method on a real-world wheel loader, where we demonstrate successful navigation in various scenarios."
Unlock the Power of Unlabeled Data in Language Driving Model.,https://doi.org/10.1109/ICRA55743.2025.11127923,"Recent Vision-based Large Language Models (VisionLLMs) for autonomous driving have seen rapid advancements. However, such promotion is extremely dependent on large-scale high-quality annotated data, which is costly and labor-intensive. To address this issue, we propose unlocking the value of abundant yet unlabeled data to improve the language-driving model in a semi-supervised learning manner. Specifically, we first introduce a series of template-based prompts to extract scene information, generating questions that create pseudo-answers for the unlabeled data based on a model trained with limited labeled data. Next, we propose a Self-Consistency Refinement method to improve the quality of these pseudo-annotations, which are later used for further training. By utilizing a pre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving Model (LDM) for driving scene question-answering, outperforming previous state-of-theart methods. Extensive experiments on the DriveLM benchmark show that our approach performs well with just 5% labeled data, achieving competitive performance against models trained with full datasets. In particular, our LDM achieves 44.85% performance with limited labeled data, increasing to 54.27 % when using unlabeled data, while models trained with full datasets reach 60.68% on the DriveLM benchmark."
CAFE-AD: Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127589,"Imitation learning based planning tasks on the nuPlan dataset have gained great interest due to their potential to generate human-like driving behaviors. However, open-loop training on the nuPlan dataset tends to cause causal confusion during closed-loop testing, and the dataset also presents a longtail distribution of scenarios. These issues introduce challenges for imitation learning. To tackle these problems, we introduce CAFE-AD, a Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving method, designed to enhance feature representation across various scenario types. We develop an adaptive feature pruning module that ranks feature importance to capture the most relevant information while reducing the interference of noisy information during training. Moreover, we propose a cross-scenario feature interpolation module that enhances scenario information to introduce diversity, enabling the network to alleviate overfitting in dominant scenarios. We evaluate our method CAFEAD, on the challenging public nuPlan Test14-Hard closed-loop simulation benchmark. The results demonstrate that CAFEAD outperforms state-of-the-art methods including rule-based and hybrid planners, and exhibits the potential in mitigating the impact of long-tail distribution within the dataset. Additionally, we further validate its effectiveness in real-world environments. The code and models will be made available at https://github.com/AlniyatRui/CAFE-AD."
Beyond Simulation: Benchmarking World Models for Planning and Causality in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127709,"World models have become increasingly popular in acting as learned traffic simulators. Recent work has explored replacing traditional traffic simulators with world models for policy training. In this work, we explore the robustness of existing metrics to evaluate world models as traffic simulators to see if the same metrics are suitable for evaluating a world model as a pseudo-environment for policy training. Specifically, we analyze the metametric employed by the Waymo Open Sim-Agents Challenge (WOSAC) and compare world model predictions on standard scenarios where the agents are fully or partially controlled by the world model (partial replay). Furthermore, since we are interested in evaluating the ego actionconditioned world model, we extend the standard WOSAC evaluation domain to include agents that are causal to the ego vehicle. Our evaluations reveal a significant number of scenarios where top-ranking models perform well under no perturbation but fail when the ego agent is forced to replay the original trajectory. To address these cases, we propose new metrics to highlight the sensitivity of world models to uncontrollable objects and evaluate the performance of world models as pseudo-environments for policy training and analyze some state-of-the-art world models under these new metrics."
Enhancing Repeatability and Reliability of Accelerated Risk Assessment in Robot Testing.,https://doi.org/10.1109/ICRA55743.2025.11128524,"Risk assessment of a robot in controlled environments, such as laboratories and proving grounds, is a common means to assess, certify, validate, verify, and characterize the robots' safety performance before, during, and even after their commercialization in the real-world. A standard testing program that acquires the risk estimate is expected to be (i) repeatable, such that it obtains similar risk assessments of the same testing subject among multiple trials or attempts with the similar testing effort by different stakeholders, and (ii) reliable against a variety of testing subjects produced by different vendors and manufacturers. Both repeatability and reliability are fundamental and crucial for a testing algorithm's validity, fairness, and practical feasibility, especially for standardization. However, these properties are rarely satisfied or ensured, especially as the subject robots become more complex, uncertain, and varied. This issue was present in traditional risk assessments through Monte-Carlo sampling, and remains a bottleneck for the recent accelerated risk assessment methods, primarily those using importance sampling. This study aims to enhance existing accelerated testing frameworks by proposing a new algorithm that provably integrates repeatability and reliability with the already established formality and efficiency. It also features demonstrations assessing the risk of instability from frontal impacts, initiated by push-over disturbances on a controlled inverted pendulum and a 7-DoF planar bipedal robot Rabbit managed by various control algorithms."
Foundation Models for Rapid Autonomy Validation.,https://doi.org/10.1109/ICRA55743.2025.11127854,"We are motivated by the problem of autonomous vehicle performance validation. A key challenge is that an autonomous vehicle requires testing in every kind of driving scenario it could encounter, including rare events, to provide a strong case for safety and show there is no edge-case pathological behavior. Autonomous vehicle companies rely on potentially millions of miles driven in realistic simulation to expose the driving stack to enough miles to estimate rates and severity of collisions. To address scalability and coverage, we propose the use of a behavior foundation model, specifically a masked autoencoder (MAE), trained to reconstruct driving scenarios. We leverage the foundation model in two complementary ways: we (i) use the learned embedding space to group qualitatively similar scenarios together and (ii) fine-tune the model to label scenario difficulty based on the likelihood of a collision upon simulation. We use the difficulty scoring as importance weighting for the groups of scenarios. The result is an approach which can more rapidly estimate the rates and severity of collisions by prioritizing hard scenarios while ensuring exposure to every kind of driving scenario."
"The Mini Wheelbot: A Testbed for Learning-based Balancing, Flips, and Articulated Driving.",https://doi.org/10.1109/ICRA55743.2025.11128020,"The Mini Wheelbot is a balancing, reaction wheel unicycle robot designed as a testbed for learning-based control. It is an unstable system with highly nonlinear yaw dynamics, non-holonomic driving, and discrete contact switches in a small, powerful, and rugged form factor. The Mini Wheelbot can use its wheels to stand up from any initial orientation - enabling automatic environment resets in repetitive experiments and even challenging half flips. We illustrate the effectiveness of the Mini Wheelbot as a testbed by implementing two popular learning-based control algorithms. First, we showcase Bayesian optimization for tuning the balancing controller. Second, we use imitation learning from an expert nonlinear MPC that uses gyroscopic effects to reorient the robot and can track higher-level velocity and orientation commands. The latter allows the robot to drive around based on user commands - for the first time in this class of robots. The Mini Wheelbot is not only compelling for testing learning-based control algorithms, but it is also just fun to work with, as demonstrated in the video of our experiments at https://youtu.be/_d7AqTRjz6g."
The Impact of Sensor Faults on Connected Autonomous Vehicle Localization.,https://doi.org/10.1109/ICRA55743.2025.11127760,"Connected autonomous vehicles (CAVs) can provide benefits over individual vehicles for precise navigation, especially in GNSS-denied environments. CAV collaboration can enhance estimation accuracy, but the safety of collaborative localization in the presence of undetected sensor faults remains underexplored. This paper introduces an integrity monitoring method for CAV collaborative localization in both centralized and decentralized implementations. Fault models for landmark and relative measurements are described, and the probability of hazardous misleading information, or integrity risk, is derived. Simulation and experimental results for notional two-CAV scenarios indicate that collaborative localization reduces integrity risk and enhances navigation safety."
Realistic Extreme Behavior Generation for Improved AV Testing.,https://doi.org/10.1109/ICRA55743.2025.11128450,"This work introduces a framework to diagnose the strengths and shortcomings of Autonomous Vehicle (AV) collision avoidance technology with synthetic yet realistic potential collision scenarios adapted from real-world, collision-free data. Our framework generates counterfactual collisions with diverse crash properties, e.g., crash angle and velocity, between an adversary and a target vehicle by adding perturbations to the adversary's predicted trajectory from a learned AV behavior model. Our main contribution is to ground these adversarial perturbations in realistic behavior as defined through the lens of data-alignment in the behavior model's parameter space. Then, we cluster these synthetic counterfactuals to identify plausible and representative collision scenarios to form the basis of a test suite for downstream AV system evaluation. We demonstrate our framework using two state-of-the-art behavior prediction models as sources of realistic adversarial perturbations, and show that our scenario clustering evokes interpretable failure modes from a baseline AV policy under evaluation."
Limits of Specifiability for Sensor-Based Robotic Planning Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128030,"There is now a large body of techniques, many based on formal methods, for describing and realizing complex robotics tasks, including those involving a variety of rich goals and time-extended behavior. This paper explores the limits of what sorts of tasks are specifiable, examining how the precise grounding of specifications -that is, whether the specification is given in terms of the robot's states, its actions and observations, its knowledge, or some other information-is crucial to whether a given task can be specified. While prior work included some description of particular choices for this grounding, our contribution treats this aspect as a first-class citizen: we introduce notation to deal with a large class of problems, and examine how the grounding affects what tasks can be posed. The results demonstrate that certain classes of tasks are specifiable under different combinations of groundings."
Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition.,https://doi.org/10.1109/ICRA55743.2025.11127637,"Employing a teleoperation system for gathering demonstrations offers the potential for more efficient learning of robot manipulation. However, teleoperating a robot arm equipped with a dexterous hand or gripper, via a teleoperation system presents inherent challenges due to the task's high dimensionality, complexity of motion, and differences between physiological structures. In this study, we introduce a novel system for joint learning between human operators and robots, that enables human operators to share control of a robot end-effector with a learned assistive agent, simplifies the data collection process, and facilitates simultaneous human demonstration collection and robot manipulation training. As data accumulates, the assistive agent gradually learns. Consequently, less human effort and attention are required, enhancing the efficiency of the data collection process. It also allows the human operator to adjust the control ratio to achieve a tradeoff between manual and automated control. We conducted experiments in both simulated environments and physical realworld settings. Through user studies and quantitative evaluations, it is evident that the proposed system could enhance data collection efficiency and reduce the need for human adaptation while ensuring the collected data is of sufficient quality for downstream tasks. For more details, please refer to our webpage https://norweig1an.github.io/HAJL.github.io/."
To Ask or not to Ask: Human-in-the-loop Contextual Bandits with Applications in Robot-Assisted Feeding.,https://doi.org/10.1109/ICRA55743.2025.11127795,"Robot-assisted bite acquisition involves picking up food items with varying shapes, compliance, sizes, and textures. Fully autonomous strategies may not generalize efficiently across this diversity. We propose leveraging feedback from the care recipient when encountering novel food items. However, frequent queries impose a workload on the user. We formulate human-in-the-loop bite acquisition within a contextual bandit framework and introduce LINUCB-QG, a method that selectively asks for help using a predictive model of querying work-load based on query types and timings. This model is trained on data collected in an online study involving 14 participants with mobility limitations, 3 occupational therapists simulating physical limitations, and 89 participants without limitations. We demonstrate that our method better balances task performance and querying workload compared to autonomous and always-querying baselines and adjusts its querying behavior to account for higher workload in users with mobility limitations. We validate this through experiments in a simulated food dataset and a user study with 19 participants, including one with severe mobility limitations. Please check out our project website at: emprise.cs.comell.edu/hilbiteacquisition/."
RoboCrowd: Scaling Robot Data Collection Through Crowdsourcing.,https://doi.org/10.1109/ICRA55743.2025.11127402,"In recent years, imitation learning from large-scale human demonstrations has emerged as a promising paradigm for training robot policies. However, the burden of collecting large quantities of human demonstrations is significant in terms of collection time and the need for access to expert operators. We introduce a new data collection paradigm, RoboCrowd, which distributes the workload by utilizing crowdsourcing principles and incentive design. RoboCrowd helps enable scalable data collection and facilitates more efficient learning of robot policies. We build RoboCrowd on top of ALOHA [1]a bimanual platform that supports data collection via puppeteeringto explore the design space for crowdsourcing in-person demonstrations in a public environment. We propose three classes of incentive mechanisms to appeal to users' varying sources of motivation for interacting with the system: material rewards, intrinsic interest, and social comparison. We instantiate these incentives through tasks that include physical rewards, engaging or challenging manipulations, as well as gamification elements such as a leaderboard. We conduct a large-scale, two-week field experiment in which the platform is situated in a university caf. We observe significant engagement with the systemover 200 individuals independently volunteered to provide a total of over 800 interaction episodes. Our findings validate the proposed incentives as mechanisms for shaping users' data quantity and quality. Further, we demonstrate that the crowdsourced data can serve as useful pre-training data for policies fine-tuned on expert demonstrationsboosting performance up to 20 % compared to when this data is not available. These results suggest the potential for RoboCrowd to reduce the burden of robot data collection by carefully implementing crowdsourcing and incentive design principles. Videos are available at https://robocrowd.github.io."
How Sound-Based Robot Communication Impacts Perceptions of Robotic Failure.,https://doi.org/10.1109/ICRA55743.2025.11127738,"One challenge in human-robot interaction is selecting communication methods that fit a given robotic system and avoid overpromising. For example, verbal speech provides a clear and easy-to-understand communication method, but can inflate expectations of robot abilities. Is verbal speech the ultimate option? Might other tactics provide similar advantages with fewer downsides? The presented work focuses on addressing these important questions by 1) quantifying any inflated opinions of robots that use verbal speech and 2) gathering perspectives on alternative nonverbal sound-based communication tactics (as a means to potentially shrink gaps between expected and actual robot performance). We conducted a within-subjects online study that varied robot communication modes in videos of successful and unsuccessful mock tasks by a modern commercial robot. Assessments of robot competence and trust after an observed robot failure were higher for verbal robots, but we observed less decline in competence and trust ratings due to the failure for a nonverbal robot using character-like sound (compared to a robot using verbal communication). Human-robot interaction practitioners can use our results to design effective and robust communication strategies for robots."
Obstacle-Avoidant Leader Following with a Quadruped Robot.,https://doi.org/10.1109/ICRA55743.2025.11127912,"Personal mobile robotic assistants are expected to find wide applications in industry and healthcare. For example, people with limited mobility can benefit from robots helping with daily tasks, or construction workers can have robots perform precision monitoring tasks on-site. However, manually steering a robot while in motion requires significant concentration from the operator, especially in tight or crowded spaces. This reduces walking speed, and the constant need for vigilance increases fatigue and, thus, the risk of accidents. This work presents a virtual leash with which a robot can naturally follow an operator. We use a sensor fusion based on a custom-built RF transponder, RGB cameras, and a LiDAR. In addition, we customize a local avoidance planner for legged platforms, which enables us to navigate dynamic and narrow environments. We successfully validate on the ANYmal platform [1] the robustness and performance of our entire pipeline in real-world experiments. The video is available at: obstacle-avoidant-leader-following."
Semantic Cross-Pose Correspondence from a Single Example.,https://doi.org/10.1109/ICRA55743.2025.11128585,"This article focuses on predicting how an object can be transformed to a semantically meaningful pose relative to another object, given only one or few examples. Current pose correspondence methods rely on vast 3D object datasets and do not actively consider semantic information, which limits the objects to which they can be applied. We present a novel method for learning cross-object pose correspondence. The proposed method detects interacting object parts, performs one-shot part correspondence, and uses geometric and visual-semantic features. Given one example of two objects posed relative to each other, the model can learn how to transfer the demonstrated relations to unseen object instances. Supplementary details can be found at https://sites.google.com/view/semantic-pose-correspondence"
H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps.,https://doi.org/10.1109/ICRA55743.2025.11127503,"Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathrm{H} 2 \mathrm{O}+$</tex>, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environments. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{H 2 O}+$</tex> over advanced cross-domain online and offline RL algorithms."
M2Distill: Multi-Modal Distillation for Lifelong Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128857,"Lifelong imitation learning for manipulation tasks poses significant challenges due to distribution shifts that occur in incremental learning steps. Existing methods often rely on unsupervised skill discovery to construct an ever-growing skill library or distillation from multiple policies, which can lead to scalability issues as diverse manipulation tasks are continually introduced and may fail to ensure a consistent latent space throughout the learning process, leading to catastrophic forgetting of previously learned skills. In this paper, we introduce M2Distill, a multimodal distillation-based method for lifelong imitation learning focusing on preserving consistent latent space across vision, language, and action distributions throughout the learning process. By regulating the shifts in latent representations across different modalities from previous to current steps, and reducing discrepancies in Gaussian Mixture Model (GMM) policies between consecutive learning steps, we ensure that the learned policy retains its ability to perform previously learned tasks while seamlessly integrating new skills. Evaluations on the LIBERO lifelong imitation learning benchmark suites, including LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL, demonstrate that our method consistently outperforms prior state-of-the-art methods across all evaluated metrics."
Expert-Enhanced Masked Point Modeling for Point Cloud Self-Supervised Learning.,https://doi.org/10.1109/ICRA55743.2025.11127773,"Recently, learning-based point cloud analysis has played a crucial role in robotic perception. Masked Point Modeling (MPM), owing to its powerful representational capabilities, has become the mainstream point cloud self-supervised learning method. However, existing MPM-based methods often suffer from the problem of negative transfer, due to the disparity in semantic distribution between upstream data and downstream data. To address this issue, we propose an expert enhancement strategy for existing MPM-based methods. Specifically, we insert a Sparse Mixture of Experts (SMoE) layer after each block of the backbone network, which utilizes a multi-branch expert architecture with routers that allocate data of different semantics to the appropriate experts for analysis. During the pre-training phase, our expert-enhanced model not only learns universal 3D representations for the backbone network but also acquires powerful semantic routing capabilities for all expert layers. In the fine-tuning phase, we freeze all backbones and conduct end-to-end fine-tuning solely on our expert layers to adaptively select multiple experts most relevant to the semantics of each downstream data for analysis. Extensive downstream experiments demonstrate the superiority of our method, especially outperforming baseline (Point-MAE) by 5.16%, 5.86%, and 4.62% in three variants of ScanObjectNN while utilizing only 12% of its trainable parameters. Our code is released at https://github.com/chenchen1104/point_e2mae."
3D Dense Captioning via Prototypical Momentum Distillation.,https://doi.org/10.1109/ICRA55743.2025.11128104,"3D dense captioning aims to describe the crucial regions in 3D visual scenes in the form of natural language. Recent prevailing approaches achieve promising results by leveraging complicated structures incorporated with large-scale models, which necessitate abundant parameters and pose challenges regarding its practical applications. Besides, with limited training data, 3D dense captioners are often susceptible to overfitting, directly degrading caption generation performance. Drawing inspiration from the recent advancements in knowledge distillation, we propose a novel approach termed Prototypical Momentum Distillation (PMD) to prompt the model to generate more detailed captions. PMD incorporates Momentum Distillation (MD) with an Uncertainty-aware Prototype-anchored Clustering (UPC) strategy to transfer knowledge by considering the uncertainty of the teacher knowledge. Specifically, we employ the original captioner as the student model and maintain an Exponential Moving Average (EMA) copy of the captioner as the teacher model to impart knowledge as the auxiliary supervision of the student. To abate the misleading caused by uncertain knowledge, we present an Uncertainty-aware Prototype-anchored Clustering (UPC) strategy to cluster the distilled knowledge according to its confidence. We then transfer the rearranged knowledge from the teacher to guide the training route of the student. We conduct extensive experiments and ablation studies on two widely used benchmark datasets, ScanRefer and Nr3D. Experimental results demonstrate that PMD outperforms all state-of-the-art approaches on the benchmarks with MLE training, highlighting its effectiveness."
Duolingo: Dynamics Utilization for Online Translation of Actions.,https://doi.org/10.1109/ICRA55743.2025.11127601,"Robots in the real world experience wear and tear, leading to changing system dynamics. This challenge is particularly exacerbated for non-rigid systems such as soft robots or robotic systems made of metamaterials with hysteresis. This setting results in a challenging problem for most learning-based controllers that typically rely on the assumption that the system dynamics remain fixed over time. In the absence of explicit mechanisms to account for this change in dynamics, learning-based control algorithms show considerable degradation in performance over time. In this work, we consider a particular class of dynamics shift in under-actuated systems, that is localized to the dynamics of the fully actuated robot itself, while independently leaving the dynamics of the environment unchanged. This captures real-world phenomena such as fatigue or hysteresis in robotic systems. In this setting, we propose an efficient algorithm that can account for dynamics shift. Using a simple calibration procedure, we propose a technique for learning a non-linear action-translation model that can capture the localized shift in dynamics. This enables continual learning and transfer despite considerable dynamics shift during the learning process. We demonstrate the efficacy of this procedure on several tasks in simulation, as well as a real-world robotic system - a 4 DoF electrically driven handed shearing auxetic (HSA) platform."
Digiforests: a Longitudinal Lidar Dataset for Forestry Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128697,"Forests are vital to our ecosystems, acting as carbon sinks, climate stabilizers, biodiversity centers, and wood sources. Due to their scale, monitoring and managing forests takes a lot of work. Forestry robotics offers the potential for enabling efficient and sustainable foresting practices through automation. Despite increasing interest in this field, the scarcity of robotics datasets and benchmarks in forest environments is hampering progress in this domain. In this paper, we present a real-world, longitudinal dataset for forestry robotics that enables the development and comparison of approaches for various relevant applications, ranging from semantic interpretation to estimating traits relevant to forestry management. The dataset consists of multiple recordings of the same plots in a forest in Switzerland during three different growth periods. We recorded the data with a mobile 3D LiDAR scanning setup. Additionally, we provide semantic annotations of trees, shrubs, and ground, instance-level annotations of trees, as well as more fine-grained annotations of tree stems and crowns. Furthermore, we provide reference field measurements of traits relevant to forestry management for a subset of the trees. Together with the data, we also provide open-source baseline panoptic segmentation and tree trait estimation approaches to enable the community to bootstrap further research and simplify comparisons in this domain."
Near Time-Optimal Hybrid Motion Planning for Timber Cranes.,https://doi.org/10.1109/ICRA55743.2025.11128003,"Efficient, collision-free motion planning is essential for automating large-scale manipulators like timber cranes. They come with unique challenges such as hydraulic actuation constraints and passive joints-factors that are seldom addressed by current motion planning methods. This paper introduces a novel approach for time-optimal, collision-free hybrid motion planning for a hydraulically actuated timber crane with passive joints. We enhance the via-point-based stochastic trajectory optimization (VP-STO) algorithm to in-clude pump flow rate constraints and develop a novel collision cost formulation to improve robustness. The effectiveness of the enhanced VP-STO as an optimal single-query global planner is validated by comparison with an informed RRT* algorithm using a time-optimal path parameterization (TOPP). The over-all hybrid motion planning is formed by combination with a gradient-based local planner that is designed to follow the global planner's reference and to systematically consider the passive joint dynamics for both collision avoidance and sway damping."
An Ultra-Light Seedling Planting Mechanism for Use in Aerial Reforestation.,https://doi.org/10.1109/ICRA55743.2025.11128303,"This article presents a novel, ultralight tree planting mechanism for use on an aerial vehicle. Current tree planting operations are typically performed manually, and existing automated solutions use large land-based vehicles or excavators which cause significant site damage and are limited to open, clear-cut plots. Our device uses a high-pressure compressed air power system and a novel double-telescoping design to achieve a weight of only 8 kg: well within the payload capacity of medium to large drones. This article describes the functionality and key components of the device and validates its feasibility through experimental testing. We propose this mechanism as a cost-effective, highly scalable solution that avoids ground damage, produces minimal emissions, and can operate equally well on open clear-cut sites as in denser, selectively-harvested forests."
Towards Autonomous Wood-Log Grasping with a Forestry Crane: Simulator and Benchmarking.,https://doi.org/10.1109/ICRA55743.2025.11127407,"Forestry machines operated in forest production environments face challenges when performing manipulation tasks, especially regarding the complicated dynamics of underactuated crane systems and the heavy weight of logs to be grasped. This study investigates the feasibility of using reinforcement learning for forestry crane manipulators in grasping and lifting heavy wood logs autonomously. We first build a simulator using Mujoco physics engine to create realistic scenarios, including modeling a forestry crane with 8 degrees of freedom from CAD data and wood logs of different sizes. We further implement a velocity controller for autonomous log grasping with deep reinforcement learning using a curriculum strategy. Utilizing our new simulator, the proposed control strategy exhibits a success rate of 96% when grasping logs of different diameters and under random initial configurations of the forestry crane. In addition, reward functions and reinforcement learning baselines are implemented to provide an open-source benchmark for the community in large-scale manipulation tasks. A video with several demonstrations can be seen at https://www.acin.tuwien.ac.at/en/d18a/."
Designing Experimental Setup Emulating Log-Loader Manipulator and Implementing Anti-Sway Trajectory Planner.,https://doi.org/10.1109/ICRA55743.2025.11128481,"Forestry machines are not easily accessible for experimentation or demonstration of research results. These mobile robots are massive, very expensive, and require a large outdoor space and permits to operate. These factors hinder conducting experiments on real forestry robots. Thus, it is essential to design experimental setups utilizing easily accessible robots in indoor labs that can effectively replicate the behavior of interest of a forestry machine. We design a setup to resemble a log-loader crane and grapple motions using a Kinova Jaco2 arm by manufacturing a specialized end-effector to attach passively to the Jaco2 arm. Passively attached grapple causes undesirable sway, which is problematic and dangerous in forestry. To address the sway problem, we employ dynamic programming to develop an anti-sway motion planner, and validate its performance for different point-to-point maneuvers in our experimental setup. We also repeat each experiment at least 6 times to ensure the repeatability and reliability of the experiments. The experimental results showcase the excellent sway-damping performance of our planner and also the very good repeatability of our experiments."
Depth Estimation Through Translucent Surfaces.,https://doi.org/10.1109/ICRA55743.2025.11127965,"In this paper, we tackle the novel computer vision problem of depth estimation through a translucent barrier. This is an important problem for robotics when manipulating objects through plastic wrapping, or when predicting the depth of items behind a translucent barrier for manipulation. We propose two approaches for providing depth prediction models the ability to see through translucent barriers: removing translucent barriers through image inpainting before passing to standard depth prediction models as input, and directly training depth models with images with translucent barriers. We show that image inpainting allows standard learned monocular and stereo depth estimation models to achieve 3 cm MAE for predicting depth of shelved items behind plastic, whereas training with real images with translucent barriers allows them to achieve centimeter or sub-centimeter MAE. We demonstrate in real robot experiments that depth-aided space estimation allows the robot to place 46 % additional items into shelves with translucent barriers. This paper also provides a publicly available dataset of objects occluded by translucent barriers in a tabletop environment and a shelf environment which will allow others to contribute to this novel problem that's critical for many robotic manipulation applications including suction gripping and item packing (available at https://sites.google.com/view/vulcan-depth-estimation)."
DroneDiffusion: Robust Quadrotor Dynamics Learning with Diffusion Models.,https://doi.org/10.1109/ICRA55743.2025.11127523,"An inherent fragility of quadrotor systems stems from model inaccuracies and external disturbances. These factors hinder performance and compromise the stability of the system, making precise control challenging. Existing model-based approaches either make deterministic assumptions, utilize Gaussian-based representations of uncertainty, or rely on nominal models, all of which often fall short in capturing the complex, multimodal nature of real-world dynamics. This work introduces DroneDiffusion, a novel framework that leverages conditional diffusion models to learn quadrotor dynamics, formulated as a sequence generation task. DroneDiffusion achieves superior generalization to unseen, complex scenarios by capturing the temporal nature of uncertainties and mitigating error propagation. We integrate the learned dynamics with an adaptive controller for trajectory tracking with stability guarantees. Extensive experiments in both simulation and real-world flights demonstrate the robustness of the framework across a range of scenarios, including unfamiliar flight paths and varying payloads, velocities, and wind disturbances. Project page: https://sites.google.com/view/dronediffusion."
FlightForge: Advancing UAV Research with Procedural Generation of High-Fidelity Simulation and Integrated Autonomy.,https://doi.org/10.1109/ICRA55743.2025.11127704,"Robotic simulators play a crucial role in the development and testing of autonomous systems, particularly in the realm of Uncrewed Aerial Vehicles (UAV). However, existing simulators often lack high-level autonomy, hindering their immediate applicability to complex tasks such as autonomous navigation in unknown environments. This limitation stems from the challenge of integrating realistic physics, photorealistic rendering, and diverse sensor modalities into a single simulation environment. At the same time, the existing photorealistic UAV simulators use mostly hand-crafted environments with limited environment sizes, which prevents the testing of long-range missions. This restricts the usage of existing simulators to only low-level tasks such as control and collision avoidance. To this end, we propose the novel FlightForge UAV opensource simulator. FlightForge offers advanced rendering capabilities, diverse control modalities, and, foremost, procedural generation of environments. Moreover, the simulator is already integrated with a fully autonomous UAV system capable of long-range flights in cluttered unknown environments. The key innovation lies in novel procedural environment generation and seamless integration of high-level autonomy into the simulation environment. Experimental results demonstrate superior sensor rendering capability compared to existing simulators, and also the ability of autonomous navigation in almost infinite environments."
Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128782,"We propose Graph2Nav, a real-time 3D object-relation graph generation framework, for autonomous navigation in the real world. Our framework fully generates and exploits both 3D objects and a rich set of semantic relationships among objects in a 3D layered scene graph, which is applicable to both indoor and outdoor scenes. It learns to generate 3D semantic relations among objects, by leveraging and advancing state-of-the-art 2D panoptic scene graph works into the 3D world via 3D semantic mapping techniques. This approach avoids previous training data constraints in learning 3D scene graphs directly from 3D data. We conduct experiments to validate the accuracy in locating 3D objects and labeling object-relations in our 3D scene graphs. We also evaluate the impact of Graph2Nav via integration with SayNav, a state-of-the-art planner based on large language models, on an unmanned ground robot to object search tasks in real environments. Our results demonstrate that modeling object relations in our scene graphs improves search efficiency in these navigation tasks."
Transferring Visual Knowledge: Semi-Supervised Instance Segmentation for Object Navigation Across Varying Height Viewpoints.,https://doi.org/10.1109/ICRA55743.2025.11128621,"The object navigation task requires robots to understand the semantic regularities in their environments. However, existing modular object navigation frameworks rely on instance segmentation models trained at fixed camera height viewpoints, limiting generalization performance and increasing labeling costs for new height viewpoints. To tackle this issue, we propose a semi-supervised method that transfers knowledge from a source height to a target height, minimizing the need for additional labels. Our approach introduces three key innovations: i) a projection policy to enhance the teacher model's detection capabilities at the target height, ii) a dynamic weight mechanism that emphasizes high-confidence pseudo-labels to reduce overfitting, and iii) a prototype contrast transferring method to transfer knowl-edge effectively. Experiments on the Habitat- Matterport 3D (HM3D) dataset show our method outperforms state-of-the-art semi-supervised techniques, improving both segmentation accuracy and navigation performance. The code is available at: https://github.com/FreeformRobotics/TransferKnowledge."
An Algorithm for Geometric Navigation Planning Under Uncertainty Using Terrain Boundary Detection.,https://doi.org/10.1109/ICRA55743.2025.11127819,"We explore a navigation planning problem under uncertainty for a simple robot with extremely limited sensing. Our robot can turn subject to significant proportional error and move forward. As it moves in an environment with a known terrain map, the robot can detect changes in the terrain at its current position. Given an initial pose and a goal segment, the robot should find some sequence of actions to travel reliably from start to goal, if such a sequence exists. The resulting plan should guarantee the robot reaches the goal segment despite any movement errors experienced within some known error bound. In this paper, we propose an algorithm to find such an action sequence, implement and evaluate this algorithm, and present evidence for the feasibility of such an algorithm in an underwater navigation setting."
Promi: an Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations.,https://doi.org/10.1109/ICRA55743.2025.11127615,"In robotics applications, few-shot segmentation is crucial because it allows robots to perform complex tasks with minimal training data, facilitating their adaptation to diverse, real-world environments. However, pixel-level annotations of even small amount of images is highly time-consuming and costly. In this paper, we present a novel few-shot binary segmentation method based on bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an efficient prototype-mixture-based method that treats the background class as a mixture of distributions. Our approach is simple, training-free, and effective, accommodating coarse annotations with ease. Compared to existing baselines, ProMi achieves the best results across different datasets with significant gains, demonstrating its effectiveness. Furthermore, we present qualitative experiments tailored to real-world mobile robot tasks, demonstrating the applicability of our approach in such scenarios. Our code: https://github.com/ThalesGroup/promi."
IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127464,"With the recent rise of large language models, vision-language models, and other general foundation models, there is growing potential for multimodal, multi-task robotics that can operate in diverse environments given natural language input. One such application is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the 3D spatial reasoning and semantic understanding required. Additionally, the language used may be imperfect or misaligned with the scene, further complicating the task. To address this challenge, we curate a benchmark dataset, IRef-VLA, for Interactive Referential Vision and Language-guided Action in 3D Scenes with imperfect references. IRef-VLA is the largest real-world dataset for the referential grounding task, consisting of over 11.5 K scanned 3D rooms from existing datasets, 7.6 M heuristically generated semantic relations, and 4.7 M referential statements. Our dataset also contains semantic object and room annotations, scene graphs, navigable free space annotations, and is augmented with statements where the language has imperfections or ambiguities. We verify the generalizability of our dataset by evaluating with state-of-the-art models to obtain a performance baseline and also develop a graphsearch baseline to demonstrate the performance bound and generation of alternatives using scene-graph knowledge. With this benchmark, we aim to provide a resource for 3D scene understanding that aids the development of robust, interactive navigation systems. The dataset and all source code is publicly released<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>https://github.com/HaochenZ11/IRef-VLA."
Efficient Non-Myopic Layered Bayesian Optimization for Large-Scale Bathymetric Informative Path Planning.,https://doi.org/10.1109/ICRA55743.2025.11128314,"Informative path planning (IPP) applied to bathy-metric mapping allows AUVs to focus on feature-rich areas to quickly reduce uncertainty and increase mapping efficiency. Existing methods based on Bayesian optimization (BO) over Gaussian Process (GP) maps work well on small scenarios but they are short-sighted and computationally heavy when mapping larger areas, hindering deployment in real applications. To overcome this, we present a 2-layered BO IPP method that performs non-myopic, online planning in a tree search fashion over large Stochastic Variational GP maps, while respecting the AUV dynamical constraints and accounting for localization uncertainty. Our framework outperforms the standard industrial lawn-mowing pattern and a myopic baseline in a set of hardware in the loop (HIL) experiments in an embedded platform over real bathymetry areas."
Visual Lidar Recursive Online Tracker (ViLiROT) for Autonomous Surface Vessels.,https://doi.org/10.1109/ICRA55743.2025.11127849,"We propose a multi-sensor fusion pipeline for multiple object tracking in autonomous surface vessels using lidar and camera data. Our approach follows the tracking-by-detection paradigm, leveraging the precision of lidar for accurate state estimation and camera data for robust association. The method addresses issues with false tracks from lidar returns by suppressing non-moving objects on the basis of optical flow. We compare the proposed pipeline against prior work, particularly in the use of lidar and stereo cameras as depth modalities, demonstrating its effectiveness in improving tracking performance."
Open-Set Semantic Uncertainty Aware Metric-Semantic Graph Matching.,https://doi.org/10.1109/ICRA55743.2025.11128388,"Underwater object-level mapping requires incorporating visual foundation models to handle the uncommon and often previously unseen object classes encountered in marine scenarios. In this work, a metric of semantic uncertainty for open-set object detections produced by visual foundation models is calculated and then incorporated into an object-level uncertainty tracking framework. Object-level uncertainties and geometric relationships between objects are used to enable robust object-level loop closure detection for unknown object classes. The above loop closure detection problem is formulated as a graph matching problem. While graph matching, in general, is NP-Complete, a solver for an equivalent formulation of the proposed graph matching problem as a graph editing problem is tested on multiple challenging underwater scenes. Results for this solver as well as three other solvers demonstrate that the proposed methods are feasible for real-time use in marine environments for the robust, open-set, multi-object, semantic-uncertainty-aware loop closure detection. Further experimental results on the KITTI dataset demonstrate that the method generalizes to large-scale terrestrial scenes."
Towards Autonomous Verification: Integrating Cognitive AI and Semantic Digital Twins in Medical Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127803,"In medical laboratory environments, where pre-cision and safety are critical, the deployment of autonomous robots requires not only accurate object manipulation but also the ability to verify task success to comply with regulatory requirements. This paper introduces a novel imagination-enabled perception framework that integrates cognitive AI with semantic digital twins to allow medical robots to sim-ulate task outcomes, compare them with real-world results, and autonomously verify the success of their actions. Our approach addresses challenges related to handling small and transparent objects commonly found in sterility testing kits and other related consumables. By enhancing the RoboKudo perception system with parthood-based reasoning, we enable more accurate task verification through focused attention on object subparts. Experiments show that our system significantly improves performance compared to traditional object-centric methods, increasing accuracy in complex environments without the need for extensive retraining. This work demonstrates a novel concept in making robotic systems more adaptable and reliable for critical tasks in medical laboratories."
SC-Former: A Segmentation Convolution Transformer for Lung Surgery Robots.,https://doi.org/10.1109/ICRA55743.2025.11128586,"For lung surgery robots, the precise segmentation of pulmonary fissures is very important. Damaging the inter-lobar fissures during surgery can have serious consequences. Accurately segmenting weak and abnormal fissures commonly found in clinical CT scans remains a challenging task. To solve the above problem, we aimed to develop a novel Convolution Transformer for accurate fissure segmentation (SC-Former). The proposed SC-Former adopts an encoder, attention block, and decoder structure. First, we designed an encoder with a hybrid CNNs-transformer block that ingeniously amalgamates coordinate convolution and coordinate transformer to effectively capture both local and global feature information. Second, we introduced the long skip connections of our designed attention block at layers of the decoder-encoder structure to emphasize the field of view for fissures. Third, we added the distance map strategy to alleviate the challenge of training the network to segment the false positives from the complex textures in the lung. Fourth, we developed a multi-scale supervision strategy for independent prediction at various decoder levels, effectively integrating multi-scale semantic information to facilitate the segmentation of weak and abnormal fissures. Because of the lack of open-source inter-pulmonary fissure datasets, we collected 3D CT scans from 400 participants in the clinical trial and created a new high-quality dataset: BMI dataset. Extensive experiments on this dataset revealed the great superiority of our method over several state-of-the-art competitors. The ablation study also validated the effectiveness and robustness of each part of SC-Former."
Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic.,https://doi.org/10.1109/ICRA55743.2025.11127725,"Optimal path planning requires finding a series of feasible states from the starting point to the goal to optimize objectives. Popular path planning algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to guide the search. Effective heuristics are accurate and computationally efficient, but achieving both can be challenging due to their conflicting nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based planner that focuses on optimizing the search direction for each edge, resulting in goal bias during exploration. We define edges as generalized vectors and integrate similarity indexes to establish a directional filter that selects the nearest neighbors and estimates direction costs. The estimated direction cost heuristics are utilized in edge evaluation. This strategy allows the exploration to share directional information efficiently. DIT* convergence faster than existing single-query, sampling-based planners on tested problems in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbb{R}^{4}$</tex> to <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbb{R}^{16}$</tex> and has been demonstrated in real-world environments with various planning tasks. A video showcasing our experimental results is available at: https://youtu.be/2SX6QT2NOek."
Optimal Motion Planning for a Class of Dynamical Systems.,https://doi.org/10.1109/ICRA55743.2025.11128122,"A novel method for optimal motion planning in the context of a class of dynamical system is proposed in this work. Our approach is based on the design of a provably safe and convergent actor structure, which is optimized via a policy iteration method. The proposed actor has wide applications, from control of mechanical systems to providing acceleration commands for more complex robotic platforms. Extra care is taken to provide theoretical guarantees, and the scheme is validated against an existing sampling-based planner."
Asymptotically-Optimal Multi-Query Path Planning for a Polygonal Robot.,https://doi.org/10.1109/ICRA55743.2025.11127599,"Shortest-path roadmaps, also known as reduced visibility graphs, provide a highly efficient multi-query method for computing optimal paths in two-dimensional environments. Combined with Minkowski sum computations, shortest-path roadmaps can compute optimal paths for a translating robot in 2D. In this study, we explore the intuitive idea of stacking up a set of reduced visibility graphs at different orientations for a polygonal holonomic robot to support the fast computation of near-optimal paths, allowing simultaneous 2D translation and rotation. The resulting algorithm, rotation-stacked visibility graph (RVG), is shown to be resolution-complete and asymptotically optimal. Extensive computational experiments show RVG significantly outperforms state-of-the-art single- and multiquery sampling-based methods on both computation time and solution optimality fronts. Source code and supplementary materials are available at https://github.com/arc-1/rvg."
Asymptotically Optimal Sampling-Based Motion Planning Through Anytime Incremental Lazy Bidirectional Heuristic Search.,https://doi.org/10.1109/ICRA55743.2025.11128259,"This paper introduces Bidirectional Lazy Informed Trees (BLIT*), the first algorithm to incorporate anytime incremental lazy bidirectional heuristic search (Bi-HS) into batch-wise sampling-based motion planning (Bw-SBMP). BLIT* operates on batches of informed states (states that can potentially improve the cost of the incumbent solution) structured as an implicit random geometric graph (RGG). The computational cost of collision detection is mitigated via a new lazy edge-evaluation strategy by focusing on states near obstacles. Experimental results, especially in high dimensions, show that BLIT* outperforms existing Bw-SBMP planners by efficiently finding an initial solution and effectively improving the quality as more computational resources are available."
Propagative Distance Optimization for Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11128392,"This paper focuses on the motion planning problem for serial articulated robots with revolute joints under kinematic constraints. Many motion planners leverage iterative local optimization methods but are often trapped in local minima due to non-convexity of the problem. A key reason for the non-convexity is the trigonometric term when parameterizing the kinematics using joint angles. Recent distance-based formulations can eliminate these trigonometric terms by formulating the kinematics based on distances, and has shown superior performance against classic joint angle based formulations in domains like inverse kinematics (IK). However, distance-based kinematics formulations have not yet been studied for motion planning, and naively applying them for motion planning may lead to poor computational efficiency. In particular, IK seeks one configuration while motion planning seeks a sequence of configurations, which greatly increases the scale of the underlying optimization problem. This paper proposes Propagative Distance Optimization for Motion Planning (PDOMP), which addresses the challenge by (i) introducing a new compact representation that reduces the number of variables in the distance-based formulation, and (ii) leveraging the chain structure to efficiently compute forward kinematics and Jacobians of the robot among waypoints along a path. Test results show that PDOMP runs up to 10 times faster than the sampling-based and angle-based-optimization baseline methods."
Dynamically Feasible Path Planning in Cluttered Environments via Reachable BZier Polytopes.,https://doi.org/10.1109/ICRA55743.2025.11127577,"The deployment of robotic systems in real world environments requires the ability to quickly produce paths through cluttered, non-convex spaces. These planned trajectories must be both kinematically feasible (i.e., collision free) and dynamically feasible (i.e., satisfy the underlying system dynamics), necessitating a consideration of both the free space and the dynamics of the robot in the path planning phase. In this work, we explore the application of reachable Bzier polytopes as an efficient tool for generating trajectories satisfying both kinematic and dynamic requirements. Furthermore, we demonstrate that by offloading specific computation tasks to the GPU, such an algorithm can meet tight real time requirements. We propose a layered control architecture that efficiently produces collision free and dynamically feasible paths for nonlinear control systems, and demonstrate the framework on the tasks of 3D hopping in a cluttered environment."
Scenario-Based Curriculum Generation for Multi-Agent Driving.,https://doi.org/10.1109/ICRA55743.2025.11128162,"The automated generation of diversified training scenarios has been an important ingredient in many complex learning tasks, especially in real-world application domains such as autonomous driving, where auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered a tedious and time-consuming task, especially in more complex simulation environments. To this end, we introduce MATS-Gym, a multi-agent training framework for autonomous driving that uses partial-scenario specifications to generate traffic scenarios with a variable number of agents which are executed in CARLA, a high-fidelity driving simulator. MATS-Gym reconciles scenario execution engines, such as Scenic and ScenarioRunner, with established multi-agent training frameworks where the interaction between the environment and the agents is modeled as a partially observable stochastic game. Furthermore, we integrate MATSGym with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula, which is the first application of such algorithms to the domain of autonomous driving. The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym."
Multi-Robot Collaboration Through Reinforcement Learning and Abstract Simulation.,https://doi.org/10.1109/ICRA55743.2025.11127323,"Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multiagent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world."
Safety and Naturalness Perceptions of Robot-to-Human Handovers Performed by Data-Driven Robotic Mimicry of Human Givers.,https://doi.org/10.1109/ICRA55743.2025.11128747,"We study human perceptions of a robot that performs robot-to-human (R2H) handovers controlled to grasp, transport, and transfer 34 objects by mimicking human givers in human-human (H2H) handover data. Recognizing the importance of human-like robotic behavior for successful collaboration, R2H studies use models of human behavior or observations of H2H data to plan robot giver motion. However, R2H studies have been limited in object counts. In this work, we use the Human-Object-Human (HOH) dataset, consisting of H2H interactions performed by 20 giver-receiver pairs with 136 objects, to conduct an R2H study with 34 objects. We teleoperate a Kinova Gen3 manipulator to grip an object as grasped by an HOH human giver, and program it to automatically transport and orient the object to a participant by mimicking the HOH giver's trajectory and transfer pose. We survey participants on safety, naturalness, and preferred choice over linear trajectory and random orientation baselines. We find that transfer pose influences perceptions of naturalness, with HOH poses showing higher naturalness ratings. Participants prefer handovers with HOH end poses when asked to pick their preferred interaction."
Integrating Human-Robot Teaming Dynamics Into Mission Planning Tools for Transparent Tactics in Multi-Robot Human Integrated Teams.,https://doi.org/10.1109/ICRA55743.2025.11128530,"This research aims to demonstrate how integrating human-robot teaming dynamics into mission planning tools impacts the abilities of robot operators as they coordinate multiple robot agents during a mission. This was investigated in a pilot study using two inter-robot collaboration modalities and interface tools, which required different human-robot interaction techniques to execute a mission with a team of four robots. In the first modality, the operator manually inserted waypoints for each robot, as they acted as individual agents. In the second modality, the operator used the Planning Execution to After-Action Review (PETAAR) toolset to plot a single waypoint for the team of robots, as the robots coordinated their movement as a group. One novel component of this study is the investigation of how human-robot teaming dynamics and the PETAAR toolset impacted robot operators' real-time situation awareness and perceived cognitive load as well as team performance. Although the teaming modalities differed greatly with respect to the level of operator input needed, the time required to complete the simulation, the participant's perceived cognitive load, and interface usability were very similar for both modalities. In contrast, the results revealed statistically significant differences between the two teaming modalities related to participants' abilities to maintain a wedge formation while remaining situationally aware. Results from this work will be used to guide development of PETAAR along with the design of future studies investigating more complex teaming scenarios and for creating a baseline for comparing future results."
CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics for Metric 3D Shape Recovery.,https://doi.org/10.1109/ICRA55743.2025.11128515,"Recovering the metric 3D shape from a single image is particularly relevant for robotics and embodied in-telligence applications, where accurate spatial understanding is crucial for navigation and interaction with environments. Usu-ally, the mainstream approaches achieve it through monocular depth estimation. However, without camera intrinsics, the 3D metric shape can not be recovered from depth alone. In this study, we theoretically demonstrate that depth serves as a 3D prior constraint for estimating camera intrinsics and uncover the reciprocal relations between these two elements. Motivated by this, we propose a collaborative learning framework for jointly estimating depth and camera intrinsics, named CoL3D, to learn metric 3D shapes from single images. Specifically, CoL3D adopts a unified network and performs collaborative optimization at three levels: depth, camera intrinsics, and 3D point clouds. For camera intrinsics, we design a canonical incidence field mechanism as a prior that enables the model to learn the residual incident field for enhanced calibration. Additionally, we incorporate a shape similarity measurement loss in the point cloud space, which improves the quality of 3D shapes essential for robotic applications. As a result, when training and testing on a single dataset with in-domain settings, CoL3D delivers outstanding performance in both depth estimation and camera calibration across several indoor and outdoor benchmark datasets, which leads to remarkable 3D shape quality for the perception capabilities of robots."
Non-Destructive 3D Root Structure Modeling.,https://doi.org/10.1109/ICRA55743.2025.11127702,"Deep neural networks (DNNs) have gained significant attention in 3D object reconstruction. However, detecting and reconstructing hidden or buried objects underground remains a challenging task. Ground Penetrating Radar (GPR) has emerged as a cost-effective and non-destructive technology for subsurface object detection, including soil structures and pipelines. In this study, we present a deep convolutional neural network-based method for detecting target signals and performing curve parameter regression using multiple B-scans from GPR data. By leveraging the detection and regression outcomes, we further generate fitted curves that represent underground structures. To reconstruct a comprehensive and detailed 3D root structure, we design a shape reconstruction network that takes sparse sliced 3D points as input. The proposed approach is extensively trained and validated using synthetic 3D root datasets and simulated GPR data generated with gprMax. Additionally, the trained model demonstrates strong generalization capabilities when applied to real-world GPR data, ensuring its practical applicability."
PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration.,https://doi.org/10.1109/ICRA55743.2025.11128129,"In this paper, we present PTZ-Calib, a robust two-stage PTZ camera calibration method, that efficiently and accurately estimates camera parameters for arbitrary viewpoints. Our method includes an offline and an online stage. In the offline stage, we first uniformly select a set of reference images that sufficiently overlap to encompass a complete 360 view. We then utilize the novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to automatically calibrate the cameras within a local coordinate system. Additionally, for practical application, we can further optimize camera parameters and align them with the geographic coordinate system using extra global reference 3D information. In the online stage, we formulate the calibration of any new viewpoints as a relocalization problem. Our approach balances the accuracy and computational efficiency to meet real-world demands. Extensive evaluations demonstrate our robustness and superior performance over state-of-the-art methods on various real and synthetic datasets. Datasets and source code can be accessed online at https://github.com/gjgjh/PTZ-Calib"
CtRNet-X: Camera-to-Robot Pose Estimation in Real-World Conditions using a Single Camera.,https://doi.org/10.1109/ICRA55743.2025.11128065,"Camera-to-robot calibration is crucial for visionbased robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to visionbased robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of realworld manipulation scenarios."
Foundation Feature-Driven Online End-Effector Pose Estimation: A Marker-Free and Learning-Free Approach.,https://doi.org/10.1109/ICRA55743.2025.11128327,"Accurate transformation estimation between camera space and robot space is essential. Traditional methods using markers for hand-eye calibration require offline image collection, limiting their suitability for online self-calibration. Recent learning-based robot pose estimation methods, while advancing online calibration, struggle with cross-robot generalization and require the robot to be fully visible. This work proposes a Foundation feature-driven online End-Effector Pose Estimation (FEEPE) algorithm, characterized by its training-free and cross end-effector generalization capabilities. Inspired by the zero-shot generalization capabilities of foundation models, FEEPE leverages pre-trained visual features to estimate 2D-3D correspondences derived from the CAD model and target image, enabling 6D pose estimation via the PnP algorithm. To resolve ambiguities from partial observations and symmetry, a multi-historical key frame enhanced pose optimization algorithm is introduced, utilizing temporal information for improved accuracy. Compared to traditional hand-eye calibration, FEEPE enables marker-free online calibration. Unlike robot pose estimation, it generalizes across robots and end-effectors in a training-free manner. Extensive experiments demonstrate its superior flexibility, generalization, and performance. Additional demon-strations are available at https://feepose.github.io/"
A Modeling and Control Strategy for the Gaze-Guided Teleoperation of Robotic Manipulators via Smart Glasses.,https://doi.org/10.1109/ICRA55743.2025.11127772,"Object manipulation is a high-frequency task required in assistive robotic systems in order to aid the elderly or those with disabilities that impact motor control. In the instance where arms cannot be used to command a robot, gaze-tracking via smart glasses is a suitable candidate. In this work, we develop a modeling method and model-based filtering and control strategy for direct gaze-guided teleoperation of robotic manipulators. We demonstrate the feasibility of this control strategy in an object manipulation case study with six participants. The results indicate that a model-based gaze filtering and control strategy produces smooth commands for the robot that are easy for the participants to use. These methods can reduce the perceived workload of the user by 37.51% and lower the gripper positioning error by 39.09% compared to using unfiltered gaze data."
Unlocking Potential: Gaze-Based Interfaces in Assistive Robotics for Users with Severe Speech and Motor Impairment.,https://doi.org/10.1109/ICRA55743.2025.11128213,"Individuals with Severe Speech and Motor Impairment (SSMI) struggle to interact with their surroundings due to physical and communicative limitations. To address these challenges, this paper presents a gaze-controlled robotic system that helps SSMI users perform stamp printing tasks. The system includes gaze-controlled interfaces and a robotic arm with a gripper, designed specifically for SSMI users to enhance accessibility and interaction. User studies with gazecontrolled interfaces such as video see-through (VST), video pass-through (VPT), and optical see-through (OST) displays demonstrated the system's effectiveness. Results showed that VST had the average stamping time of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$28.45 s(SD = 15.44s)$</tex> and the average stamp count <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$7.36(SD = 3.83)$</tex>, outperforming VPT and OST."
Do Looks Matter? Exploring Functional and Aesthetic Design Preferences for a Robotic Guide Dog.,https://doi.org/10.1109/ICRA55743.2025.11128599,"Dog guides offer an effective mobility solution for blind or visually impaired (BVI) individuals, but conventional dog guides have limitations including the need for care, potential distractions, societal prejudice, high costs, and limited availability. To address these challenges, we seek to develop a robot dog guide capable of performing the tasks of a conventional dog guide, enhanced with additional features. In this work, we focus on design research to identify functional and aesthetic design concepts to implement into a quadrupedal robot. The aesthetic design remains relevant even for BVI users due to their sensitivity toward societal perceptions and the need for smooth integration into society. We collected data through interviews and surveys to answer specific design questions pertaining to the appearance, texture, features, and method of controlling and communicating with the robot. Our study identified essential and preferred features for a future robot dog guide, which are supported by relevant statistics aligning with each suggestion. These findings will inform the future development of user-centered designs to effectively meet the needs of BVI individuals."
Comparison of Three Interface Approaches for Gaze Control of Assistive Robots for Individuals with Tetraplegia.,https://doi.org/10.1109/ICRA55743.2025.11127558,"Individuals with tetraplegia have their independence and quality of life severely affected. Assistive robotic arms can enhance their autonomy, but effective control interfaces are essential for optimizing their usability and performance. This study aims to evaluate the performance and user experience of three control interfaces for an assistive robotic arm: Graphical User Interfaces (GUI), Embedded Interface, and Directional Gaze. Thirty-three able-bodied participants were recruited to control an assistive robotic arm through the three different interfaces in a between-subjects experiment. Performance was measured using the Yale-CMU-Berkeley (YCB) Block Pick and Place Protocol. Usability (SUS) and task, workload (NASATLX) were measured through subjective questionnaires. Additionally, we report saccades per minute and fixation duration. The results revealed statistically significant differences showing that Embedded and GUI interfaces, when compared to the Directional Gaze interface, can lead to lower workloads and higher performance in pick-up tasks."
Unsupervised Domain Adaptation for Gait State Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128510,"Exoskeleton controllers have recently employed machine learning (ML) techniques to provide appropriate assistance throughout the terrains of the real world. One successful approach has been to learn a mapping between an exoskeleton wearer's kinematic measurements and a gait state vector that encodes how the wearer is currently walking (i.e. gait phase, speed), and then dynamically update the assistance based on the gait state. However, these methods require paired datasets of input kinematics to output gait states, which usually involves manual, time-consuming labeling of data from participants wearing specific exoskeletons and thus limits the scalability of these ML methods. A prior solution to this challengeleveraging large pre-labeled datasets of normative human walkingintroduces another problem, in that networks trained on these datasets learn only normative locomotion patterns, and thus may deteriorate when the data are changed by wearing the exoskeleton itself. In this context, we present an unsupervised-learning-based approach to both bypass the requirement of labeled data for gait state prediction and address the difficulty of domain adaptation from normative to exoskeleton-assisted walking. We validate our method in a set of walking simulations that featured exoskeleton data from 14 participants. This model showed significant improvements in state estimation relative to a model trained solely on pre-labeled normative walking, while also not requiring ground truth labels. This work presents a foundation that demonstrates labeled, device-specific data may not be required for predicting walking behavior in real time."
Anti-Sensing: Defense Against Unauthorized Radar-Based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators.,https://doi.org/10.1109/ICRA55743.2025.11127941,"Recent advancements in Ultra-Wideband (UWB) radar technology have enabled contactless, non-line-of-sight vital sign monitoring, making it a valuable tool for healthcare. However, UWB radar's ability to capture sensitive physiological data, even through walls, raises significant privacy concerns, particularly in human-robot interactions and autonomous systems that rely on radar for sensing human presence and physiological functions. In this paper, we present Anti-Sensing, a novel defense mechanism designed to prevent unauthorized radarbased sensing. Our approach introduces physically realizable perturbations, such as oscillatory motion from wearable devices, to disrupt radar sensing by mimicking natural cardiac motion, thereby misleading heart rate (HR) estimations. We develop a gradient-based algorithm to optimize the frequency and spatial amplitude of these oscillations for maximal disruption while ensuring physiological plausibility. Through both simulations and real-world experiments with radar data and neural networkbased HR sensing models, we demonstrate the effectiveness of Anti-Sensing in significantly degrading model accuracy, offering a practical solution for privacy preservation."
Vision-Based Fuzzy Control System with Intention Detection for Smart Walkers: Enhancing Usability for Stroke Survivors with Unilateral Upper Limb Impairments.,https://doi.org/10.1109/ICRA55743.2025.11127891,"Mobility impairments, particularly those caused by stroke-induced hemiparesis, significantly impact independence and quality of life. Current smart walker controllers operate by using input forces from the user to control linear motion and input torques to dictate rotational movement; however, because they predominantly rely on user-applied torque exerted on the device handle as an indicator of user intent to turn, they fail to adequately accommodate users with unilateral upper limb impairments. This leads to increased physical strain and cognitive load. This paper introduces a novel smart walker equipped with a fuzzy control algorithm that leverages shoulder abduction angles to intuitively interpret user intentions using just one functional hand. By integrating a force sensor and stereo camera, the system enhances walker responsiveness and usability. Experimental evaluations with five participants showed that the fuzzy controller outperformed the traditional admittance controller, reducing wrist torque while using the right hand to operate the walker by 12.65 % for left turns, 80.36 % for straight paths, and 81.16 % for right turns. Additionally, average user comfort ratings on a Likert scale increased from 1 to 4. Results confirmed a strong correlation between shoulder abduction angles and directional intent, with users reporting decreased effort and enhanced ease of use. This study contributes to assistive robotics by providing an adaptable control mechanism for smart walkers, suggesting a pathway towards enhancing mobility and independence for individuals with mobility impairments. Project page: https://tbs-ualberta.github.io/fuzzy-sw/"
EPRecon: An Efficient Framework for Real-Time Panoptic 3D Reconstruction from Monocular Video.,https://doi.org/10.1109/ICRA55743.2025.11128258,"Panoptic 3D reconstruction from a monocular video is a fundamental perceptual task in robotic scene understanding. However, existing efforts suffer from inefficiency in terms of inference speed and accuracy, limiting their practical applicability. We present EPRecon, an efficient real-time panoptic 3D reconstruction framework. Current volumetric-based reconstruction methods usually utilize multi-view depth map fusion to obtain scene depth priors, which is time-consuming and poses challenges to real-time scene reconstruction. To address this issue, we propose a lightweight module to directly estimate scene depth priors in a 3D volume for reconstruction quality improvement by generating occupancy probabilities of all voxels. In addition, compared with existing panoptic segmentation methods, EPRecon extracts panoptic features from both voxel features and corresponding image features, obtaining more detailed and comprehensive instance-level semantic information and achieving more accurate segmentation results. Experimental results on the ScanNet V2dataset demonstrate the superiority of EPRecon over current state-of-the-art methods in terms of both panoptic 3D reconstruction quality and real-time inference. Code is available at https://github.com/zhen6618/EPRecon."
Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems.,https://doi.org/10.1109/ICRA55743.2025.11128255,"This paper presents a unified surface reconstruction and rendering framework for LiDAR-visual systems, integrating Neural Radiance Fields (NeRF) and Neural Distance Fields (NDF) to recover both appearance and structural information from posed images and point clouds. We address the structural visible gap between NeRF and NDF by utilizing a visible-aware occupancy map to classify space into the free, occupied, visible unknown, and background regions. This classification facilitates the recovery of a complete appearance and structure of the scene. We unify the training of the NDF and NeRF using a spatial-varying scale SDF-to-density transformation for levels of detail for both structure and appearance. The proposed method leverages the learned NDF for structure-aware NeRF training by an adaptive sphere tracing sampling strategy for accurate structure rendering. In return, NeRF further refines structural in recovering missing or fuzzy structures in the NDF. Extensive experiments demonstrate the superior quality and versatility of the proposed method across various scenarios. To benefit the community, the codes will be released at https://github.com/hku-mars/M2Mapping."
LVBA: LiDAR-Visual Bundle Adjustment for RGB Point Cloud Mapping.,https://doi.org/10.1109/ICRA55743.2025.11127400,"Point cloud maps with accurate color are crucial in robotics and mapping applications. Existing approaches for producing RGB-colorized maps are primarily based on realtime localization using filter-based estimation or sliding window optimization, which may lack accuracy and global consistency. In this work, we introduce a novel global LiDAR-Visual bundle adjustment (BA) named LVBA to improve the quality of RGB point cloud mapping beyond existing baselines. LVBA first optimizes LiDAR poses via a global LiDAR BA, followed by a photometric visual BA incorporating planar features from the LiDAR point cloud for camera pose optimization. Additionally, to address the challenge of map point occlusions in constructing optimization problems, we implement a novel LiDAR-assisted global visibility algorithm in LVBA. To evaluate the effectiveness of LVBA, we conducted extensive experiments by comparing its mapping quality against existing state-of-the-art baselines (i.e., <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{R}^{3}$</tex> LIVE and FAST-LIVO). Our results prove that LVBA can proficiently reconstruct high-fidelity, accurate RGB point cloud maps, outperforming its counterparts."
LiDAR-enhanced 3D Gaussian Splatting Mapping.,https://doi.org/10.1109/ICRA55743.2025.11127556,"This paper introduces LiGSM, a novel LiDARenhanced 3D Gaussian Splatting (3DGS) mapping framework that improves the accuracy and robustness of 3D scene mapping by integrating LiDAR data. LiGSM constructs joint loss from images and LiDAR point clouds to estimate the poses and optimize their extrinsic parameters, enabling dynamic adaptation to variations in sensor alignment. Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a denser and more reliable starting points compared to sparse SfM points. In scene rendering, the framework augments standard image-based supervision with depth maps generated from LiDAR projections, ensuring an accurate scene representation in both geometry and photometry. Experiments on public and self-collected datasets demonstrate that LiGSM outperforms comparative methods in pose tracking and scene rendering."
Foresee and Act Ahead: Task Prediction and Pre-Scheduling Enabled Efficient Robotic Warehousing.,https://doi.org/10.1109/ICRA55743.2025.11127751,"In warehousing systems, to enhance efficiency amid surging demand volumes, much attention has been placed on how to reasonably allocate tasks of delivery to robots. However, the labor of robots is still inevitably wasted to some extent. In this paper, we propose a pre-scheduling enhanced warehousing framework aiming to foresee and act in advance, which consists of task flow prediction and hybrid task allocation. For task prediction, we design the spatio-temporal representations of the task flow and introduce a periodicity-decoupled mechanism tailored for the generation patterns of aggregated orders, and then further extract spatial features of task distribution with a novel combination of graph structures. In hybrid tasks allocation, we consider the known tasks and predicted future tasks simultaneously to optimize the task allocation. In addition, we consider factors such as predicted task uncertainty and sector-level efficiency to realize more balanced and rational allocations. We validate our task prediction model across datasets derived from factories, achieving SOTA performance. Furthermore, we implement our system in a real-world robotic warehouse, demonstrating more than 30% improvements in efficiency."
Embodiment-agnostic Action Planning via Object-Part Scene Flow.,https://doi.org/10.1109/ICRA55743.2025.11127873,"Observing that the key for robotic action planning is to understand the target-object motion when its associated part is manipulated by the end effector, we propose to generate the 3D object-part scene flow and extract its transformations to solve the action trajectories for diverse embodiments. The advantage of our approach is that it derives the robot action explicitly from object motion prediction, yielding a more robust policy by understanding the object motions. Also, beyond policies trained on embodiment-centric data, our method is embodiment-agnostic, generalizable across diverse embodiments, and being able to learn from human demonstrations. Our method comprises three components: an object-part predictor to locate the part for the end effector to manipulate, an RGBD video generator to predict future RGBD videos, and a trajectory planner to extract embodiment-agnostic transformation sequences and solve the trajectory for diverse embodiments. Trained on videos even without trajectory data, our method still outperforms existing works significantly by 27.7% and 26.2% on the prevailing virtual environments MetaWorld and Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments, showing that our policy, trained only with human demonstration, can be deployed to various embodiments."
Acoustic Wave Manipulation Through Sparse Robotic Actuation.,https://doi.org/10.1109/ICRA55743.2025.11128588,"Recent advancements in robotics, control, and machine learning have facilitated progress in the challenging area of object manipulation. These advancements include, among others, the use of deep neural networks to represent dynamics that are partially observed by robot sensors, as well as effective control using sparse control signals. In this work, we explore a more general problem: the manipulation of acoustic waves, which are partially observed by a robot capable of influencing the waves through spatially sparse actuators. This problem holds great potential for the design of new artificial materials, ultrasonic cutting tools, energy harvesting, and other applications. We develop an efficient data-driven method for robot learning that is applicable to either focusing scattered acoustic energy in a designated region or suppressing it, depending on the desired task. The proposed method is better in terms of a solution quality and computational complexity as compared to a state-of-the-art learning based method for manipulation of dynamical systems governed by partial differential equations. Furthermore our proposed method is competitive with a classical semi-analytical method in acoustics research on the demonstrated tasks. We have made the project code publicly available, along with a web page featuring video demonstrations: https://gladisor.github.io/waves/."
Integrating Model-Based Control and RL for Sim2Real Transfer of Tight Insertion Policies.,https://doi.org/10.1109/ICRA55743.2025.11128860,"Object insertion under tight tolerances (<Imm) is an important but challenging assembly task as even small errors can result in undesirable contacts. Recent efforts focused on Reinforcement Learning (RL), which often depends on careful definition of dense reward functions. This work proposes an effective strategy for such tasks that integrates traditional model-based control with RL to achieve improved insertion accuracy. The policy is trained exclusively in simulation and is zero-shot transferred to the real system. It employs a potential field-based controller to acquire a model-based policy for inserting a plug into a socket given full observability in simulation. This policy is then integrated with residual RL, which is trained in simulation given only a sparse, goal-reaching reward. A curriculum scheme over observation noise and action magnitude is used for training the residual RL policy. Both policy components use as input the SE(3) poses of both the plug and the socket and return the plug's SE (3) pose transform, which is executed by a robotic arm using a controller. The integrated policy is deployed on the real system without further training or fine-tuning, given a visual SE (3) object tracker. The proposed solution and alternatives are evaluated across a variety of objects and conditions in simulation and reality. The proposed approach outperforms recent RL-based methods in this domain and prior efforts with hybrid policies. Ablations highlight the impact of each component of the approach. For more information please refer to the corresponding website."
Equivariant Filter for Tightly Coupled LiDAR-Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11127793,"Pose estimation is a crucial problem in simultaneous localization and mapping (SLAM). However, developing a robust and consistent state estimator remains a significant challenge, as the traditional extended Kalman filter (EKF) struggles to handle the model nonlinearity, especially for inertial measurement unit (IMU) and light detection and ranging (LiDAR). To provide a consistent and efficient solution of pose estimation, we propose Eq-LIO, a robust state estimator for tightly coupled LIO systems based on an equivariant filter (EqF). Compared with the invariant Kalman filter based on the SE<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf> (3) group structure, the EqF uses the symmetry of the semi-direct product group to couple the system state including IMU bias, navigation state, and LiDAR extrinsic calibration state, thereby suppressing linearization error and improving the behavior of the estimator in the event of unexpected state changes. The proposed Eq-LIO owns natural consistency and higher robustness, which is theoretically proven with mathematical derivation and experimentally verified through a series of tests on both public and private datasets."
Monocular Visual Place Recognition in LiDAR Maps via Cross-Modal State Space Model and Multi-View Matching.,https://doi.org/10.1109/ICRA55743.2025.11128591,"Achieving monocular camera localization within pre-built LiDAR maps can bypass the simultaneous mapping process of visual SLAM systems, potentially reducing the computational overhead of autonomous localization. To this end, one of the key challenges is cross-modal place recognition, which involves retrieving 3D scenes (point clouds) from a LiDAR map according to online RGB images. In this paper, we introduce an efficient framework to learn descriptors for both RGB images and point clouds. It takes visual state space model (VMamba) as the backbone and employs a pixel-view-scene joint training strategy for cross-modal contrastive learning. To address the field-of-view differences, independent descriptors are generated from multiple evenly distributed viewpoints for point clouds. A visible 3D points overlap strategy is then designed to quantify the similarity between point cloud views and RGB images for multi-view supervision. Additionally, when generating descriptors from pixel-level features using NetVLAD, we compensate for the loss of geometric information, and introduce an efficient scheme for multi-view generation. Experimental results on the KITTI and KITTI-360 datasets demonstrate the effectiveness and generalization of our method. The code is available at https://github.com/y2w-oc/I2P-CMPR."
Learning IMU Bias with Diffusion Model.,https://doi.org/10.1109/ICRA55743.2025.11128152,"Motion sensing and tracking with IMU data is essential for spatial intelligence, which however is challenging due to the presence of time-varying stochastic bias. IMU bias is affected by various factors such as temperature and vibration, making it highly complex and difficult to model analytically. Recent data-driven approaches using deep learning have shown promise in predicting bias from IMU readings. However, these methods often treat the task as a regression problem, overlooking the stochatic nature of bias. In contrast, we model bias, conditioned on IMU readings, as a probabilistic distribution and design a conditional diffusion model to approximate this distribution. Through this approach, we achieve improved performance and make predictions that align more closely with the known behavior of bias."
Helios: Heterogeneous Lidar Place Recognition via Overlap-Based Learning and Local Spherical Transformer.,https://doi.org/10.1109/ICRA55743.2025.11127677,"LiDAR place recognition is a crucial module in localization that matches the current location with previously observed environments. Most existing approaches in LiDAR place recognition dominantly focus on the spinning type LiDAR to exploit its large FOV for matching. However, with the recent emergence of various LiDAR types, the importance of matching data across different LiDAR types has grown significantly-a challenge that has been largely overlooked for many years. To address these challenges, we introduce HeLiOS, a deep network tailored for heterogeneous LiDAR place recognition, which utilizes small local windows with spherical transformers and optimal transport-based cluster assignment for robust global descriptors. Our overlap-based data mining and guided-triplet loss overcome the limitations of traditional distance-based mining and discrete class constraints. HeLiOS is validated on public datasets, demonstrating performance in heterogeneous LiDAR place recognition while including an evaluation for longterm recognition, showcasing its ability to handle unseen LiDAR types. We release the HeLiOS code as an open source for the robotics community at https://github.com/minwoo0611/HeLiOS."
InsCMPR: Efficient Cross-Modal Place Recognition via Instance-Aware Hybrid Mamba-Transformer.,https://doi.org/10.1109/ICRA55743.2025.11128210,"Place recognition is an important technique for autonomous mobile robotic applications. While single-modal sensor-based approaches have shown satisfactory performance, cross-modal place recognition remains underexplored due to the challenge of bridging the cross-modal heterogeneity gap. In this work, we introduce an instance-aware cross-modal place recognition approach, named InsCMPR. We design a novel instance-aware modality alignment module, which aligns multi-modal data at both pixel-level and instance-level by leveraging a pre-trained vision foundation model SAM. Then a novel dual-branch hybrid Mamba-Transformer network is proposed to efficiently enhance the distinctiveness of the produced descriptors by integrating global features with local instance features. Experimental results on the KITTI, NCLT, and HAOMO datasets show that our proposed methods achieve state-of-the-art performance while operating in real time. We will open source the implementation of our method at: https://github.com/nubot-nudt/InsCMPR."
Adaptive Thresholding for Sequence-Based Place Recognition.,https://doi.org/10.1109/ICRA55743.2025.11128422,"Robots need to know where they are in the world to operate effectively without human support. One common first step for precise robot localization is visual place recognition. It is a challenging problem, especially when the output is required in an online fashion, and the current state-of-the-art approaches that tackle it usually require either large amounts of labeled training data or rely on parameters that need to be tuned manually, often per dataset. One such parameter often used for sequence-based place recognition is the image similarity threshold that allows to differentiate between pairs of images that represent the same place even in the presence of severe environmental and structural changes, and those that represent different places even if they share a similar appearance. Currently, selecting this threshold is a manual procedure and requires human expertise. We propose an automatic similarity threshold selection technique and integrate it into a complete sequence-based place recognition system. The experiments on a broad range of real-world and simulated data show that our approach is capable of matching image sequences under various illumination, viewpoint and underlying structural changes, runs online, and requires no manual parameter tuning while yielding performance comparable to a manual, dataset-specific parameter tuning. Thus, this paper substantially increases the ease of use of visual place recognition in real-world settings."
RE-TRIP: Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition.,https://doi.org/10.1109/ICRA55743.2025.11128417,"While most people associate LiDAR primarily with its ability to measure distances and provide geometric information about the environment (via point clouds), LiDAR also captures additional data, including reflectivity or intensity values. Unfortunately, when LiDAR is applied to Place Recognition (PR) in mobile robotics, most previous works on LiDAR-based PR rely only on geometric measurements, neglecting the additional reflectivity information that LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new descriptor leverages both geometric measurements and reflectivity to enhance robustness in challenging scenarios such as geometric degeneracy, high geometric similarity, and the presence of dynamic objects. To implement RE-TRIP in real-world applications, we further propose (1) keypoint extraction method, (2) key instance segmentation method, (3) RE-TRIP matching method, and (4) reflectivity combined loop verification method. Finally, we conduct a series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios-including long corridors, bridges, large-scale urban areas, and highly dynamic environments-our experimental results show that the proposed method outperforms existing state-of-the-art methods in terms of Scan Context, Intensity Scan Context and STD. Our code is available at: https://github.com/pycS714IRE-TRIP."
Context Graph-Based Visual-Language Place Recognition.,https://doi.org/10.1109/ICRA55743.2025.11127699,"In vision-based robot localization and SLAM, Visual Place Recognition (VPR) is essential. This paper addresses the problem of VPR, which involves accurately recognizing the location corresponding to a given query image. A popular approach to vision-based place recognition relies on low-level visual features. Despite significant progress in recent years, place recognition based on low-level visual features is challenging when there are changes in scene appearance. To address this, end-to-end training approaches have been proposed to overcome the limitations of hand-crafted features. However, these approaches still fail under drastic changes and require large amounts of labeled data to train models, presenting a significant limitation. Methods that leverage high-level semantic information, such as objects or categories, have been proposed to handle variations in appearance. In this paper, we introduce a novel VPR approach that remains robust to scene changes and does not require additional training. Our method constructs semantic image descriptors by extracting pixel-level embeddings using a zero-shot, language-driven semantic segmentation model. We validate our approach in challenging place recognition scenarios using real-world public dataset. The experiments demonstrate that our method outperforms non-learned image representation techniques and off-the-shelf convolutional neural network (CNN) descriptors. Our code is available at https://github.com/woo-soojin/context-based-vlpr."
Self-Mixing Laser Interferometry for Robotic Tactile Sensing.,https://doi.org/10.1109/ICRA55743.2025.11128331,"Self-mixing interferometry (SMI) has been lauded for its sensitivity in detecting microvibrations, while requiring no physical contact with its target. In robotics, microvibrations have traditionally been interpreted as a marker for object slip, and recently as a salient indicator of extrinsic contact. We present the first-ever robotic fingertip making use of SMI for slip and extrinsic contact sensing. The design is validated through measurement of controlled vibration sources, both before and after encasing the readout circuit in its fingertip package. Then, the SMI fingertip is compared to acoustic sensing through four experiments. The results are distilled into a technology decision map. SMI was found to be more sensitive to subtle slip events and significantly more resilient against ambient noise. We conclude that the integration of SMI in robotic fingertips offers a new, promising branch of tactile sensing in robotics. Design and data files are available at https://github.com/RemkoPr/icra2025-SMI-tactile-sensing."
Estimating High-Resolution Neural Stiffness Fields Using Visuotactile Sensors.,https://doi.org/10.1109/ICRA55743.2025.11128319,"High-resolution visuotactile sensors provide detailed contact information that is promising to infer the physical properties of objects in contact. This paper introduces a novel technique for high-resolution stiffness estimation of heterogeneous deformable objects using the Punyo bubble sensor. We developed an observation model for dense contact forces to estimate object stiffness using a visuotactile sensor and a dense force estimator. Additionally, we propose a neural Volumetric Stiffness Field (VSF) formulation that represents stiffness as a continuous function, which allows dynamic point sampling at visuotactile sensor observation resolution. The neural VSF significantly reduces artifacts commonly associated with traditional point-based methods, particularly in stiff inclusion estimation and heterogeneous stiffness estimation. We further apply our method in a blind localization task, where objects within opaque bags are accurately modeled and localized, demonstrating the superior performance of neural VSF compared to existing techniques. Project page: https://hjh371.github.io/Neural-VSF/."
High-Resolution Reconstruction of Non-Planar Tactile Patterns From Low-Resolution Taxel-Based Tactile Sensors.,https://doi.org/10.1109/ICRA55743.2025.11127544,"Over the past decades, the development of tactile sensors has gained increasing attention and has gradually become a fundamental device for robots. Especially in today's context where human-robot interaction demands are growing and the requirements for tactile perception are becoming stricter, how to enable robots to better perceive their environment has become a topic worth discussing. Tactile sensors, after years of development, have emerged in two main types: taxel-based and vision-based sensors, where the latter can provide relatively low resolution (LR) tactile patterns compared with the former. Both of them have seen significant enhancements in their tactile perception capabilities on flat and regular surfaces. However, as application scenarios expand, current flat tactile perception can no longer meet the robots' needs for multi-dimensional and complex perception capabilities. Therefore, we investigate the high-resolution (HR) reconstruction of non-planar tactile patterns captured by LR taxel-based sensors in this paper. We first develop a new dataset, where the ground truth of non-planar tactile patterns are obtained with a vision-based GelSight Mini tactile sensor, and the LR data are collected via a commercial taxel-based Xela sensor. In addition, we propose to adapt the state-of-the-art CNN- and GAN-based tactile super-resolution model of flat/planar surfaces to the non-planar scenario, and also develop a diffusion-based model for the nonplanar HR reconstruction. Experimental results confirm the efficiency of the proposed models."
Blind Tactile Exploration for Surface Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128235,"Accurate 3d reconstruction capturing the fine details of an object's shape is essential for tasks such as automated assembly, inspection, and quality control. While monocular cameras provide broad visual structure but often miss critical surface details and depth accuracy in underexposed or occluded environments. Tactile sensors offer precise, localized depth information, capturing fine textures, yet exploring varied curvature surfaces with only tactile input remains challenging. To address this, the paper proposes a blind surface exploration method for convex objects using a set of sequential controllers to efficiently guide the manipulator's interaction with surfaces featuring sharp edge changes. This approach ensures precise tactile exploration, leading to highly detailed surface reconstruction. With the controller employed, the algorithm was able to move along the surface while maintaining contact along normal and reconstruct the object with IoU as high as <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{9 1 \%}$</tex> for objects with sharp edges."
Control Reallocation Using Deep Reinforcement Learning for Actuator Fault Recovery of an Autonomous Underwater Vehicle.,https://doi.org/10.1109/ICRA55743.2025.11128023,"Actuator faults in dynamic systems pose significant challenges, particularly for robotic systems operating in hostile environments such as Autonomous Underwater Vehicles (AUVs), risking loss of stability and performance degradation. Fault Tolerant Control (FTC) strategies, including Control Reallocation (CR), have been developed to mitigate such risks. However, these strategies extensively depend on explicit fault diagnosis, which may present challenges regarding computational demands and efficiency, particularly when dealing with unknown faults. This paper presents a novel method that performs CR with Deep Reinforcement Learning (DRL) for actuator fault recovery without explicit fault diagnosis. The approach is implemented on a BlueROV2 underwater vehicle and demonstrates improved performance for fault recovery compared to a standard Proportional-Integral-Derivative (PID) controller and a variable gain PID controller, both in simulation and in real-world conditions. The DRL-based CR method demonstrates generalisability by successfully handling faults not encountered during training, highlighting its adaptability to unforeseen circumstances."
A New Framework for Repetitive Control of Robot Manipulators via Operator-Theoretic Robust Stabilization.,https://doi.org/10.1109/ICRA55743.2025.11127417,"This paper establishes a new framework for repetitive control of uncertain robot manipulators via operator-theoretic robust stabilization. After applying the inverse dynamics approach to robot manipulators, by which the relevant nonlinear input/output behavior is converted to a linear time-invariant (LTI) equation, we take the repetitive control approach. Even though such a repetitive controller is known to achieve high performances for periodic reference inputs, it is quite difficult to derive the stability analysis for the resulting closed-loop systems in a rigorous fashion. To solve this difficulty, we construct an operator-theoretic approach to the repetitive control treatment, and show that the closed-loop systems are exponentially stable if and only if the spectrum radius of the relevant monodromy operator is less than 1. Based on the necessary and sufficient condition, we develop a guideline to take the relevant control parameters. Finally, some experiment results are given to demonstrate the overall arguments developed in this paper."
Learning Robust Policies via Interpretable Hamilton-Jacobi Reachability-Guided Disturbances.,https://doi.org/10.1109/ICRA55743.2025.11127287,"Deep Reinforcement Learning (RL) has shown remarkable success in robotics with complex and heterogeneous dynamics. However, its vulnerability to unknown disturbances and adversarial attacks remains a significant challenge. In this paper, we propose a robust policy training framework that integrates model-based control principles with adversarial RL training to improve robustness without the need for external black-box adversaries. Our approach introduces a novel Hamilton-Jacobi reachability-guided disturbance for adversarial RL training, where we use interpretable worst-case or near-worst-case disturbances as adversaries against the robust policy. We evaluated its effectiveness across three distinct tasks: a reach-avoid game in both simulation and real-world settings, and a highly dynamic quadrotor stabilization task in simulation. We validate that our learned critic network is consistent with the ground-truth HJ value function, while the policy network shows comparable performance with other learning-based methods."
Optimal Fault-Tolerant Control for Tugboats Robust Path Following in Nearshore.,https://doi.org/10.1109/ICRA55743.2025.11128240,"External ocean disturbances (EODs) and internal thruster loss-of-effectiveness faults (ITLEFs) are key factors influencing the accuracy of the autonomous tugboat's path following, as well as the stability and safety of the tugboat's hull during maritime operations. To achieve robust path following for the autonomous tugboat, this paper proposes an optimal fault-tolerant control scheme. Firstly, we formulate the robust path following of the tugboat as an optimal fault-tolerance control problem. A matrixed error system for the control scheme is constructed to uniformly consider both EODs and ITLEFs. Secondly, considering the time and economic costs associated with algorithm deployment and tuning process on tugboats in real world, we present an adaptive dynamic programming algorithm to solve the proposed optimal fault-tolerance problem, which is characterized by ease of tuning. Then, the stability of the control system is proven based on the Lyapunov criterion. Finally, the proposed control scheme is evaluated under practical conditions with EODs and ITLEFs. The comparative results with backstepping-based control scheme demonstrate that the proposed control scheme exhibits more robustness for path following under EODs and ITLEFs."
Neural $\mathcal{L}_{1}$ Adaptive Control of Vehicle Lateral Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11128353,"We address the problem of stable and robust control of vehicles with lateral error dynamics for the application of lane keeping. Lane departure is the primary reason for half of the fatalities in road accidents, making the development of stable, adaptive and robust controllers a necessity. Any disturbance or uncertainty introduced to the steering-angle input can be catastrophic for the vehicle. Therefore, controllers must be developed to actively handle such uncertainties. In this work, we introduce a Neural <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathcal{L}_1$</tex> Adaptive controller (Neural-L1) which learns the uncertainties in the lateral error dynamics of a front-steered Ackermann vehicle and guarantees stability and robustness. Our contributions are threefold: i) We extend the theoretical results for guaranteed stability and robustness of conventional <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathcal{L}_1$</tex> Adaptive controllers to Neural-L1; ii) We implement a Neural-L1 for the lane keeping application which learns uncertainties in the dynamics accurately; iii) We evaluate the performance of Neural-L1 on a physics-based simulator, PyBullet, and conduct extensive real-world experiments with the FlTENTH platform to demonstrate superior reference trajectory tracking performance of Neural-L1 compared to other state-of-the-art controllers, in the presence of uncertainties. Our project page, including supplementary material and videos, can be found at https://mukhe027.github.io/Neural-Adaptive-Control/"
Efficient Imitation Without Demonstrations via Value-Penalized Auxiliary Control from Examples.,https://doi.org/10.1109/ICRA55743.2025.11128275,"Common approaches to providing feedback in reinforcement learning are the use of hand-crafted rewards or full-trajectory expert demonstrations. Alternatively, one can use examples of completed tasks, but such an approach can be extremely sample inefficient. We introduce value-penalized auxiliary control from examples (VPACE), an algorithm that significantly improves exploration in example-based control by adding examples of simple auxiliary tasks and an above-success-level value penalty. Across both simulated and real robotic environments, we show that our approach substantially improves learning efficiency for challenging tasks, while maintaining bounded value estimates. Preliminary results also suggest that VPACE may learn more efficiently than the more common approaches of using full trajectories or true sparse rewards. Project site: https://papers.starslab.ca/vpace/."
QuasiNav: Asymmetric Cost-Aware Navigation Planning with Constrained Quasimetric Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127346,"Autonomous navigation in unstructured outdoor environments is inherently challenging due to the presence of asymmetric traversal costs, such as varying energy expenditures for uphill versus downhill movement. Traditional reinforcement learning methods often assume symmetric costs, which can lead to suboptimal navigation paths and increased safety risks in realworld scenarios. In this paper, we introduce QuasiNav, a novel reinforcement learning framework that integrates quasimetric embeddings to explicitly model asymmetric costs and guide efficient, safe navigation. QuasiNav formulates the navigation problem as a constrained Markov decision process (CMDP) and employs quasimetric embeddings to capture directionally dependent costs, allowing for a more accurate representation of the terrain. We combine this approach with adaptive constraint tightening. This ensures that safety constraints are dynamically enforced during learning. We validate QuasiNav on a Clearpath Jackal robot in three challenging navigation scenarios-undulating terrains, asymmetric hill traversal, and directionally dependent terrain traversal-demonstrating its effectiveness in both simulated and real-world environments. Experimental results show that QuasiNav significantly outperforms conventional methods, achieving higher success rates, improved energy efficiency (13.6 % reduction in energy consumption compared to baseline methods), and better adherence to safety constraints."
Learning a High-Quality Robotic Wiping Policy Using Systematic Reward Analysis and Visual-Language Model Based Curriculum.,https://doi.org/10.1109/ICRA55743.2025.11128687,"Autonomous robotic wiping is an important task in various industries, ranging from industrial manufacturing to sanitization in healthcare. Deep reinforcement learning (Deep RL) has emerged as a promising algorithm, however, it often suffers from a high demand for repetitive reward engineering. Instead of relying on manual tuning, we first analyze the convergence of quality-critical robotic wiping, which requires both high-quality wiping and fast task completion, to show the poor convergence of the problem and propose a new bounded reward formulation to make the problem feasible. Then, we further improve the learning process by proposing a novel visual-language model (VLM) based curriculum, which actively monitors the progress and suggests hyperparameter tuning. We demonstrate that the combined method can find a desirable wiping policy on surfaces with various curvatures, frictions, and waypoints, which cannot be learned with the baseline formulation. The demo of this project can be found at: https://sites.google.com/view/highqualitywiping"
Actor-Critic Cooperative Compensation to Model Predictive Control for Off-Road Autonomous Vehicles Under Unknown Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11128460,"This study presents an Actor-Critic Cooperative Compensated Model Predictive Controller <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(\text{AC}^3 \text{MPC})$</tex> designed to address unknown system dynamics. To avoid the difficulty of modeling highly complex dynamics and ensuring real-time control feasibility and performance, this work uses deep reinforcement learning with a model predictive controller in a cooperative framework to handle unknown dynamics. The model-based controller takes on the primary role as both controllers are provided with predictive information about the other. This improves tracking performance and retention of inherent robustness of the model predictive controller. We evaluate this framework for off-road autonomous driving on unknown deformable terrains that represent sandy deformable soil, sandy and rocky soil, and cohesive clay-like deformable soil. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers by upto 29.2% and 10.2%. This framework generalized well over varied and previously unseen terrain characteristics to track longitudinal reference speeds with lower errors. Furthermore, this required significantly less training data compared to purely learning-based controller, while delivering better performance even when under-trained."
Soft Actor-Critic-Based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments.,https://doi.org/10.1109/ICRA55743.2025.11128470,"Motion planning failures during autonomous navigation often occur when safety constraints are either too conservative, leading to deadlocks, or too liberal, resulting in collisions. To improve robustness, a robot must dynamically adapt its safety constraints to ensure it reaches its goal while balancing safety and performance measures. To this end, we propose a Soft Actor-Critic (SAC)-based policy for adapting Control Barrier Function (CBF) constraint parameters at runtime, ensuring safe yet non-conservative motion. The proposed approach is designed for a general high-level motion planner, low-level controller, and target system model, and is trained in simulation only. Through extensive simulations and physical experiments, we demonstrate that our framework effectively adapts CBF constraints, enabling the robot to reach its final goal without compromising safety."
Watch Your STEPP: Semantic Traversability Estimation Using Pose Projected Features.,https://doi.org/10.1109/ICRA55743.2025.11127781,"Understanding the traversability of terrain is essential for autonomous robot navigation, particularly in unstructured environments such as natural landscapes. Although traditional methods, such as occupancy mapping, provide a basic framework, they often fail to account for the complex mobility capabilities of some platforms such as legged robots. In this work, we propose a method for estimating terrain traversability by learning from demonstrations of human walking. Our approach leverages dense, pixel-wise feature embeddings generated using the DINOv2 vision Transformer model, which are processed through an encoder-decoder MLP architecture to analyze terrain segments. The averaged feature vectors, extracted from the masked regions of interest, are used to train the model in a reconstruction-based framework. By minimizing reconstruction loss, the network distinguishes between familiar terrain with a low reconstruction error and unfamiliar or hazardous terrain with a higher reconstruction error. This approach facilitates the detection of anomalies, allowing a legged robot to navigate more effectively through challenging terrain. We run real-world experiments on the ANYmal legged robot both indoor and outdoor to prove our proposed method. The code is open-source, while video demonstrations can be found on our website: https://rpl-cs-ucl.github.io/STEPP/"
Gnd: Global Navigation Dataset With Multi-Modal Perception and Multi-Category Traversability in Outdoor Campus Environments.,https://doi.org/10.1109/ICRA55743.2025.11128193,"Navigating large-scale outdoor environments requires complex reasoning in terms of geometric structures, environmental semantics, and terrain characteristics, which are typically captured by onboard sensors such as LiDAR and cameras. While current mobile robots can navigate such environments using pre-defined, high-precision maps based on hand-crafted rules catered for the specific environment, they lack commonsense reasoning capabilities, especially the traversability analysis, that most humans possess when navigating unknown outdoor spaces. To address this gap, we introduce the Global Navigation Dataset (GND), a large-scale dataset that integrates multi-modal sensory data, including 3D LiDAR point clouds and RGB and 360 images, as well as multi-category traversability maps (pedestrian walkways, vehicle roadways, stairs, off-road terrain, and obstacles) from ten university campuses. These environments encompass a variety of parks, urban settings, elevation changes, and campus layouts of different scales. The dataset covers approximately <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$2.7 ~\text{km}^{2}$</tex> and includes at least 350 buildings in total. We also present a set of novel applications of GND to showcase its utility to enable global robot navigation, such as map-based global navigation, mapless navigation, and global place recognition. GND's website can be found at https://cs.gmu.edu/xiao/Research/GND/."
VLM-GroNav: Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments.,https://doi.org/10.1109/ICRA55743.2025.11128264,"We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with physical grounding that is used to assess intrinsic terrain properties such as deformability and slipperiness. We use proprioceptive-based sensing, which provides direct measurements of these physical properties, and enhances the overall semantic understanding of the terrains. Our formulation uses in-context learning to ground the VLM's semantic understanding with proprioceptive data to allow dynamic updates of traversability estimates based on the robot's real-time physical interactions with the environment. We use the updated traversability estimations to inform both the local and global planners for real-time trajectory replanning. We validate our method on a legged robot (Ghost Vision 60) and a wheeled robot (Clearpath Husky), in diverse real-world outdoor environments with different deformable and slippery terrains. In practice, we observe significant improvements over state-of-the-art methods by up to 50% increase in navigation success rate."
TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals.,https://doi.org/10.1109/ICRA55743.2025.11127998,"Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and in-corporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO."
Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent.,https://doi.org/10.1109/ICRA55743.2025.11128234,"Adept traffic models are critical to both real-time prediction/planning and closed-loop simulation for autonomous vehicles (AV). Key design objectives include accuracy, diverse multimodal behaviors, interpretability, and compatibility with other modules in the autonomy stack, e.g., the downstream planner. We present Categorical Traffic Transformer (CTT), a traffic model that outputs both continuous trajectory predictions and categorical predictions with clear semantic meanings (lane modes, homotopies, etc.). The most outstanding feature of CTT is its fully interpretable latent space, which enables direct supervision of the latent variables from the ground truth during training and avoids mode collapse completely. As a result, CTT can generate diverse behaviors conditioned on different semantic modes while significantly beating SOTA on prediction accuracy. In addition, CTT's ability to input and output tokens enables direct integration with semantic-heavy modules such as behavior planners and language models, bridging the tokenized representation and the continuous trajectory space."
LACNS: Language-Assisted Continuous Navigation in Structured Spaces.,https://doi.org/10.1109/ICRA55743.2025.11128355,"Current autonomous driving technology typically relies on high-precision (HD) maps to ensure safe, reliable, and accurate navigation in urban environments. While these maps provide essential road information, their creation and maintenance are costly, limiting their widespread application. To mitigate this reliance, we propose a novel system, Language-Assisted Continuous Navigation in Structured Spaces (LACNS). LACNS facilitates autonomous driving without the need for HD maps by integrating vehicle-centric local perception with real-time language instructions from map software or human navigators. LACNS begins by generating a BEV map using the vehicle's front-facing camera. Simultaneously, a pretrained Visual Language Model (VLM) detects intersections from the camera images, assigning a score to each. Road elements are then extracted from the BEV map and combined with the intersection scores to identify potential navigation frontiers. Language instructions, processed by a pretrained Large Language Model(LLM), are used to select the most suitable frontier. Finally, the chosen frontier and BEV map are employed to plan a safe route and control the vehicle's movement. We evaluated LACNS using the Carla simulator to validate its navigation capabilities in continuous spaces. Initial experiments involved navigating through four intersections with varying directional instructions, where LACNS demonstrated high and consistent success rates across multiple trials. Further simulations in real-time navigation scenarios revealed that LACNS consistently maintained a high success rate across three progressively challenging routes. These results highlight the effectiveness of our novel autonomous driving navigation method without HD maps."
Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset and Consensus-Based Models.,https://doi.org/10.1109/ICRA55743.2025.11127341,"A significant portion of roads, particularly in densely populated developing countries, lacks explicitly defined right-of-way rules. These understructured roads pose substantial challenges for autonomous vehicle motion planning, where efficient and safe navigation relies on understanding decentralized human coordination for collision avoidance. This coordination, often termed social driving etiquette, remains underexplored due to limited open-source empirical data and suitable modeling frameworks. In this paper, we present a novel dataset and modeling framework designed to study motion planning in these understructured environments. The dataset includes 20 aerial videos of representative scenarios, an image dataset for training vehicle detection models, and a development kit for vehicle trajectory estimation. We demonstrate that a consensus-based modeling approach can effectively explain the emergence of priority orders observed in our dataset, and is therefore a viable framework for decentralized collision avoidance planning."
CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11127664,"Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications."
BETTY Dataset: A Multi-Modal Dataset for Full-Stack Autonomy.,https://doi.org/10.1109/ICRA55743.2025.11127350,"We present the BETTY dataset, a large-scale, multi-modal dataset collected on several autonomous racing vehicles, targeting supervised and self-supervised state estimation, dynamics modeling, motion forecasting, perception, and more. Existing large-scale datasets, especially autonomous vehicle datasets, focus primarily on supervised perception, planning, and motion forecasting tasks. Our work enables multi-modal, data-driven methods by including all sensor inputs and the outputs from the software stack, along with semantic metadata and ground truth information. The dataset encompasses 4 years of data, currently comprising over 13 hours and 32 TB, collected on autonomous racing vehicle platforms. This data spans 6 diverse racing environments, including high-speed oval courses, for single and multi-agent algorithm evaluation in feature-sparse scenarios, as well as high-speed road courses with high longitudinal and lateral accelerations and tight, GPSdenied environments. It captures highly dynamic states, such as <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$63 \mathrm{m} / \mathrm{s}$</tex> crashes, loss of tire traction, and operation at the limit of stability. By offering a large breadth of cross-modal and dynamic data, the BETTY dataset enables the training and testing of full autonomy stack pipelines, pushing the performance of all algorithms to the limits. The current dataset is available at https://pitt-mit-iac.github.io/betty-dataset/."
Error-Subspace Transform Kalman Filter Based Real-Time Gait Prediction for Rehabilitation Exoskeletons.,https://doi.org/10.1109/ICRA55743.2025.11128574,"With the rapid development of rehabilitation robotics, there is a pressing need for efficient and accurate gait prediction methods. However, due to the complexity and variability of individual gait characteristics and external disturbances, accurately predicting gait in real time remains a significant challenge. This paper proposes an innovative Bayesian-inference-based method for real-time gait prediction while a subject walks with a lower-limb exoskeleton. Periodic gait information is represented using von Mises basis functions, and the weight parameters serve as real-time updated state variables. The error-subspace transform Kalman filter (ESTKF) is applied for gait trajectory prediction. A fully connected neural network (FCNN) is used to estimate the walking speeds in real time based on predicted trajectories. Comparative experiments based on an open-source database prove the advantages of ESKTF compared with other Bayesian filters. Walking experiments are conducted to estimate phase and speed in real time, and to predict the joint angle, total joint torque, and lower-limb muscle surface electromyography (sEMG) values. Experimental results validate the method's prediction performance across different speeds and demonstrate its resilience to external interference."
"A Comparative Study of Pulley and Bowden Transmissions in a Novel Cable-Driven Exosuit, the Stillsuit.",https://doi.org/10.1109/ICRA55743.2025.11127825,"Cable-driven exosuits assist users in ambulatory activities by transmitting assistive torques from motors to the actuated joints. State-of-the-art exosuits typically use Bowden cable transmissions, albeit their limited efficiencies (4060 %) and non-linear response in curved paths. This paper evaluates the efficiency and responsiveness of a new cable-pulley transmission compared to a Bowden transmission, using both steel and Dyneema cables. The analysis includes three experiments: a test bench simulating a curved transmission path, followed by a static and dynamic experiment where six unimpaired participants donned an exosuit featuring both transmissions across the hips and knees. Our findings demonstrate that the pulley transmission consistently outperformed the Bowden's efficiency by absolute margins of 18.77 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\pm 7.29\%$</tex> using a steel cable and by 40.60 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\pm 6.76\%$</tex> using a Dyneema cable across all experiments. Additionally, the steel cable was on average 19.19 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\pm 5.29\%$</tex> more efficient than the Dyneema cable in the pulley transmission and 41.02 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\pm 6.34\%$</tex> in the Bowden tube. These results led to the development of the Stillsuit, a novel lower-limb cable-driven exosuit that uses a pulley transmission and steel cable. The Stillsuit sets a new benchmark for exosuits with 87.56 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\pm$</tex> 3.92 % transmission efficiency, generating similar biological torques to those found in literature (16.4% and 19.0% of the biological knee and hip torques, respectively) while using smaller motors, resulting in a lighter actuation unit (1.92 kg)."
Rapid Online Learning of Hip Exoskeleton Assistance Preferences.,https://doi.org/10.1109/ICRA55743.2025.11128809,"Hip exoskeletons are increasing in popularity due to their effectiveness across various scenarios and their ability to adapt to different users. However, personalizing the assistance often requires lengthy tuning procedures and computationally intensive algorithms, and most existing methods do not incorporate user feedback. In this work, we propose a novel approach for rapidly learning users' preferences for hip exoskeleton assistance. We perform pairwise comparisons of distinct randomly generated assistive profiles, and collect participants preferences through active querying. Users' feed-back is integrated into a preference-learning algorithm that updates its belief, learns a user-dependent reward function, and changes the assistive torque profiles accordingly. Results from eight healthy subjects display distinct preferred torque profiles, and users' choices remain consistent when compared to a perturbed profile. A comprehensive evaluation of users' preferences reveals a close relationship with individual walking strategies. The tested torque profiles do not disrupt kinematic joint synergies, and participants favor assistive torques that are synchronized with their movements, resulting in lower negative power from the device. This straightforward approach enables the rapid learning of users preferences and rewards, grounding future studies on reward-based human-exoskeleton interaction."
A Human-in-the-Loop Simulation Framework for Evaluating Control Strategies in Gait Assistive Robots.,https://doi.org/10.1109/ICRA55743.2025.11127863,"As the global population ages, effective rehabilitation and mobility aids will become increasingly critical. Gait assistive robots are promising solutions, but designing adaptable controllers for various impairments poses a significant challenge. This paper presented a Human-In-The-Loop (HITL) simulation framework tailored specifically for gait assistive robots, addressing unique challenges posed by passive support systems. We incorporated a realistic physical human-robot interaction (pHRI) model to enable a quantitative evaluation of robot control strategies, highlighting the performance of a speed-adaptive controller compared to a conventional PID controller in maintaining compliance and reducing gait distortion. We assessed the accuracy of the simulated interactions against that of the real-world data and revealed discrepancies in the adaptation strategies taken by the human and their effect on the human's gait. This work underscored the potential of HITL simulation as a versatile tool for developing and fine-tuning personalized control policies for various users."
Individual and Collective Behaviors in Soft Robot Worms Inspired by Living Worm Blobs.,https://doi.org/10.1109/ICRA55743.2025.11127959,"California blackworms constitute a recently identified animal system exhibiting unusual collective behaviors, in which dozens to thousands of worms entangle to form a blob capable of actions like locomotion as an aggregate. In this paper we describe a system of pneumatic soft robots inspired by the blackworms, intended for the study of collective behaviors enabled and mediated by such physical entanglement. Both the robots and worms have high aspect ratio (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\gtrsim 1: 50$</tex>), intertwine in complex 3D configurations, operate both in air and underwater, and can locomote both individually and as a collective. We demonstrate and characterize locomotion for both individual robots and entangled blobs, explore the tunability of entanglement strength, and compare these to the analogous versions in living worms. The robots provide a testbed for studying mechanisms underlying behaviors observed in worm blobs, as well as serving as a platform for studies of novel collective behaviors based on physical entanglement."
Informed Repurposing of Quadruped Legs for New Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128237,"Redesigning and remanufacturing robots are infeasible for resource-constrained environments like space or undersea. This work thus studies how to evaluate and repurpose existing, complementary, quadruped legs for new tasks. We implement this approach on 15 robot designs generated from combining six pre-selected leg designs. The performance maps for force-based locomotion tasks like pulling, pushing, and carrying objects are constructed via a learned policy that works across all designs and adapts to the limits of each. Performance predictions agree well with real-world validation results. The robot can locomote at 0.5 body lengths per second while exerting a force that is almost 60% of its weight."
Intelligent Self-Healing Artificial Muscle: Mechanisms for Damage Detection and Autonomous Repair of Puncture Damage in Soft Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128570,"Soft robotics are characterized by their high deformability, mechanical robustness, and inherent resistance to damage. These unique properties present exciting new opportunities to enhance both emerging and existing fields such as healthcare, manufacturing, and exploration. However, to function effectively in unstructured environments, these technologies must withstand the same real-world conditions to which human skin and other soft biological materials are typically subjected. Here, we present a novel soft material architecture designed for active detection of material damage and autonomous repair in soft robotic actuators. By integrating liquid metal (LM) microdroplets within a silicone elastomer, the system can detect and localize damage through the formation of conductive pathways that arise from extreme pressure (> 1 MPa) or puncture events. These newly formed conductive networks function as in situ Joule heating elements, facilitating the reprocessing and healing of the material. The architecture allows for the reconfiguration of the newly formed electrical network using controlled electrical and thermal mechanisms to restore functionality. The entire process from damage detection to repair and reconfiguration occurs without any manual intervention or external mechanisms to facilitate healing. This innovative approach not only enhances the resilience and performance of soft materials but also supports a wide range of applications in soft robotics and wearable technologies, where adaptive and autonomous systems are crucial for operation in dynamic and unpredictable environments."
SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128420,"Despite significant advancements in large language models (LLMs) that enhance robot agents' understanding and execution of natural language (NL) commands, ensuring the agents adhere to user-specified constraints remains challenging, particularly for complex commands and long-horizon tasks. To address this challenge, we present three key insights, equivalence voting, constrained decoding, and domain-specific fine-tuning, which significantly enhance LLM planners' capability in handling complex tasks. Equivalence voting ensures consistency by generating and sampling multiple Linear Temporal Logic (LTL) formulas from NL commands, grouping equivalent LTL formulas, and selecting the majority group of formulas as the final LTL formula. Constrained decoding then uses the generated LTL formula to enforce the autoregressive inference of plans, ensuring the generated plans conform to the LTL. Domain-specific fine-tuning customizes LLMs to produce safe and efficient plans within specific task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these insights to create LLM planners to generate plans adhering to user commands with high confidence. We demonstrate the effective-ness and generalizability of SELP across different robot agents and tasks, including drone navigation and robot manipulation. For drone navigation tasks, SELP outperforms state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks conforming to NL commands) and by 19.8% in plan efficiency. For robot manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our datasets for evaluating NL-to-LTL and robot task planning will be released in github.com/lt-asset/selp."
Marginalizing and Conditioning Gaussians onto Linear Approximations of Smooth Manifolds with Applications in Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128000,"We present closed-form expressions for marginalizing and conditioning Gaussians onto linear manifolds, and demonstrate how to apply these expressions to smooth non-linear manifolds through linearization. Although marginalization and conditioning onto axis-aligned manifolds are well-established procedures, doing so onto non-axis-aligned manifolds is not as well understood. We demonstrate the utility of our expressions through three applications: 1) approximation of the projected normal distribution, where the quality of our linearized approximation increases as problem non-linearity decreases; 2) covariance extraction in Koopman SLAM, where our covariances are shown to be consistent on a real-world dataset; and 3) covariance extraction in constrained GTSAM, where our covariances are shown to be consistent in simulation."
Dynamic Tube MPC: Learning Tube Dynamics with Massively Parallel Simulation for Robust Safety in Practice.,https://doi.org/10.1109/ICRA55743.2025.11127279,
DGS-SLAM: A Visual Dense SLAM Based on Gaussian Splatting in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11128821,"Visual dense SLAM can facilitate pose estimation and map reconstruction for sensor carriers in unknown environments. However, in uncontrolled environments such as offices, shopping malls, and train stations, frequent occurrences of people walking back and forth or temporary movement of objects within the scene are common. Most existing visual dense SLAM systems do not account for these dynamic factors, leading to localization drift and map distortion. In this paper, we propose DGS-SLAM, a system capable of achieving robust localization and high-fidelity static map reconstruction in dynamic environments. We utilize semantic 3D Gaussians for scene representation, effectively eliminating interference from dynamic objects and refining the reconstruction of static background. We enhance the tracking accuracy and mapping quality of dense SLAM by using a distance distribution-based Gaussian pruning algorithm and implementing a coarse-to-fine tracking strategy with bundle adjustment and differentiable rendering. We perform qualitative and quantitative evaluations on two publicly available dynamic environment datasets. The results indicate that our method effectively reduces the interference caused by dynamic objects, enabling visual dense SLAM to maintain competitive tracking accuracy and mapping performance in dynamic environments."
ARS-SLAM: Accurate Robust Spinning LiDAR SLAM for a Quadruped Robot in Large-Scale Scenario.,https://doi.org/10.1109/ICRA55743.2025.11128745,"It is challenging to employ a quadruped robot for real-time mapping and positioning in a large range of scenes. The significant vibration and instability of the quadruped robot during mobility, as well as the quantity of computation required to convey a wide variety of complex landscapes, result in unsatisfactory drawing construction accuracy and inefficient real-time performance. Therefore, we propose an accurate robust spinning LiDAR SLAM (ARS-SLAM) algorithm for a quadruped robot under the large-scale scene. The tightly coupled iterative Kalman filter in FAST-LIO2 is introduced into the front end of the cartographer framework to improve the accuracy and robustness of robot pose estimation. To reduce the computational complexity of the original cartographer framework, a pose threshold optimization algorithm was introduced to effectively remove redundant information from loop detection and improve computational efficiency and real-time performance. We tested the system's performance against the most advanced point-cloud-based methods, LIO-SAM and FAST-LIO2, on a large dataset of large science parks and underground parking lots, and the results show that the proposed system achieves the same or better accuracy and real-time performance."
Tightly Coupled Range Inertial Odometry and Mapping with Exact Point Cloud Downsampling.,https://doi.org/10.1109/ICRA55743.2025.11128146,"In this work, to facilitate the real-time processing of multi-scan registration error minimization on factor graphs, we devise a point cloud downsampling algorithm based on coreset extraction. This algorithm extracts a subset of the residuals of input points such that the subset yields exactly the same quadratic error function as that of the original set for a given pose. This enables a significant reduction in the number of residuals to be evaluated without approximation errors at the sampling point. Using this algorithm, we devise a complete SLAM framework that consists of odometry estimation based on sliding window optimization and global trajectory optimization based on registration error minimization over the entire map, both of which can run in real time on a standard CPU. The experimental results demonstrate that the proposed framework outperforms state-of-the-art CPU-based SLAM frameworks without the use of GPU acceleration."
Scalable Multi-Session Visual SLAM in Large-Scale Scenes with Subgraph Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128582,"Multi-session visual SLAM systems enable 6-DoF camera localization along with long-term maintenance and expansion of the global map, by utilizing image data from different sessions. However, in large-scale environments, these systems often suffer from severe scale drift. While modern SLAM systems attempt to maintain global map consistency through loop detection and correction, they still face challenges in terms of convergence and accuracy. In this paper, we propose a robust large-scale multi-session SLAM system for long-term localization and mapping that achieves global consistency. Furthermore, to address the backend optimization problem in large-scale environments, we introduce a hierarchical optimization strategy based on the graph structure. More specifically, a subgraph structure is introduced to reduce the size of problem while effectively propagating scale correction information. In addition, a hierarchical strategy enables coarse-to-fine updates of the graph states. Experimental results not only demonstrate that our method efficiently optimizes the pose graph and maintains map consistency in large-scale environments, but also highlight the effectiveness and scalability of the proposed approach."
Winding Number-Guided Edge-Preserving Implicit Neural Representation of CAD Surfaces.,https://doi.org/10.1109/ICRA55743.2025.11127938,"Implicit surface representations have emerged as a powerful tool for the task of 3D reconstruction due to their excellent performance. Yet, when the normal information cannot be available, the previous methods often lead to unsatisfactory reconstruction results, even failure. To this end, we propose a winding numberguided implicit surface reconstruction method, which mainly consists of a winding numberguided regularizer and a dynamic edge sampling strategy. Among them, the winding number-guided regularizer can effectively constrain the global normal consistency of the input raw data, as well as improve the unsatisfactory implicit surface reconstruction result caused by the unavailability of normal information. Meanwhile, in order to reduce the excessive smoothing at sharp edges of implicit surface, we proposed a dynamic edge sampling strategy for sampling near the sharp edge regions of 3D shape, which can effectively avoid the regularizer from smoothing all regions. Finally, we combine them with a simple data term for robust implicit surface reconstruction. Compared with the state-of-the-art methods, experimental results show that our method significantly improves the quality of 3D reconstruction results. In addition, since the winding number-guided regularizer effectively constraints the globally consistent normal of the input 3D raw data, our method can also receive an additional gift, namely the globally consistent normal estimation results of 3D raw data."
A Refined 3D Gaussian Representation for High-Quality Dynamic Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128618,"In recent years, Neural Radiance Fields (NeRF) has revolutionized three-dimensional (3D) reconstruction with its implicit representation. Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the implicit representation of neural networks and instead directly represents scenes as point clouds with Gaussian-shaped distributions. While this shift has notably elevated the rendering quality and speed of radiance fields but inevitably led to a significant increase in memory usage. Additionally, effectively rendering dynamic scenes in 3D-GS has emerged as a pressing challenge. To address these concerns, this paper proposes a refined 3D Gaussian representation for high-quality dynamic scene reconstruction. Firstly, we use a deformable multi-layer perceptron (MLP) network to capture the dynamic offset of Gaussian points and express the color features of points through hash encoding and a tiny MLP to reduce storage requirements. Subsequently, we introduce a learnable denoising mask coupled with denoising loss to eliminate noise points from the scene, thereby further compressing 3D Gaussian model. Finally, motion noise of points is mitigated through static constraints and motion consistency constraints. Experimental results demonstrate that our method surpasses existing approaches in rendering quality and speed, while significantly reducing the memory usage associated with 3D-GS, making it highly suitable for various tasks such as novel view synthesis, and dynamic mapping."
GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11127828,"Embodied intelligence requires precise reconstruction and rendering to simulate large-scale real-world data. Although 3D Gaussian Splatting (3DGS) has recently demonstrated high-quality results with real-time performance, it still faces challenges in indoor scenes with large, textureless regions, resulting in incomplete and noisy reconstructions due to poor point cloud initialization and underconstrained optimization. Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, we propose a unified optimization framework that integrates neural signed distance fields (SDFs) with 3DGS for accurate geometry reconstruction and real-time rendering. This framework incorporates a neural SDF field to guide the densification and pruning of Gaussians, enabling Gaussians to model scenes accurately even with poor initialized point clouds. Simultaneously, the geometry represented by Gaussians improves the efficiency of the SDF field by piloting its point sampling. Additionally, we introduce two regularization terms based on normal and edge priors to resolve geometric ambiguities in textureless areas and enhance detail accuracy. Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. Project page: https://xhd0612.github.io/GaussianRoom.github.io/"
Hide-in-Motion: Embedding Steganographic Copyright Information into 4D Gaussian Splatting Assets.,https://doi.org/10.1109/ICRA55743.2025.11128285,"As 4D extensions of 3D Gaussian Splatting (4D-GS) emerge as groundbreaking techniques for dynamic scene reconstruction and novel view synthesis in robotics and computer vision, ensuring the security and trustworthiness of these assets becomes crucial. While steganography has advanced significantly in 2D and 3D media, existing methods are inadequate for the complex, dynamic nature of 4D-GS representations. To address this gap, we propose Hide-in-Motion, a novel 4D steganography method for hiding information through deformation in Gaussian splatting. Our approach introduces a composite attribute and a Decouple Feature Field for coarse-to-fine deformation modeling and embedding implicit information, along with an Opacity-Guided Adaptive strategy. Hide-in-Motion overcomes the limitations of previous techniques, enhancing both the robustness of embedded information and the quality of 4D reconstruction. Extensive evaluations demonstrate that our method successfully embeds and recovers implicit information across various modalities while maintaining high rendering quality in dynamic scenes. This work not only advances copyright protection and secure data transmission for 4D assets but also paves the way for enhancing the security and integrity of 4D digital assets. Code is available at https://github.com/CUHK-AIM-Group/Hide-in-Motion."
DENSER: 3D Gaussian Splatting for Scene Reconstruction of Dynamic Urban Environments.,https://doi.org/10.1109/ICRA55743.2025.11127771,"This paper presents DENSER, a framework leveraging 3D Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments. While several methods for photorealistic scene representations, both implicitly using neural radiance fields (NeRF) and explicitly using 3DGS have shown promising results in scene reconstruction of relatively complex dynamic scenes, modeling the dynamic appearance of foreground objects tends to be challenging, limiting the applicability of these methods to capture subtleties and details of the scenes, especially for dynamic objects. To this end, we propose DENSER, a framework that significantly enhances the representation of dynamic objects and accurately models the appearance of dynamic objects in the driving scene. Instead of directly using Spherical Harmonics (SH) to model the appearance of dynamic objects, we introduce and integrate a new method aiming at dynamically estimating SH bases using wavelets, resulting in better representation of dynamic objects appearance in both space and time. Besides object appearance, DENSER enhances object shape representation through densification of its point cloud across multiple scene frames, resulting in faster convergence of model training. Extensive evaluations on the KITTI dataset show that the proposed approach outperforms state-of-the-art methods by a wide margin. Source codes and models will be uploaded to this repository https://github.com/sntubix/denser"
CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query.,https://doi.org/10.1109/ICRA55743.2025.11128057,"Cooperative perception enhances the individual perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the environment. However, balancing perception performance and transmission costs remains a significant challenge. Current approaches that transmit regionlevel features across agents are limited in interpretability and demand substantial bandwidth, making them unsuitable for practical applications. In this work, we propose CoopDETR, a novel cooperative perception framework that introduces objectlevel feature cooperation via object query. Our framework consists of two key modules: single-agent query generation, which efficiently encodes raw sensor data into object queries, reducing transmission cost while preserving essential information for detection; and cross-agent query fusion, which includes Spatial Query Matching (SQM) and Object Query Aggregation (OQA) to enable effective interaction between queries. Our experiments on the OPV2V and V2XSet datasets demonstrate that CoopDETR achieves state-of-the-art performance and significantly reduces transmission costs to 1/782 of previous methods."
Learning Better Representations for Crowded Pedestrians in Offboard LiDAR-Camera 3D Tracking-by-detection.,https://doi.org/10.1109/ICRA55743.2025.11128508,"Perceiving pedestrians in highly crowded urban environments is a difficult long-tail problem for learning-based autonomous perception. Speeding up 3D ground truth generation for such challenging scenes is performance-critical yet very challenging. The difficulties include the sparsity of the captured pedestrian point cloud and a lack of suitable benchmarks for a specific system design study. To tackle the challenges, we first collect a new multi-view LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded pedestrians for in-depth analysis. We then build an offboard auto-labeling system that reconstructs pedestrian trajectories from LiDAR point cloud and multi-view images. To improve the generalization power for crowded scenes and the performance for small objects, we propose to learn high-resolution representations that are density-aware and relationship-aware. Extensive experiments validate that our approach significantly improves the 3D pedestrian tracking performance towards higher auto-labeling efficiency. The code will be publicly available at this HTTP URL<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>https://github.com/Nicholasli1995/PCP-MV."
Bi-Stream Knowledge Transfer for Semi-Supervised 3D Point Cloud Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11127349,"3D point cloud object detection plays an important role in autonomous driving. However, labeling 3D object boxes is expensive and time-consuming, limiting the number of annotated point clouds used in fully-supervised training. This has led to a rise in semi-supervised 3D object detection research, which aims to improve model performance by leveraging both labeled and unlabeled point clouds. Existing methods typically rely on the Mean Teacher (MT) paradigm, which uses unlabeled instances discovered by the teacher with confidence scores higher than certain thresholds to train the student. However, this leads to a loss of information as it overlooks ambiguous instances from the teacher that could also contain valuable knowledge. To address this issue, we propose a Bi-Stream Knowledge Transfer (BiKT) framework that fully exploits and transfers knowledge from both confident and ambiguous instances to the student network. Specifically, all pseudo labels are allocated into two knowledge streams, the deterministic stream and the noisy stream, and then subsequently guide the student network through bi-level supervision. We also introduce a Dynamic Stream Switching (DSS) algorithm that sets the stream boundary tailored for the current learning status. To further improve the quality of pseudo labels in the knowledge streams, we propose a Diffusive Label Denoising (DLD) module, which is trained by explicitly generating noised instances and then learning to denoise them, as in diffusion models. Experiments show the state-of-the-art performance of our BiKT on the ONCE validation and testing sets, as well as the robust generalization capability when confronted with diverse base detectors, increased amount of unlabeled data, and distinct datasets (e.g., Waymo), unveiling the power of semi-supervised learning in 3D object detection."
Semantic-Supervised Spatial-Temporal Fusion for LiDAR-Based 3D Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11127867,"LiDAR-based 3D object detection presents significant challenges due to the inherent sparsity of LiDAR points. A common solution involves long-term temporal LiDAR data to densify the inputs. However, efficiently leveraging spatial-temporal information remains an open problem. In this paper, we propose a novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, which introduces a novel fusion module to relieve the spatial misalignment caused by the object motion over time and a feature-level semantic supervision to sufficiently unlock the capacity of the proposed fusion module. Specifically, the ST- Fusion consists of a Spatial Aggregation (SA) module and a Temporal Merging (TM) module. The SA module employs a convolutional layer with progressively expanding receptive fields to aggregate the object features from the local regions to alleviate the spatial misalignment, the TM module dynamically extracts object features from the preceding frames based on the attention mechanism for a comprehensive sequential presentation. Besides, in the semantic supervision, we propose a Semantic Injection method to enrich the sparse LiDAR data via injecting the point-wise semantic labels, using it for training a teacher model and providing a reconstruction target at the feature level supervised by the proposed object-aware loss. Extensive experiments on various LiDAR-based detectors demonstrate the effectiveness and universality of our proposal, yielding an improvement of approximately +2.8% in NDS based on the nuScenes benchmark."
OoDIS: Anomaly Instance Segmentation and Detection Benchmark.,https://doi.org/10.1109/ICRA55743.2025.11128111,"Safe navigation of self-driving cars and robots requires a precise understanding of their environment. Training data for perception systems cannot cover the wide variety of objects that may appear during deployment. Thus, reliable identification of unknown objects, such as wild animals and untypical obstacles, is critical due to their potential to cause serious accidents. Significant progress in semantic segmentation of anomalies has been facilitated by the availability of out-of-distribution (OOD) benchmarks. However, a comprehensive understanding of scene dynamics requires the segmentation of individual objects, and thus the segmentation of instances is essential. Development in this area has been lagging, largely due to the lack of dedicated benchmarks. The situation is similar in object detection. While there is interest in detecting and potentially tracking every anomalous object, the availability of dedicated benchmarks is clearly limited. To address this gap, this work extends some commonly used anomaly segmentation benchmarks to include the instance segmentation and object detection tasks. Our evaluation of anomaly instance segmentation and object detection methods shows that both of these challenges remain unsolved problems. We provide a competition and benchmark website under https://vision.rwth-aachen.de/oodis."
Safe Interval Motion Planning for Quadrotors in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127946,"Trajectory generation in dynamic environments presents a significant challenge for quadrotors, particularly due to the non-convexity in the spatial-temporal domain. Many existing methods either assume simplified static environments or struggle to produce optimal solutions in real-time. In this work, we propose an efficient safe interval motion planning framework for navigation in dynamic environments. A safe interval refers to a time window during which a specific configuration is safe. Our approach addresses trajectory generation through a two-stage process: a front-end graph search step followed by a back-end gradient-based optimization. We ensure completeness and optimality by constructing a dynamic connected visibility graph and incorporating low-order dynamic bounds within safe intervals and temporal corridors. To avoid local minima, we propose a Uniform Temporal Visibility Deformation (UTVD) for the complete evaluation of spatial-temporal topological equivalence. We represent trajectories with B-Spline curves and apply gradient-based optimization to navigate around static and moving obstacles within spatial-temporal corridors. Through simulation and real-world experiments, we show that our method can achieve a success rate of over <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{9 5 \%}$</tex> in environments with different density levels, exceeding the performance of other approaches, demonstrating its potential for practical deployment in highly dynamic environments."
Towards Safe and Energy-Efficient Real-Time Motion Planning in Windy Urban Environments.,https://doi.org/10.1109/ICRA55743.2025.11127986,"Urban winds are a serious hazard for low-altitude autonomous aerial operations in urban airspaces. Previous methods for motion planning in urban winds require global knowledge of the obstacles and flow field and do not lend themselves to real-time application. In this paper, a planning and control framework is proposed for safe and energy-efficient navigation through urban flow fields that strictly relies on onboard sensing. The algorithm incorporates predictions of local wind flow fields into a receding horizon optimal controller, balancing energy consumption with obstacle avoidance on the fly to reach a goal destination. Simulation studies on a procedurally generated urban map with diverse wind conditions demonstrate that the energy-aware motion planner reduces energy consumption by as much as 30% and results in 32% fewer crashes on average compared to the wind-agnostic baseline. Comparisons to a global wind-aware planner indicate only minor trade-offs associated with planning on a local horizon."
Dynamic Perception-Enhanced Motion Planning and Control for UAVs Flights in Challenging Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127690,"The autonomous flights of unmanned aerial vehicles (UAVs) in unknown environments have garnered significant attention. However, most existing methods only achieve safe navigation in static environments or spacious scenes with few moving obstacles. Motivated by this open problem, this paper presents a complete system for safe and autonomous UAVs flights in unknown clustered environments with multiple dynamic obstacles. To properly represent complex dynamic environments, we develop a 3D dynamic Euclidean Signed Distance Field (ESDF) mapping method that initially segments and tracks dynamic obstacles using a novel feature-based association strategy, while fusing the remaining static obstacles into ESDF map. Then, we propose a joint trajectory planning and motion control framework for safely avoiding surrounding obstacles. Specifically, the gradient-based B-spline trajectory optimization algorithm is employed to generate a collision-free static trajectory with respect to static obstacles. To avoid dynamic obstacles while adaptively tracking the static trajectory, we utilize time-adaptive model predictive control combined with Dynamic Control Barrier Function (D-CBF), which maps the collision avoidance constraints of dynamic obstacles onto the control inputs. Extensive simulated and real-world experiments confirm that our proposed method outperforms previous approaches for UAVs flights in challenging dynamic environments."
Real-Time Sampling-based Online Planning for Drone Interception.,https://doi.org/10.1109/ICRA55743.2025.11128529,"This paper studies high-speed online planning in dynamic environments. The problem requires finding time-optimal trajectories that conform to system dynamics, meeting computational constraints for real-time adaptation, and accounting for uncertainty from environmental changes. To address these challenges, we propose a sampling-based online planning algorithm that leverages neural network inference to replace time-consuming nonlinear trajectory optimization, enabling rapid exploration of multiple trajectory options under uncertainty. The proposed method is applied to the drone interception problem, where a defense drone must intercept a target while avoiding collisions and handling imperfect target predictions. The algorithm efficiently generates trajectories toward multiple potential target drone positions in parallel. It then assesses trajectory reachability by comparing traversal times with the target drone's predicted arrival time, ultimately selecting the minimum-time reachable trajectory. Through extensive validation in both simulated and real-world environments, we demonstrate our method's capability for high-rate online planning and its adaptability to unpredictable movements in unstructured settings."
Optimal Trajectory Planning for Cooperative Manipulation with Multiple Quadrotors Using Control Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11128734,"In this paper, we present a novel trajectory planning algorithm for cooperative manipulation with multiple quadrotors using control barrier functions (CBFs). Our approach addresses the complex dynamics of a system in which a team of quadrotors transports and manipulates a cable-suspended rigid-body payload in environments cluttered with obstacles. The proposed algorithm ensures obstacle avoidance for the entire system, including the quadrotors, cables, and the payload in all six degrees of freedom (DoF). We introduce the use of CBFs to enable safe and smooth maneuvers, effectively navigating through cluttered environments while accommodating the system's nonlinear dynamics. To simplify complex constraints, the system components are modeled as convex polytopes, and the Duality theorem is employed to reduce the computational complexity of the optimization problem. We validate the performance of our planning approach both in simulation and real-world environments using multiple quadrotors. The results demonstrate the effectiveness of the proposed approach in achieving obstacle avoidance and safe trajectory generation for cooperative transportation tasks."
Topology-Based Visual Active Room Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127294,"Room segmentation plays a significant role in scene understanding, semantic mapping, and scene coverage for robots navigating in real-world indoor environments. However, most previous works take a passive segmentation that requires a complete and uncluttered grid map as input, often resulting in lower segmentation accuracy and cannot be deployed in unknown environments. In this paper, we propose an active room segmentation framework that can enable a robot to incrementally and autonomously perform room segmentation in cluttered indoor environments. Our framework consists of three key components: i) a door extraction module where a visual semantic feature, specifically, door, is extracted to better identify rooms in cluttered environments, ii) a within-room exploration module that detects frontiers within the currently exploring room, and iii) a topological module that represents connectivity between rooms and determines next room for exploration. We show through experiments that the proposed method depicts two distinct advantages against existing methods in segmentation accuracy and autonomy. The code is available at https://github.com/FreeformRobotics/Active_room_segmentation."
ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127706,"Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr."
Online Diffusion-Based 3D Occupancy Prediction at the Frontier with Probabilistic Map Reconciliation.,https://doi.org/10.1109/ICRA55743.2025.11128832,"Autonomous navigation and exploration in unmapped environments remains a significant challenge in robotics due to the difficulty robots face in making commonsense inference of unobserved geometries. Recent advancements have demonstrated that generative modeling techniques, particularly diffusion models, can enable systems to infer these geometries from partial observation. In this work, we present implementation details and results for real-time, online occupancy prediction using a modified diffusion model. By removing attention-based visual conditioning and visual feature extraction components, we achieve a 73% reduction in runtime with minimal accuracy reduction. These modifications enable occupancy prediction across the entire map, rather than limiting it to the area around the robot where sensor data can be collected. We introduce a probabilistic update method for merging predicted occupancy data into running occupancy maps, resulting in a 71% improvement in predicting occupancy at map frontiers compared to previous methods. Finally, our code and a ROS node for on-robot operation can be found on our website: https://arpg.github.io/scenesense/."
Point2Graph: An End-to-End Point Cloud-Based 3D Open-Vocabulary Scene Graph for Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127471,"Current open-vocabulary scene graph generation algorithms highly rely on both 3D scene point cloud data and posed RGB-D images and thus have limited applications in scenarios where RGB-D images or camera poses are not readily available. To solve this problem, we propose Point2Graph, a novel end-to-end point cloud-based 3D open-vocabulary scene graph generation framework in which the requirement of posed RGB-D image series is eliminated. This hierarchical framework contains room and object detection/segmentation and openvocabulary classification. For the room layer, we leverage the advantage of merging the geometry-based border detection algorithm with the learning-based region detection to segment rooms and create a Snap-Lookup framework for openvocabulary room classification. In addition, we create an end-toend pipeline for the object layer to detect and classify 3D objects based solely on 3D point cloud data. Our evaluation results show that our framework can outperform the current state-of-the-art (SOTA) open-vocabulary object and room segmentation and classification algorithm on widely used real-scene datasets."
Estimating Commonsense Scene Composition on Belief Scene Graphs.,https://doi.org/10.1109/ICRA55743.2025.11127920,"This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/f0tqtPVFZ2A"
VLM-Vac: Enhancing Smart Vacuums Through VLM Knowledge Distillation and Language-Guided Experience Replay.,https://doi.org/10.1109/ICRA55743.2025.11128167,"In this paper, we propose VLM-Vac, a novel framework designed to enhance the autonomy of smart robot vacuum cleaners. Our approach integrates the zero-shot object detection capabilities of a Vision-Language Model (VLM) with a Knowledge Distillation (KD) strategy. By leveraging the VLM, the robot can categorize objects into actionable classes-either to avoid or to suck-across diverse backgrounds. However, frequently querying the VLM is computationally expensive and impractical for real-world deployment. To address this issue, we implement a KD process that gradually transfers the essential knowledge of the VLM to a smaller, more efficient model. Our real-world experiments demonstrate that this smaller model progressively learns from the VLM and requires significantly fewer queries over time. Additionally, we tackle the challenge of continual learning in dynamic home environments by exploiting a novel experience replay method based on languageguided sampling. Our results show that this approach not only reduces energy consumption by 53 % compared to cumulative learning but also surpasses conventional vision-based clustering methods, particularly in detecting small objects across diverse backgrounds."
Development of a New Biped Robot with Adaptive Suction Modules for Climbing on Curved Surfaces.,https://doi.org/10.1109/ICRA55743.2025.11127762,"Regular cleaning and maintenance of high-altitude pipes and curved surfaces on high-rise buildings are high-risk tasks for human workers due to the difficulty of working on curved planes. To address such challenge, automated robots are widely used for cleaning buildings with flat walls, but they cannot climb on curved surfaces, limiting their practical applications. This paper proposes a novel biped curved-surface climbing robot (BCCR) with five-degree-of-freedom (5-DOF) motion. The BCCR features adaptive vacuum suction modules that can adhere to both curved and flat surfaces, allowing seamless movement of the BCCR across various surfaces. Each terminal suction module is composed of three small suction cups, which are capable of rotating in all directions to achieve adaptive adhesion on various surfaces. The 5-DOF structure enables the robot to cross obstacles and makes it highly versatile for various cleaning tasks on a wide range of surfaces, including large curved pipes. The mechanism design and analytical modeling of the BCCR are carried out, demonstrating its robust curved-surface climbing capabilities. Moreover, a prototype is fabricated for experimental investigation. The results indicate that the proposed 5-DOF BCCR can achieve stable climbing on curved surfaces."
Berkeley Humanoid: A Research Platform for Learning-Based Control.,https://doi.org/10.1109/ICRA55743.2025.11127524,"We introduce Berkeley Humanoid, a reliable and low-cost mid-scale humanoid research platform for learningbased control. Our lightweight, in-house-built robot is designed specifically for learning algorithms with accurate simulation, low simulation complexity, anthropomorphic motion, and high reliability against falls. The narrow sim-to-real gap enables agile and robust locomotion across various terrains in outdoor environments, achieved with a simple reinforcement learning controller using light domain randomization. Furthermore, we demonstrate the robot traversing for hundreds of meters, walking on a steep unpaved trail, and hopping with single and double legs as a testimony to its high performance in dynamic walking. Capable of omnidirectional locomotion and withstanding large perturbations with a compact setup, our system aims for rapid sim-to-real deployment of learningbased humanoid systems. Please check our website https:// berkeley-humanoid.com/ and code https://github. com/HybridRobotics/isaac_berkeley_humanoid/."
Zippy: The Smallest Power-Autonomous Bipedal Robot.,https://doi.org/10.1109/ICRA55743.2025.11128531,"Miniaturizing legged robot platforms is challenging due to hardware limitations that constrain the number, power density, and precision of actuators at that size. By leveraging design principles of quasi-passive walking robots at any scale, stable locomotion and steering can be achieved with simple mechanisms and open-loop control. Here, we present the design and control of Zippy, the smallest self-contained bipedal walking robot at only 3.6 cm tall. Zippy has rounded feet, a single motor without feedback control, and is capable of turning, skipping, and ascending steps. At its fastest pace, the robot achieves a forward walking speed of 25 cm/s, which is 10 leg lengths per second, the fastest biped robot of any size by that metric. This work explores the design and performance of the robot and compares it to similar dynamic walking robots at larger scales."
Exploration and Analysis of Torso-Limb Coordination of Quadruped Walkers with Compliant Torso.,https://doi.org/10.1109/ICRA55743.2025.11127568,"Quadrupeds exhibit remarkable locomotion performance through the coordination between their limbs and torso. From past biological knowledge, it is understood that during walking, the forelimbs primarily contribute to braking, while the hindlimb are responsible for propulsion. However, in the field of quadruped robot dynamics, effectively leveraging this coordination remains a challenge. To investigate the torso-limb coordination, this study explores the walking performance of a quadruped walker with a compliant torso, driven by the forelimb or the hindlimb. Through numerical simulations, we analyze the walking behavior under different control drive methods. The findings provide insights into the design of compliant-bodied robots and the optimal distribution of propulsion forces between the forelimbs and hindlimbs."
Effective Self-Righting Strategies for Elongate Multi-Legged Robots.,https://doi.org/10.1109/ICRA55743.2025.11128306,"Centipede-like robots offer an effective and robust solution to navigation over complex terrain with minimal sensing. However, when climbing over obstacles, such multi-legged robots often elevate their center-of-mass into unstable configurations, where even moderate terrain uncertainty can cause tipping. Robust mechanisms for such elongate multi-legged robots to self-right remain unstudied. Here, we use a comparative biological and robophysical approach to investigate self-righting strategies. We first released S. polymorpha upside down from a 10 cm height and recorded their self-righting behaviors using top and side view high-speed cameras. Using kinematic analysis, we hypothesize that these behaviors can be prescribed by two traveling waves superimposed in the body's lateral and vertical planes, respectively. We tested our hypothesis on an elongate robot with static (non-actuated) limbs, and we successfully reconstructed these self-righting behaviors. We further evaluated how wave parameters affect self-righting effectiveness. We identified two key wave parameters: the spatial frequency, which characterizes the sequence of body-rolling, and the wave amplitude, which characterizes body curvature. By empirically obtaining a behavior diagram of spatial frequency and amplitude, we identify effective and versatile self-righting strategies for general elongate multi-legged robots, which greatly enhances these robots' mobility and robustness in practical applications such as agricultural terrain inspection and search-and-rescue."
Addition of a Peristaltic Wave Improves Multi-Legged Locomotion Performance on Complex Terrains.,https://doi.org/10.1109/ICRA55743.2025.11127656,"Characterized by their elongate bodies and relatively simple legs, multi-legged robots have the potential to locomote through complex terrains for applications such as search-and-rescue and terrain inspection. Prior work has developed effective and reliable locomotion strategies for multilegged robots by propagating the two waves of lateral body undulation and leg stepping, which we will refer to as the twowave template. However, these robots have limited capability to climb over obstacles with sizes comparable to their heights. We hypothesize that such limitations stem from the twowave template that we used to prescribe the multi-legged locomotion. Seeking effective alternative waves for obstacleclimbing, we designed a five-segment robot with static (nonactuated) legs, where each cable-driven joint has a rotational degree-of-freedom (DoF) in the sagittal plane (vertical wave) and a linear DoF (peristaltic wave). We tested robot locomotion performance on a flat terrain and a rugose terrain. While the benefit of peristalsis on flat-ground locomotion is marginal, the inclusion of a peristaltic wave substantially improves the locomotion performance in rugose terrains: it not only enables obstacle-climbing capabilities with obstacles having a similar height as the robot, but it also significantly improves the traversing capabilities of the robot in such terrains. Our results demonstrate an alternative actuation mechanism for multilegged robots, paving the way towards all-terrain multi-legged robots."
"Pre-Surgical Planner for Robot-Assisted Vitreoretinal Surgery: Integrating Eye Posture, Robot Position and Insertion Point.",https://doi.org/10.1109/ICRA55743.2025.11128537,"Several robotic frameworks have been recently developed to assist ophthalmic surgeons in performing complex vitreoretinal procedures such as subretinal injection of advanced therapeutics. These surgical robots show promising capabilities; however, most of them have to limit their working volume to achieve maximum accuracy. Moreover, the visible area seen through the surgical microscope is limited and solely depends on the eye posture. If the eye posture, trocar position, and robot configuration are not correctly arranged, the instrument may not reach the target position, and the preparation will have to be redone. Therefore, this paper proposes the optimization framework of the eye tilting and the robot positioning to reach various target areas for different patients. Our method was validated with an adjustable phantom eye model, and the error of this workflow was 0.13  1.65 deg (rotational joint around Y axis), -1.40  1.13 deg (around X axis), and 1.80  1.51 mm (depth, Z). The potential error sources are also analyzed in the discussion section."
Suture Thread Modeling Using Control Barrier Functions for Autonomous Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128571,"Automating surgical systems enhances precision and safety while reducing human involvement in high-risk environments. A major challenge in automating surgical procedures like suturing is accurately modeling the suture thread, a highly flexible and compliant component. Existing models either lack the accuracy needed for safety-critical procedures or are too computationally intensive for real-time execution. In this work, we introduce a novel approach for modeling suture thread dynamics using control barrier functions (CBFs), achieving both realism and computational efficiency. Thread-like behavior, collision avoidance, stiffness, and damping are all modeled within a unified CBF and control Lyapunov function (CLFs) framework. Our approach eliminates the need to calculate complex forces or solve differential equations, significantly reducing computational overhead while maintaining a realistic model suitable for both automation and virtual reality surgical training systems. The framework also allows visual cues to be provided based on the thread's interaction with the environment, enhancing user experience when performing suture or ligation tasks. The proposed model is tested on the MagnetoSuture system, a minimally invasive robotic surgical platform that uses magnetic fields to manipulate suture needles, offering a less invasive solution for surgical procedures."
Robotic Colonoscopy: Can High Fidelity Simulation Optimize Robot Design and Validation?,https://doi.org/10.1109/ICRA55743.2025.11128805,"This paper presents the use of a simulation environment as an accurate, ethical, and sustainable alternative to testing robotic prototypes in animal models and simplified phantom models, specifically developed for robotic colonoscopy devices inside the human colon. A virtual simulation of the locomotion mechanism of a prototype robotic colonoscope and the colon was created in Ansys, and robot/colon experiments were conducted on different colon surfaces to validate simulation results. The successfully simulated propulsion force generated by the prototype produced an RMSE of 7% when compared at the optimal operating condition of the device, and 25-30% when compared to a full range of device velocities. The larger RMSE is due to physical phenomena that were not present in the simulation due to the constraints applied. The simulation, however, allowed evaluation of difficult quantities to measure in a real world setting such as the normal interaction force between the device and tissue wall, and stress distribution across the locomotion mechanism, as well as a phenomenon of oscillating propulsion force resulting from the device design. This work demonstrates feasibility of using finite element simulation to shape the design and optimization of a robotic colonoscope and understands its interaction with complex human anatomy."
Robotic Tissue Manipulation in Endoscopic Submucosal Dissection Via Visual Feedback.,https://doi.org/10.1109/ICRA55743.2025.11127708,"Colorectal cancer is the third most commonly diagnosed cancer and the second leading cause of cancer-related deaths in the United States. Despite advancements in screening and treatment, there remains a critical need for more effective and minimally invasive methods to manage complex polyps and early-stage colorectal cancers. This study introduces a novel approach to magnetic tissue manipulation for Endoscopic Submucosal Dissection (ESD), leveraging visual feedback to enhance precision and control. We develop and evaluate the proposed system within a ROS Gazebo simulation environment, integrating a small magnetic endoscopic clip affixed to tissue, which is manipulated by an external large magnet mounted on a robotic arm. A key challenge in ESD is achieving adequate tissue exposure for precise cutting, particularly in the confined space of the colon where the endoscope is manually controlled. To address this, our system enables controlled manipulation of the magnetic clip to optimize tissue retraction. The robotic arm, guided by real-time visual feedback, dynamically adjusts the internal clip's orientation. Multiple virtual cameras were used to validate the proposed method. The simulation results demonstrated that the robot arm successfully manipulated the internal magnetic clip to the desired tilt angle within an average of 8.4 seconds (range 5.3 to <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{1 5. 2 ~ s}$</tex>). Our findings suggest that robotic-assisted magnetic tissue manipulation has the potential to improve ESD success rates while reducing procedure time, paving the way for further advancements in minimally invasive endoscopic surgery."
Multi-Covering a Point Set by $m$ Disks with Minimum Total Area.,https://doi.org/10.1109/ICRA55743.2025.11127835,"A common robotics sensing problem is to place sensors to robustly monitor a set of assets, where robustness is assured by requiring asset <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$p$</tex> to be monitored by at least <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\kappa(p)$</tex> sen-sors. Given <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$n$</tex> assets that must be observed by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$m$</tex> sensors, each with a disk-shaped sensing region, where should the sensors be placed to minimize the total area observed? We provide and analyze a fast heuristic for this problem. We then use the heuristic to initialize an exact Integer Program-ming solution. Subsequently, we enforce separation constraints between the sensors by modifying the integer program formulation and by changing the disk candidate set."
Non-Conservative Obstacle Avoidance for Multi-Body Systems Leveraging Convex Hulls and Predicted Closest Points.,https://doi.org/10.1109/ICRA55743.2025.11128772,"This paper introduces a novel approach that integrates future closest point predictions into the distance constraints of a collision avoidance controller, leveraging convex hulls with closest point distance calculations. By addressing abrupt shifts in closest points, this method effectively reduces collision risks and enhances controller performance. Applied to an Image Guided Therapy robot and validated through simulations and user experiments, the framework demonstrates improved distance prediction accuracy, smoother trajectories, and safer navigation near obstacles."
Adaptive Distance Functions via Kelvin Transformation.,https://doi.org/10.1109/ICRA55743.2025.11128013,"The term safety in robotics is often understood as a synonym for avoidance. Although this perspective has led to progress in path planning and reactive control, a generalization of this perspective is necessary to include task semantics relevant to contact-rich manipulation tasks, especially during teleoperation and to ensure the safety of learned policies. We introduce the semantics-aware distance function and a corresponding computational method based on the Kelvin Transformation. This allows us to compute smooth distance approximations in an unbounded domain by instead solving a Laplace equation in a bounded domain. The semantics-aware distance generalizes signed distance functions by allowing the zero level set to lie inside of the object in regions where contact is allowed, effectively incorporating task semantics, such as object affordances, in an adaptive implicit representation of safe sets. In numerical experiments we show the computational viability of our method for real applications and visualize the computed function on a wrench with various semantic regions."
Provable Methods for Searching with an Imperfect Sensor.,https://doi.org/10.1109/ICRA55743.2025.11128471,"Assume that a target is known to be present at an unknown point among a finite set of locations in the plane. We search for it using a mobile robot that has imperfect sensing capabilities. It takes time for the robot to move between locations and search a location; we have a total time budget within which to conduct the search. We study the problem of computing a search path/strategy for the robot that maximizes the probability of detection of the target. Considering non-uniform travel times between points (e.g., based on the distance between them) is crucial for search and rescue applications; such problems have been investigated to a limited extent due to their inherent complexity. In this paper, we describe fast algorithms with performance guarantees for this search problem and some variants, complement them with complexity results, and perform experiments to characterize their performance."
Safe Radial Segregation Algorithm for Swarms of Dubins-Like Robots.,https://doi.org/10.1109/ICRA55743.2025.11128484,"This work addresses the problem of radially segregating heterogeneous robotic swarms. Such swarms are those composed of different groups of robots. Unlike other works on segregation in the literature, we propose a controller for Dubins-like robots, motivated by autonomous aerial, wheeled, and underwater vehicles. Our controller can drive the robots individually to converge to circles that are shared only by robots of the same group. We present a heuristic and a collision avoidance scheme in which the information required is locally acquired. We present several simulations widely varying the number of robots per group and the number of groups in which segregation is always reached and collisions between robots are always avoided."
Impossibility of Self-Organized Aggregation Without Computation.,https://doi.org/10.1109/ICRA55743.2025.11127642,"In their seminal work, Gauci et al. (2014) studied the fundamental task of aggregation, wherein multiple robots need to gather without an a priori agreed-upon meeting location, using extremely limited hardware. That paper considered differential-drive robots that are memoryless and unable to compute. Moreover, the robots cannot communicate with one another and are only equipped with a simple sensor that determines whether another robot is directly in front of them. Despite those severe limitations, Gauci et al. introduced a controller and proved mathematically that it aggregates a system of two robots for any initial state. Unfortunately, for larger systems, the same controller aggregates empirically in many cases but not all. Thus, the question of whether there exists a controller that aggregates for any number of robots remains open. In this paper, we show that no such controller exists by investigating the geometric structure of controllers. In addition, we disprove the aggregation proof of the aforementioned paper for two robots and present an alternative controller alongside a simple and rigorous aggregation proof."
Realizing Emergent Collective Behaviors Through Robotic Swarmalators.,https://doi.org/10.1109/ICRA55743.2025.11128695,"Swarmalators move as a function of their pairwise phase interactions, and control their phase as a function of their relative position or motion to other agents. This enables dual sync and swarm behaviors that mimic those exhibited by diverse natural and artificial swarms; these behaviors have almost entirely been explored only through computational simulations. Here, we realize through a 15-robot collective many of the predicted swarmalator behaviors when agents are chiral and non-chiral, when there is frequency coupling, and when the natural frequency distribution is homogeneous and heterogeneous. This work presents an experimental platform that can realize many theoretically predicted collective behaviors, it sheds light on the differences between the simulations and experiments, and it will serve in future studies to realize swarmalator and active matter collective behaviors."
Evaluating Robotic Performative Autonomy in Collaborative Contexts Impacted by Latency.,https://doi.org/10.1109/ICRA55743.2025.11128197,"Maintaining Situational Awareness (SA) is critical in space exploration contexts, yet made particularly difficult due to the presence of communication latency. In order to increase human SA without inducing cognitive overload, researchers have proposed Performative Autonomy (PA), in which robots intentionally interact at a lower level of autonomy than they are capable of. While researchers have demonstrated positive impacts of PA on team performance even under high latency, previous work on PA has not examined how the benefits of PA might be mediated by latency. In this work, we thus evaluate the impact of latency and PA on trust, SA, and human perceptions of robot intelligence and autonomy. Our results suggest that lower performed autonomy leads to increased cognitive load, especially when robot communication happens frequently and latency is present. In addition, we observe no effect of the PA strategies used within our experimental paradigm on SA, and instead find evidence that operating under high latency leads to negative perceptions of robots regardless of choice of PA strategy."
SYNERGAI: Perception Alignment for Human-Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11128658,"Recently, large language models (LLMs) have shown strong potential in facilitating human-robotic interaction and collaboration. However, existing LLM-based systems often overlook the misalignment between human and robot perceptions, which hinders their effective communication and real-world robot deployment. To address this issue, we introduce SYNERGAI, a unified system designed to achieve both perceptual alignment and human-robot collaboration. At its core, SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and innate representation. This enables the system to leverage LLM to break down complex tasks and allocate appropriate tools in intermediate steps to extract relevant information from the 3DSG, modify its structure, or generate responses. Importantly, SYNERGAI incorporates an automatic mechanism that enables perceptual misalignment correction with users by updating its 3DSG with online interaction. SYNERGAI achieves comparable performance with the data-driven models in ScanQA in a zero-shot manner. Through comprehensive experiments across 10 real-world scenes, SYNERGAI demonstrates its effectiveness in establishing common ground with humans, realizing a success rate of 61.9 % in alignment tasks. It also significantly improves the success rate from 3.7% to 45.68 % on novel tasks by transferring the knowledge acquired during alignment."
Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task Allocation in Human-Robot Collaborative Assemblies.,https://doi.org/10.1109/ICRA55743.2025.11127401,"This paper addresses the optimization of human-robot collaborative work-cells before their physical deployment. Most of the times, such environments are designed based on the experience of the system integrators, often leading to sub-optimal solutions. Accurate simulators of the robotic cell, accounting for the presence of the human as well, are available today and can be used in the pre-deployment. We propose an iterative optimization scheme where a digital model of the work-cell is updated based on a genetic algorithm. The methodology focuses on the layout optimization and task allocation, encoding both the problems simultaneously in the design variables handled by the genetic algorithm, while the task scheduling problem depends on the result of the upper-level one. The final solution balances conflicting objectives in the fitness function and is validated to show the impact of the objectives with respect to a baseline, which represents possible initial choices selected based on the human judgment."
Context-Aware Collaborative Pushing of Heavy Objects Using Skeleton-Based Intention Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128820,"In physical human-robot interaction, force feedback has been the most common sensing modality to convey the human intention to the robot. It is widely used in admittance control to allow the human to direct the robot. However, it cannot be used in scenarios where direct force feedback is not available since manipulated objects are not always equipped with a force sensor. In this work, we study one such scenario: the collaborative pushing and pulling of heavy objects on frictional surfaces, a prevalent task in industrial settings. When humans do it, they communicate through verbal and non-verbal cues, where body poses, and movements often convey more than words. We propose a novel context-aware approach using Directed Graph Neural Networks to analyze spatiotemporal human posture data to predict human motion intention for non-verbal collaborative physical manipulation. Our experiments demonstrate that robot assistance significantly reduces human effort and improves task efficiency. The results indicate that incorporating posture-based context recognition, either together with or as an alternative to force sensing, enhances robot decision-making and control efficiency."
A Stochastic Cloning Square-Root Information Filter with Accurate Feature Tracking for Visual-Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128386,"In this work, we introduce an enhanced square-root information filter for visual-inertial odometry. This filter utilizes stochastic cloning, implemented via Gaussian elimination, to facilitate time offset calibration and feature anchor changes. By using single-precision numbers within the filter, we significantly reduce computational load and memory requirements. In addition, we employ a fast Mahalanobis distance test and block Householder triangulation to accelerate the calculations. To mitigate feature drift from frame-to-frame optical flow, we create keyframes at regular intervals and refine long-tracked features between them. We use affine optical flow to compensate for patch deformations induced by possible large spatial transformations between keyframes. An analytical approach to computing the affine transformation is proposed. Experiments conducted on real-world data show that the proposed method achieves state-of-the-art performance at a much faster speed."
Large-Scale UWB Anchor Calibration and One-Shot Localization Using Gaussian Process.,https://doi.org/10.1109/ICRA55743.2025.11127688,"Ultra-wideband (UWB) is gaining popularity with devices like AirTags for precise home item localization but faces significant challenges when scaled to large environments like seaports. The main challenges are calibration and localization under obstructed conditions, which are common in logistics environments. Traditional calibration methods, dependent on line-of-sight (LoS), are slow, costly, and unreliable in seaports and warehouses, making large-scale localization a significant pain point in the industry. To overcome these challenges, we propose a one-shot calibration and localization framework based on UWB-LiDAR fusion. Our method uses Gaussian processes to estimate the anchor position from continuous-time LiDAR Inertial Odometry with sampled UWB ranges. This approach ensures accurate and reliable calibration with only one round of sampling in large-scale areas, i.e., <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$600 \times 450 ~\mathrm{m}^{2}$</tex>. With LoS issues, UWB-only localization can be problematic, even when anchor positions are known. We demonstrate that by applying a UWB-range filter, the search range for LiDAR loop closure descriptors is significantly reduced, improving both accuracy and speed. This concept can be applied to other loop closure detection methods, enabling cost-effective localization in large-scale warehouses and seaports. It significantly improves precision in challenging environments where the UWB-only and LiDAR-Inertial methods fail, as shown in the video https://https://youtu.be/oY8jQKdM7lU. We will open-source our datasets and calibration codes for community use."
Online Identification of Skidding Modes with Interactive Multiple Model Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128086,"Skid-steered wheel mobile robots (SSWMRs) operate in a variety of outdoor environments exhibiting motion behaviors dominated by the effects of complex wheel-ground interactions. Characterizing these interactions is crucial from both the immediate robot autonomy perspective (for motion prediction and control) and a long-term predictive maintenance and diagnostics perspective. An ideal solution entails capturing precise state measurements for decisions and controls, which is considerably difficult, especially in increasingly unstructured outdoor regimes of operations for these robots. In this milieu, a framework to identify pre-determined discrete modes of operation can considerably simplify the motion model identification process. To this end, we propose an interactive multiple model (IMM) based filtering framework to probabilistically identify predefined robot operation modes that could arise due to traversal in different terrains or loss of wheel traction."
RLCNet: A Novel Deep Feature-Matching-Based Method for Online Target-Free Radar-LiDAR Calibration.,https://doi.org/10.1109/ICRA55743.2025.11127342,"While millimeter-wave radars are widely used in robotics and autonomous driving, extrinsic calibration with other sensors remains challenging due to the sparsity and uncertainty of radar point clouds. In this paper, we propose a novel deep feature-matching-based online extrinsic calibration approach for a 4D millimeter-wave radar and 3D LiDAR system. We formulate the calibration problem as a crossmodal point cloud registration task, initiating with keypointlevel matching followed by dense matching refinement. Efficient yet powerful neural networks are employed to extract prior keypoint matches, which are then expanded to surrounding regions, establishing dense point correspondences. Our approach effectively leverages the majority of the information from millimeter-wave radar, mitigating the impact of radar point cloud sparsity. We evaluate our approach on two datasets, and experimental results demonstrate that it outperforms state-of-the-art baseline methods and achieves an average improvement of 66.96% in calibration success rate, while reducing translational error and rotational error by 23.84% and 30.31%, respectively. Our implementation will be made open-source at https://github.com/nubot-nudt/RLCNet."
Gaussian Splatting Visual MPC for Granular Media Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128002,"Recent advancements in learned 3D representations have enabled significant progress in solving complex robotic manipulation tasks, particularly for rigid-body objects. However, manipulating granular materials such as beans, nuts, and rice remains challenging due to the intricate physics of particle interactions, high-dimensional and partially observable state, inability to visually track individual particles in a pile, and the computational demands of accurate dynamics prediction. Current deep latent dynamics models often struggle to generalize in granular material manipulation due to a lack of inductive biases. In this work, we propose a novel approach that learns a visual dynamics model over Gaussian splatting repre-sentations of scenes and leverages this model for manipulating granular media via Model-Predictive Control. Our method enables efficient optimization for complex manipulation tasks on piles of granular media. We evaluate our approach in both simulated and real-world settings, demonstrating its ability to solve unseen planning tasks and generalize to new environments in a zero-shot transfer. We also show significant prediction and manipulation performance improvements compared to existing granular media manipulation methods."
LE-Object: Language Embedded Object-Level Neural Radiance Fields for Open-Vocabulary Scene.,https://doi.org/10.1109/ICRA55743.2025.11128304,"Recent advancements in Visual Language Models (VLMs) have significantly driven research in open-vocabulary 3D scene reconstruction, showcasing strong potential in open-set retrieval and semantic understanding. However, existing approaches face challenges in open-world environments: they either suffer from insufficient precision in semantic segmentation, leading to inadequate fine-grained scene understanding, or they are limited to object-level reconstruction, failing to capture intricate object details and lack applicability in open-world settings. To address these issues, we introduce LE-Object, an object-centric Neural Implicit Radiance Field (NeRF) method for open-world scenarios to achieve fine-grained scene understanding and high-fidelity object reconstruction. LE-Object integrates spatial features (SF) from object point clouds with visual features (VF) from VLMs to perform object association, ensuring spatiotemporal consistency in object mask segmentation, and extends VLM features from 2D images into 3D space, enabling precise open-world semantic inference and detailed object reconstruction. Experimental results demonstrate that LE-Object excels in zero-shot semantic segmentation and open-world object reconstruction, offering innovative solutions for global navigation and local object manipulation in open-world applications."
TranSplat: Surface Embedding-Guided 3D Gaussian Splatting for Transparent Object Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127360,"Transparent object manipulation remains a significant challenge in robotics due to the difficulty of acquiring accurate and dense depth measurements. Conventional depth sensors often fail with transparent objects, resulting in incomplete or erroneous depth data. Existing depth completion methods struggle with interframe consistency and incorrectly model transparent objects as Lambertian surfaces, leading to poor depth reconstruction. To address these challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian Splatting method tailored for transparent objects. TranSplat uses a latent diffusion model to generate surface embeddings that provide consistent and continuous representations, making it robust to changes in viewpoint and lighting. By integrating these surface embeddings with input RGB images, TranSplat effectively captures the complexities of transparent surfaces, enhancing the splatting of 3D Gaussians and improving depth completion. Evaluations on synthetic and real-world transparent object benchmarks, as well as robot grasping tasks, show that TranSplat achieves accurate and dense depth completion, demonstrating its effectiveness in practical applications. We open-source synthetic dataset and model: https://github.com/jeongyun0609/TranSplat"
NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection.,https://doi.org/10.1109/ICRA55743.2025.11127348,"Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/."
Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127233,"We propose a framework for active next best view and touch selection for robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to represent scenes in a both photorealistic and geometrically accurate manner. However, in real-world, online robotic scenes where the number of views is limited given efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping and redundant. We address this issue by proposing an end-to-end online training and active view selection pipeline, which enhances the performance of 3DGS in few-view robotics settings. We first elevate the performance of few-shot 3DGS with a novel semantic depth alignment method using Segment Anything Model 2 (SAM2) that we supplement with Pearson depth and surface normal loss to improve color and depth reconstruction of real-world scenes. We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based on depth uncertainty. We perform online view selection on a real robot system during live 3DGS training. We motivate our improvements to few-shot GS scenes, and extend depth-based FisherRF to them, where we demonstrate both qualitative and quantitative improvements on challenging robot scenes. For more information, please see our project page at arm.stanford.edu/next-best-sense."
Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects.,https://doi.org/10.1109/ICRA55743.2025.11127328,"Tracking and manipulating irregularly-shaped, previously unseen objects in dynamic environments is important for robotic applications in manufacturing, assembly, and logistics. Recently introduced Gaussian Splats [1] efficiently model object geometry, but lack persistent state estimation for taskoriented manipulation. We present Persistent Object Gaussian Splat (POGS), a system that embeds semantics, self-supervised visual features, and object grouping features into a compact representation that can be continuously updated to estimate the pose of scanned objects. POGS updates object states without requiring expensive rescanning or prior CAD models of objects. After an initial multi-view scene capture and training phase, POGS uses a single stereo camera to integrate depth estimates along with self-supervised vision encoder features for object pose estimation. POGS supports grasping, reorientation, and natural language-driven manipulation by refining object pose estimates, facilitating sequential object reset operations with human-induced object perturbations and tool servoing, where robots recover tool pose despite tool perturbations of up to 30. POGS achieves up to 12 consecutive successful object resets and recovers from 80% of in-grasp tool perturbations."
Tactile Functasets: Neural Implicit Representations of Tactile Datasets.,https://doi.org/10.1109/ICRA55743.2025.11127929,"Modern incarnations of tactile sensors produce high-dimensional raw sensory feedback such as images, making it challenging to efficiently store, process, and generalize across sensors. To address these concerns, we introduce a novel implicit function representation for tactile sensor feedback. Rather than directly using raw tactile images, we propose neural implicit functions trained to reconstruct the tactile dataset, producing compact representations that capture the underlying structure of the sensory inputs. These representations offer several advantages over their raw counterparts: they are compact, enable probabilistically interpretable inference, and facilitate generalization across different sensors. We demonstrate the efficacy of this representation on the downstream task of in-hand object pose estimation, achieving improved performance over image-based methods while simplifying downstream models. We release code, demos and datasets at https://www.mmintlab.com/tactile-functasets."
LoFSORT: Sample Online and Real-time Tracking in Low Frame Rate Scenarios.,https://doi.org/10.1109/ICRA55743.2025.11128838,"We propose a novel motion-based tracker specifically designed for tracking multiple people in low frame rate scenarios. While previous studies have predominantly focused on scenarios with high frame rates (exceeding 10 frames per second), tracking in low frame rate conditions is significant for robotic platforms with limited computational resources. Our tracker optimizes the cost function, cascade structure and Kalman filter correction to better adapt to the characteristics of low frame rate environments. First, we enhance the cost function by incorporating stable variables through the introduction of height-based and displacement-based cost terms. Second, we prioritize handling occlusion among individuals during association, which reduces ambiguity in subsequent tracking processes. Third, we utilize the error-compensated detection to correct the Kalman filter, thereby improving tracking accuracy. Experimental results demonstrate that our proposed tracker, LoFSORT, outperforms other motion model-based trackers across various frame rate scenarios. Ablation studies further confirm that each component of our tracker enhances tracking performance in low frame rate scenarios."
Multirotor Target Tracking through Policy Iteration for Visual Servoing.,https://doi.org/10.1109/ICRA55743.2025.11127879,"This paper presents a novel vision-based approach for tracking deformable contour targets using Unmanned Aerial Vehicles (UAVs) through combining image moments descriptor and a Policy Iteration scheme ensuring stability and generalization of knowledge to new tasks. This computationally efficient and optimal control scheme is suitable for diverse dynamic environments such as the surveillance and tracking of targets with evolving features. Due to the ability of the proposed scheme to comprehend an optimization output, the generated control sequence, from an offline successively approximated policy, makes the process less challenging. The proposed methodology is validated through extensive simulations and real-word exper-iments of environmental target surveillance using an octorotor UAV."
BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using Camera-LiDAR Data.,https://doi.org/10.1109/ICRA55743.2025.11128796,"Compared with real-time multi-object tracking (MOT), offline multi-object tracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous link correction, and full track optimization but has to deal with the challenges from bounding box misalignment and track evaluation, editing, and refinement. This paper proposes BiTrack, a 3D OMOT framework that includes modules of 2D-3D detection fusion, initial trajectory generation, and bidirectional trajectory re-optimization to achieve optimal tracking results from camera-LiDAR data. The novelty of this paper includes threefold: (1) development of a point-level object registration technique that employs a density-based similarity metric to achieve accurate fusion of 2D-3D detection results; (2) development of a set of data association and track management skills that utilizes a vertex-based similarity metric as well as false alarm rejection and track recovery mechanisms to generate reliable bidirectional object trajectories; (3) development of a trajectory re-optimization scheme that re-organizes track fragments of different fidelities in a greedy fashion, as well as refines each trajectory with completion and smoothing techniques. The experiment results on the KITTI dataset demonstrate that BiTrack achieves the state-of-the-art performance for 3D OMOT tasks in terms of accuracy and efficiency."
ConTrack3D: Contrastive Learning Contributes Concise 3D Multi-Object Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128704,"Online object detection and tracking are crucial for embodied intelligence systems, including autonomous vehicles and robotics. Traditional approaches employ a pipeline structure to perform detection and tracking separately, which can not fully leverage information from the detector. Moreover, most prior tracking methods rely on motion models such as constant velocity for state updates, which can lead to incorrect associations when the velocity estimates are inaccurate. To address these limitations, we propose ConTrack3D, an online tracking approach that jointly performs detection and tracking in an end-to-end manner. Specifically, ConTrack3D incorporates a Joint Encoder module to capture detection embeddings and a Temporal Extender module for data-driven state updates. By employing contrastive learning, ConTrack3D learns discriminative tracking representation for more accurate association. ConTrack3D is evaluated on the nuScenes benchmark, and the experimental results demonstrate its significant improvements in tracking performance."
LMH-MOT : A Light Multiple Hypothesis Framework for 3D Multi-Object Tracking.,https://doi.org/10.1109/ICRA55743.2025.11127329,"3D multi-object tracking (3D MOT) is a key area in the field of autonomous driving. In systems that track by detection, the detection results of deep learning models will inevitably have FP(False Positives) and FN(False Nagatives), and detector always cannot continuously and accurately detect targets when facing obstacle occlusion and sensor blind spots. The task of 3D-MOT is to combine the discrete and disordered target detection results in time sequence into continuous and reliable tracks for use by downstream planning modules. At present, multi-target tracking algorithms in the field of autonomous driving are all based on single-hypothesis. In crowded scenarios, both false negatives (FN) and false positives (FP) significantly increase, making it difficult for single-hypothesis-based tracking algorithms to accurately output tracks. Towards this end, we propose LMH-MOT, a light multiple hypothesis framework for 3D MOT. Specifically, LMH-MOT effectively handles complex data association problems in autonomous driving scenarios by generating and maintaining multiple sets of hypotheses. Recognizing the possibility of switching between different motion states of the object, we use multiple motion models to more accurately estimate the motion state of the same object at the same time, and select the best estimation result for output. Additionally, we introduce a data association method based on decision trees, making full use of various features of the track and greatly reducing false matches and missing matches. In order to ensure the real-time performance of the entire algorithm framework, we also use gibbs sampling to significantly reduce the calculation time. On the NuScenes dataset, our proposed method achieves state-of-the-art performance with 76.2% AMOTA."
RISED: Accurate and Efficient RGB-Colorized Mapping Using Image Selection and Point Cloud Densification.,https://doi.org/10.1109/ICRA55743.2025.11127540,"Recent advances in robotics have underscored the critical role of colorized point clouds in enhancing environmental perception accuracy. However, conventional multisensor fusion Simultaneous Localization and Mapping (SLAM) systems typically employ all available images indiscriminately for point cloud colorization, resulting in suboptimal outcomes with blurred textures. Notably, achieving precise texture-togeometry alignment remains a challenge despite the availability of accurate pose estimation. This study introduces RISED, an advanced colorized mapping system that tackles this challenge from two perspectives: projection accuracy and distribution uniformity. For projection accuracy, we analyze the influence of camera poses on colorization and carefully select the optimal viewpoint to minimize errors. Regarding distribution uniformity, point cloud densification is applied to eliminate LiDAR scanning traces. Furthermore, a novel evaluation method is introduced to provide comprehensive assessment of colorized point clouds, filling a gap in this field. Experimental results show that our method outperforms traditional approaches in RGB-colorized mapping. Specifically, our method achieves notable improvements in projection accuracy (55.2 %), geometric accuracy (63.1 %), and surface coverage (30.8 %)."
Modeling Uncertainty in 3D Gaussian Splatting Through Continuous Semantic Splatting.,https://doi.org/10.1109/ICRA55743.2025.11128341,"In this paper, we present a novel algorithm for probabilistically updating and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although previous methods have introduced algorithms which learn to rasterize features in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which presents a challenge for safety-critical robotic applications. To address this gap, we propose a method which advances the literature of continuous semantic mapping from voxels to ellipsoids, combining the precise structure of 3D-GS with the ability to quantify uncertainty of probabilistic robotic maps. Given a set of images, our algorithm performs a probabilistic semantic update directly on the 3D ellipsoids to obtain an expectation and variance through the use of conjugate priors. We also propose a probabilistic rasterization which returns per-pixel segmentation predictions with quantifiable uncertainty. We compare our method with similar probabilistic voxel-based methods to verify our extension to 3D ellipsoids, and perform ablation studies on uncertainty quantification and temporal smoothing."
OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127312,"Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead."
SMART: Advancing Scalable Map Priors for Driving Topology Reasoning.,https://doi.org/10.1109/ICRA55743.2025.11127994,"Topology reasoning is crucial for autonomous driving as it enables comprehensive understanding of connec-tivity and relationships between lanes and traffic elements. While recent approaches have shown success in perceiving driving topology using vehicle-mounted sensors, their scalability is hindered by the reliance on training data captured by consistent sensor configurations. We identify that the key factor in scalable lane perception and topology reasoning is the elimination of this sensor-dependent feature. To address this, we propose SMART, a scalable solution that leverages easily available standard-definition (SD) and satellite maps to learn a map prior model, supervised by large-scale geo-referenced high-definition (HD) maps independent of sensor settings. Attributed to scaled training, SMART alone achieves superior offline lane topology understanding using only SD and satellite inputs. Extensive experiments further demonstrate that SMART can be seamlessly integrated into any online topology reasoning methods, yielding significant improvements of up to 28% on the OpenLane-V2 benchmark. Project page: https://jay-ye.github.io/smart."
DynORecon: Dynamic Object Reconstruction for Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128794,"This paper presents DynORecon, a Dynamic Object Reconstruction system that leverages the information provided by Dynamic SLAM to simultaneously generate a volumetric map of observed moving entities while estimating free space to support navigation. By capitalising on the motion estimations provided by Dynamic SLAM, DynORecon continuously refines the representation of dynamic objects to eliminate residual artefacts from past observations and incrementally reconstructs each object, seamlessly integrating new observations to capture previously unseen structures. Our system is highly efficient (~20 FPS) and produces accurate (~10 cm) object reconstructions using simulated and real-world outdoor datasets."
Ephemerality Meets Lidar-Based Lifelong Mapping.,https://doi.org/10.1109/ICRA55743.2025.11127618,"Lifelong mapping is crucial for the long-term deployment of robots in dynamic environments. In this paper, we present ELite, an ephemerality-aided LiDAR-based lifelong mapping framework which can seamlessly align multiple session data, remove dynamic objects, and update maps in an end-toend fashion. Map elements are typically classified as static or dynamic, but cases like parked cars indicate the need for more detailed categories than binary. Central to our approach is the probabilistic modeling of the world into two-stage ephemerality, which represent the transiency of points in the map within two different time scales. By leveraging the spatiotemporal context encoded in ephemeralities, ELite can accurately infer transient map elements, maintain a reliable up-to-date static map, and improve robustness in aligning the new data in a more finegrained manner. Extensive real-world experiments on long-term datasets demonstrate the robustness and effectiveness of our system. The source code is publicly available for the robotics community: https://github.com/dongjae0107/ELite."
ViViDex: Learning Vision-Based Dexterous Manipulation from Human Videos.,https://doi.org/10.1109/ICRA55743.2025.11127358,"In this work, we aim to learn a unified vision-based policy for multi-fingered robot hands to manipulate a variety of objects in diverse poses. Though prior work has shown benefits of using human videos for policy learning, performance gains have been limited by the noise in estimated trajectories. Moreover, reliance on privileged object information such as ground-truth object states further limits the applicability in realistic scenarios. To address these limitations, we propose a new framework ViViDex to improve vision-based policy learning from human videos. It first uses reinforcement learning with trajectory guided rewards to train state-based policies for each video, obtaining both visually natural and physically plausible trajectories from the video. We then rollout successful episodes from state-based policies and train a unified visual policy without using any privileged information. We propose coordinate transformation to further enhance the visual point cloud representation, and compare behavior cloning and diffusion policy for the visual policy training. Experiments both in simulation and on the real robot demonstrate that ViViDex outperforms state-of-theart approaches on three dexterous manipulation tasks. Project website: zerchen.github.io/projects/vividex.html."
Bridging the Human to Robot Dexterity Gap Through Object-Oriented Rewards.,https://doi.org/10.1109/ICRA55743.2025.11128690,"Training robots directly from human videos is an emerging area in robotics and computer vision. While there has been notable progress with two-fingered grippers, learning autonomous tasks without teleoperation remains a difficult problem for multi-fingered robot hands. A key reason for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand with a different morphology. In this work, we present HUDOR, a technique that enables online fine-tuning of the policy by constructing a reward function from the human video. Importantly, this reward function is built using object-oriented rewards derived from off-the-shelf point trackers, which allows for meaningful learning signals even when the robot hand is in the visual observation, while the human hand is used to construct the reward. Given a single video of human solving a task, such as gently opening a music box, HUDOR allows our four-fingered Allegro hand to learn this task with just an hour of online interaction. Our experiments across four tasks, show that HUDOR outperforms alternatives with an average of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$4 \times$</tex> improvement. Code and videos are available on our website https://object-rewards.github.io/."
Hand-Object Interaction Pretraining from Videos.,https://doi.org/10.1109/ICRA55743.2025.11127811,"We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: https://hgaurav2k.github.io/hop/."
SuperQ-GRASP: Superquadrics-Based Grasp Pose Estimation on Larger Objects for Mobile-Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127681,"Grasp planning and estimation have been a longstanding research problem in robotics, with two main approaches to find graspable poses on the objects: 1) geometric approach, which relies on 3D models of objects and the gripper to estimate valid grasp poses, and 2) data-driven, learning-based approach, with models trained to identify grasp poses from raw sensor observations. The latter assumes comprehensive geometric coverage during the training phase. However, the data-driven approach is typically biased toward tabletop scenarios and struggle to generalize to out-of-distribution scenarios with larger objects (e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single view of these larger objects is often incomplete and necessitates additional observations. In this paper, we take a geometric approach, leveraging advancements in object modeling (e.g. NeRF) to build an implicit model by taking RGB images from views around the target object. This model enables the extraction of explicit mesh model while also capturing the visual appearance from novel viewpoints that is useful for perception tasks like object detection and pose estimation. We further decompose the NeRFreconstructed 3D mesh into superquadrics (SQs) - parametric geometric primitives, each mapped to a set of precomputed grasp poses, allowing grasp composition on the target object based on these primitives. Our proposed pipeline overcomes the problems: a) noisy depth and incomplete view of the object, with a modeling step, and b) generalization to objects of any size. For more qualitative results, refer to the supplementary video and webpage https://rpm-lab-umn.github.io/superq-grasp-webpage/."
Collaborative Motion Planning for Multi-Manipulator Systems Through Reinforcement Learning and Dynamic Movement Primitives.,https://doi.org/10.1109/ICRA55743.2025.11127855,"Robotic tasks often require multiple manipulators to enhance task efficiency and speed, but this increases complexity in terms of collaboration, collision avoidance, and the expanded state-action space. To address these challenges, we propose a multi-level approach combining Reinforcement Learning (RL) and Dynamic Movement Primitives (DMP) to generate adaptive, real-time trajectories for new tasks in dynamic environments using a demonstration library. This method ensures collision-free trajectory generation and efficient collaborative motion planning. We validate the approach through experiments in the PyBullet simulation environment with UR5e robotic manipulators. Project Website: https://sites.google.com/virginia.edu/oncoldmp/home"
Compliance Control with Dynamic and Self-Sensing Hydraulic Artificial Muscles for Wearable Assistive Devices.,https://doi.org/10.1109/ICRA55743.2025.11128209,"While wearable robots that utilize intrinsically soft materials for actuation offer enhanced safety and biological compatibility, the challenges of sensing and control significantly affect their performance. The control problem in such systems is inherently complex, and the inclusion of 'softness' introduces additional nonlinearities, hysteresis, and uncertainties. Furthermore, the effectiveness of control strategies is highly dependent on sensor selection and integration, which presents its own challenges. Most robotic systems require separate sensors for control purposes. In this study, a new sensing and control scheme are introduced for soft wearable robots, leveraging the intrinsic soft-sensing capability of fluidic filament actuators without adding computational complexity. This method enables simultaneous sensing and actuation with <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{9 6 \%}$</tex> position accuracy, even under physical disturbances. This approach is demonstrated with a soft assistive device for elbow flexion/extension, achieving 70.5% tracking accuracy and a 0.09s response delay to human intention, ensuring the system provides minimal resistance when assistance is not needed, while delivering the required support when necessary."
Braided Artificial Muscle with Programmable Body Morphing and its Application to Elbow Joint Flexion.,https://doi.org/10.1109/ICRA55743.2025.11128260,"For pneumatic artificial muscles, it is always considered the more maximum contraction ratio the better. While for human joint assisting applications, PAMs with configurable maximum contraction rate are more suitable because of advantageous safety and adaptability. A PAM based on planar-to-specific-wave body shape morph is proposed in this work. Shape-morphing-based braided artificial muscles (SBAMs) have uniqueness of initial elasticity and maximum contraction ration programmability, which meet the favors of human joint assisting applications. The basic structure and working mechanism of contraction in SBAMs will be explained, and their mathematical model will also be established. According to the experimental results, a SBAM prototype generates a force more than 140 times its weight under an easily accessible pressure of 150 kPa. A mannequin wearing the SBAM enables actively flexes its elbow over 120 ."
Physics-Informed Hybrid Modeling of Pneumatic Artificial Muscles.,https://doi.org/10.1109/ICRA55743.2025.11127748,"Pneumatic Artificial Muscles (PAMs) are complex nonlinear systems characterized by hysteresis, making them challenging to model with classical system identification methods. While deep learning has emerged as a powerful tool for modeling nonlinear systems from data, purely neural networkbased models often lack interpretability and are prone to overfitting. To address these challenges, this study explores several hybrid approaches that combine analytical models with neural networks to model PAM behavior more effectively. The results demonstrate that hybrid models significantly outperform both purely analytical and black-box neural network models, particularly in terms of generalization and dynamic accuracy. Among the approaches, the Physics-Informed Neural Network (PINN) unsupervised model shows the most robust performance, capturing complex PAM dynamics while maintaining computational efficiency. These findings suggest that hybrid modeling is a promising and scalable solution for accurately representing the intricate behavior of PAMs."
Anisotropic Stiffness and Programmable Actuation for Soft Robots Enabled by an Inflated Rotational Joint.,https://doi.org/10.1109/ICRA55743.2025.11127236,"Soft robots are known for their ability to perform tasks with great adaptability, enabled by their distributed, non-uniform stiffness and actuation. Bending is the most fundamental motion for soft robot design, but creating robust, and easy-to-fabricate soft bending joint with tunable properties remains an active problem of research. In this work, we demonstrate an inflatable actuation module for soft robots with a defined bending plane enabled by forced partial wrinkling. This lowers the structural stiffness in the bending direction, with the final stiffness easily designed by the ratio of wrinkled and unwrinkled regions. We show the stiffness properties of the actuation module through a first-principle model validated by experimental characterization, and demonstrate the module's ability to maintain the kinematic constraint over a large range of loading conditions. We illustrate how these properties give the potential for complex actuation in a soft continuum robot and for decoupling actuation force and efficiency from load capacity. The module provides a novel method for embedding intelligent actuation into soft pneumatic robots."
Online Aggregation of Trajectory Predictors.,https://doi.org/10.1109/ICRA55743.2025.11127597,"Trajectory prediction, the task of forecasting future agent behavior from past data, is central to safe and efficient autonomous driving. A diverse set of methods (e.g., rule-based or learned with different architectures and datasets) have been proposed, yet it is often the case that the performance of these methods is sensitive to the deployment environment (e.g., how well the design rules model the environment, or how accurately the test data match the training data). Building upon the principled theory of online convex optimization but also going beyond convexity and stationarity, we present a lightweight and model-agnostic method to aggregate different trajectory predictors online. We propose treating each individual trajectory predictor as an expert and maintaining a probability vector to mix the outputs of different experts. Then, the key technical approach lies in leveraging online data - the true agent behavior to be revealed at the next timestepto form a convex-or-nonconvex, stationary-or-dynamic loss function whose gradient steers the probability vector towards choosing the best mixture of experts. We instantiate this method to aggregate trajectory predictors trained on different cities in the nuScenes dataset and show that it performs just as well, if not better than, any singular model, even when deployed on the out-of-distribution LYFT dataset."
Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward Modeling and Reinforcement Learning Fine-Tuning.,https://doi.org/10.1109/ICRA55743.2025.11127286,"Autonomous driving necessitates the ability to reason about future interactions between traffic agents and to make informed evaluations for planning. This paper introduces the Gen-Drive framework, which shifts from the traditional prediction and deterministic planning framework to a generation-then-evaluation planning paradigm. The framework employs a behavior diffusion model as a scene generator to produce diverse possible future scenarios, thereby enhancing the capability for joint interaction reasoning. To facilitate decision-making, we propose a scene evaluator (reward) model, trained with pairwise preference data collected through VLM assistance, thereby reducing human workload and enhancing scalability. Furthermore, we utilize an RL fine-tuning framework to improve the generation quality of the diffusion model, rendering it more effective for planning tasks. We conduct training and closed-loop planning tests on the nuPlan dataset, and the results demonstrate that employing such a generation-then-evaluation strategy outperforms other learning-based approaches. Additionally, the fine-tuned generative driving policy shows significant enhancements in planning performance. We further demonstrate that utilizing our learned reward model for evaluation or RL fine-tuning leads to better planning performance compared to relying on human-designed rewards. Project website: https://mczhi.github.io/GenDrive."
Optimizing Efficiency of Mixed Traffic Through Reinforcement Learning: A Topology-Independent Approach and Benchmark.,https://doi.org/10.1109/ICRA55743.2025.11127732,"This paper presents a mixed traffic control policy designed to optimize traffic efficiency across diverse road topologies, addressing issues of congestion prevalent in urban environments. A model-free reinforcement learning (RL) approach is developed to manage large-scale traffic flow, using data collected by autonomous vehicles to influence human-driven vehicles. A real-world mixed traffic control benchmark is also released, which includes 444 scenarios from 20 countries, representing a wide geographic distribution and covering a variety of scenarios and road topologies. This benchmark serves as a foundation for future research, providing a realistic simulation environment for the development of effective policies. Comprehensive experiments demonstrate the effectiveness and adaptability of the proposed method, achieving better performance than existing traffic control methods in both intersection and roundabout scenarios. To the best of our knowledge, this is the first project to introduce a real-world complex scenarios mixed traffic control benchmark. Videos and code of our work are available at https://sites.google.com/berkeley.edu/mixedtrafficplus/home"
Internal-Stably Energy-Saving Cooperative Control of Articulated Wheeled Robot with Distributed Drive Units.,https://doi.org/10.1109/ICRA55743.2025.11128843,"Articulated wheeled robots play a crucial role in the logistics industry. However, conventional tractor-driven articulated wheeled robots exhibit poor internal stability and are prone to jackknifing, while also consuming a significant amount of energy. By deploying distributed drives and coordinating control among multiple drives, these issues can be effectively addressed. However, the flexible connections between the bodies of articulated vehicles pose significant challenges to the coordinated control of distributed drives. This paper proposes a multi-drive unit coordinated control algorithm based on driving force equivalence and allocation. A neural network is used to predict the driving force, and through non-linear driving force equivalence, a feedforward driving force is obtained. This is combined with a closed-loop feedback compensation controller to form a control architecture that integrates feedforward and feedback, resulting in the equivalent total driving force for the vehicle queue. Subsequently, an equivalent distribution strategy allocates the required driving force to each drive, enabling the vehicle bodies to achieve accurate and stable speed tracking while allowing each drive to operate near its efficient operating point, thereby reducing total energy consumption. Experiments demonstrate that our algorithm significantly lowers the total energy consumption of the vehicle queue under standard operating conditions while ensuring speed-tracking accuracy and improving internal stability."
A System for Endoscopic Submucosal Dissection Featuring Concentric Push-Pull Manipulators.,https://doi.org/10.1109/ICRA55743.2025.11127293,"Endoscopic Submucosal Dissection (ESD) is an effective minimally invasive approach to removing colon cancer, yet it is underutilized, since it is challenging to learn and perform. To promote the adoption of ESD by making it easier, we propose a system in which two small, flexible robotic manipulators are delivered through a colonoscope. Our system differs from prior robotic systems aimed at this application in that our manipulators are small enough to fit through a clinically used colonoscope. By not re-engineering the colonoscope, we maintain overall system diameter at the current clinical gold standard, and streamline the path to eventual clinical deployment. Our concentric push-pull robot (CPPR) manipulators offer dexterity and simultaneously provide a conduit for grasper or cutting tool deployment. Each manipulator in our system consists of two push-pull tube pairs, and we describe how they are actuated. We describe for the first time our approach to compensating for undesirable CPPR tip motion induced by differences in the tubes' transmission stiffness. We also evaluate the workspace of the manipulators and demonstrate teleoperation in a point-touching experiment. Lastly, we demonstrate the ability of the system to resect tissue via ex vivo animal experiments."
Model-Free Safety Filter for Soft Robots: A Q-Learning Approach.,https://doi.org/10.1109/ICRA55743.2025.11128125,
Reachability Analysis for Black-Box Dynamical Systems.,https://doi.org/10.1109/ICRA55743.2025.11127528,"Hamilton-Jacobi (HJ) reachability analysis is a powerful framework for ensuring safety and performance in autonomous systems. However, existing methods typically rely on a white-box dynamics model of the system, limiting their applicability in many practical robotics scenarios where only a black-box model of the system is available. In this work, we propose a novel reachability method to compute reachable sets and safe controllers for black-box dynamical systems. Our approach efficiently approximates the Hamiltonian function using samples from the black-box dynamics. This Hamiltonian is then used to solve the HJ Partial Differential Equation (PDE), providing the reachable set of the system. The proposed method can be applied to general nonlinear systems and can be seamlessly integrated with existing reachability toolboxes for white-box systems to extend their use to black-box systems. Through simulation studies on a black-box slip-wheel car and a quadruped robot, we demonstrate the effectiveness of our approach in accurately obtaining the reachable sets for blackbox dynamical systems."
SAFE-GIL: SAFEty Guided Imitation Learning for Robotic Systems.,https://doi.org/10.1109/ICRA55743.2025.11128298,Behavior cloning (BC) is a widely used approach in imitation learning where a robot learns a control policy by observing an expert supervisor. However the learned policy can make errors and might lead to safety violations which limits their utility in safety-critical robotics applications. While prior works have tried improving a BC policy via additional real or synthetic action labels adversarial training or runtime filtering none of them explicitly focus on reducing the BC policy's safety violations during training time. We propose SAFE-GIL a design-time method to learn safety-aware behavior cloning policies. SAFE-GIL deliberately injects adversarial disturbance in the system during data collection to guide the expert towards safety-critical states. This disturbance injection simulates potential policy errors that the system might encounter during the test time. By ensuring that training more closely replicates expert behavior in safety-critical states our approach results in safer policies despite policy errors during the test time. We further develop a reachability-based method to compute this adversarial disturbance. We compare SAFE-GIL with various behavior cloning techniques and online safety-filtering methods in three domains autonomous ground navigation aircraft taxiing and aerial navigation on a quadrotor testbed. Our method demonstrates a significant reduction in safety failures particularly in low data regimes where the likelihood of learning errors and therefore safety violations is higher. See our website here: https://y-u-c.github.io/safegil/.
Guaranteed Reach-Avoid for Black-Box Systems through Narrow Gaps via Neural Network Reachability.,https://doi.org/10.1109/ICRA55743.2025.11127579,"In the classical reach-avoid problem, autonomous mobile robots are tasked to reach a goal while avoiding obstacles. However, it is difficult to provide guarantees on the robot's performance when the obstacles form a narrow gap and the robot is a black-box (i.e. the dynamics are not known analytically, but interacting with the system is cheap). To address this challenge, this paper presents NeuralPARC. The method extends the authors' prior Piecewise Affine Reach-avoid Computation (PARC) method to systems modeled by rectified linear unit (ReLU) neural networks, which are trained to represent parameterized trajectory data demonstrated by the robot. NeuralPARC computes the reachable set of the network while accounting for modeling error, and returns a set of states and parameters with which the black-box system is guaranteed to reach the goal and avoid obstacles. NeuralPARC is shown to outperform PARC, generating provably-safe extreme vehicle drift parking maneuvers in simulations and in real life on a model car, as well as enabling safety on an autonomous surface vehicle (ASV) subjected to large disturbances and controlled by a deep reinforcement learning (RL) policy."
RAIL: Reachability-Aided Imitation Learning for Safe Policy Execution.,https://doi.org/10.1109/ICRA55743.2025.11128656,"Imitation learning (IL) has shown great success in learning complex robot manipulation tasks. However, there remains a need for practical safety methods to justify widespread deployment. In particular, it is important to certify that a system obeys hard constraints on unsafe behavior in settings when it is unacceptable to design a tradeoff between performance and safety via tuning the policy (i.e. soft constraints). This leads to the question, how does enforcing hard constraints impact the performance (meaning safely completing tasks) of an IL policy? To answer this question, this paper builds a reach ability - based safety filter to enforce hard constraints on IL, which we call Reachability-Aided Imitation Learning (RAIL). Through evaluations with state-of-the-art IL policies in mobile robots and manipulation tasks, we make two key findings. First, the highest-performing policies are sometimes only so because they frequently violate constraints, and significantly lose performance under hard constraints. Second, surprisingly, hard constraints on the lower-performing policies can occasionally increase their ability to perform tasks safely. Finally, hardware evaluation confirms the method can operate in real time. More results can be found at our website: https://safe-robotics-lab-gt.github.io/rail/."
"Reference-Free Formula Drift with Reinforcement Learning: From Driving Data to Tire Energy-Inspired, Real-World Policies.",https://doi.org/10.1109/ICRA55743.2025.11128630,"The skill to drift a car-i.e., operate in a state of controlled oversteer like professional drivers-could give future autonomous cars maximum flexibility when they need to retain control in adverse conditions or avoid collisions. We investigate real-time drifting strategies that put the car where needed while bypassing expensive trajectory optimization. To this end, we design a reinforcement learning agent that builds on the concept of tire energy absorption to autonomously drift through changing and complex waypoint configurations while safely staying within track bounds. We achieve zero-shot deployment on the car by training the agent in a simulation environment built on top of a neural stochastic differential equation vehicle model learned from pre-collected driving data. Experiments on a Toyota GR Supra and Lexus LC 500 show that the agent is capable of drifting smoothly through varying waypoint configurations with tracking error as low as 10 cm while stably pushing the vehicles to sideslip angles of up to 63."
FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning.,https://doi.org/10.1109/ICRA55743.2025.11127934,"In recent years, the Robotics field has initiated several efforts toward building generalist robot policies through large-scale multi-task Behavior Cloning. However, direct deployments of these policies have led to unsatisfactory performance, where the policy struggles with unseen states and tasks. How can we break through the performance plateau of these models and elevate their capabilities to new heights? In this paper, we propose FLaRe, a large-scale Reinforcement Learning fine-tuning framework that integrates robust pre-trained representations, large-scale training, and gradient stabilization techniques. Our method aligns pre-trained policies towards task completion, achieving state-of-the-art (SoTA) performance both on previously demonstrated and on entirely novel tasks and embodiments. Specifically, on a set of long-horizon mobile manipulation tasks, FLaRe achieves an average success rate of 79.5% in unseen environments, with absolute improvements of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$+23.6 \%$</tex> in simulation and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$+30.7 \%$</tex> on real robots over prior SoTA methods. By utilizing only sparse rewards, our approach can enable generalizing to new capabilities beyond the pretraining data with minimal human effort. Moreover, we demonstrate rapid adaptation to new embodiments and behaviors with less than a day of fine-tuning. Videos, code, and appendix can be found on the project website at robot-flare.github.io"
Suite-IN: Aggregating Motion Features from Apple Suite for Robust Inertial Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127228,"With the rapid development of wearable technology, devices like smartphones, smartwatches, and headphones equipped with IMUs have become essential for applications such as pedestrian positioning. However, traditional pedestrian dead reckoning (PDR) methods struggle with diverse motion patterns, while recent data-driven approaches, though improving accuracy, often lack robustness due to reliance on a single device. In our work, we attempt to enhance the positioning performance using the low-cost commodity IMUs embedded in the wearable devices. We propose a multi-device deep learning framework named Suite-IN, aggregating motion data from Apple Suite for inertial navigation. Motion data captured by sensors on different body parts contains both local and global motion information, making it essential to reduce the negative effects of localized movements and extract global motion representations from multiple devices. Our model innovatively introduces a contrastive learning module to disentangle motionshared and motion-private latent representations, enhancing positioning accuracy. We validate our method on a self-collected dataset consisting of Apple Suite: iPhone, Apple Watch and Airpods, which supports a variety of movement patterns and flexible device configurations. Experimental results demonstrate that our approach outperforms state-of-the-art models while maintaining robustness across diverse sensor configurations."
Sample-Efficient Unsupervised Policy Cloning from Ensemble Self-Supervised Labeled Videos.,https://doi.org/10.1109/ICRA55743.2025.11127791,"Current advanced policy learning methodologies have demonstrated the ability to develop expert-level strategies when provided enough information. However, their requirements, including task-specific rewards, action-labeled expert trajectories, and huge environmental interactions, can be expensive or even unavailable in many scenarios. In contrast, humans can efficiently acquire skills within a few trials and errors by imitating easily accessible internet videos, in the absence of any other supervision. In this paper, we try to let machines replicate this efficient watching-and-learning process through Unsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a novel framework to efficiently learn policies from action-free videos without rewards and any other expert supervision. UPESV trains a video labeling model to infer the expert actions in expert videos through several organically combined self-supervised tasks. Each task performs its duties, and they together enable the model to make full use of both action-free videos and reward-free interactions for robust dynamics understanding and advanced action prediction. Simultaneously, UPESV clones a policy from the labeled expert videos, in turn collecting environmental interactions for self-supervised tasks. After a sample-efficient, unsupervised, and iterative training process, UPESV obtains an advanced policy based on a robust video labeling model. Extensive experiments in sixteen challenging procedurally generated environments demonstrate that the proposed UPESV achieves state-of-the-art interaction-limited policy learning performance (outperforming five current advanced baselines on 12/16 tasks) without exposure to any other supervision except for videos."
Privileged-Dreamer: Explicit Imagination of Privileged Information for Rapid Adaptation of Learned Policies.,https://doi.org/10.1109/ICRA55743.2025.11127672,"Numerous real-world control problems involve dynamics and objectives affected by unobservable hidden parameters, ranging from autonomous driving to robotic manipulation, which cause performance degradation during sim-to-real transfer. To represent these kinds of domains, we adopt hiddenparameter Markov decision processes (HIP-MDPs), which model sequential decision problems where hidden variables parameterize transition and reward functions. Existing approaches, such as domain randomization, domain adaptation, and meta-learning, simply treat the effect of hidden parameters as additional variance and often struggle to effectively handle HIP-MDP problems, especially when the rewards are parameterized by hidden variables. We introduce PrivilegedDreamer, a model-based reinforcement learning framework that extends the existing model-based approach by incorporating an explicit parameter estimation module. PrivilegedDreamer features its novel dual recurrent architecture that explicitly estimates hidden parameters from limited historical data and enables us to condition the model, actor, and critic networks on these estimated parameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates that PrivilegedDreamer outperforms state-of-the-art model-based, model-free, and domain adaptation learning algorithms. Additionally, we conduct ablation studies to justify the inclusion of each component in the proposed architecture."
Dynamic Non-Prehensile Object Transport via Model-Predictive Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127521,"We investigate the problem of teaching a robot manipulator to perform dynamic non-prehensile object transport, also known as the robot waiter task, from a limited set of real-world demonstrations. We propose an approach that combines batch reinforcement learning (RL) with modelpredictive control (MPC) by pretraining an ensemble of value functions from demonstration data, and utilizing them online within an uncertainty-aware MPC scheme to ensure robustness to limited data coverage. Our approach is straightforward to integrate with off-the-shelf MPC frameworks and enables learning solely from task space demonstrations with sparsely labeled transitions, while leveraging MPC to ensure smooth joint space motions and constraint satisfaction. We validate the proposed approach through extensive simulated and real-world experiments on a Franka Panda robot performing the robot waiter task and demonstrate robust deployment of value functions learned from 50  100 demonstrations. Furthermore, our approach enables generalization to novel objects not seen during training and can improve upon suboptimal demonstrations. We believe that such a framework can reduce the burden of providing extensive demonstrations and facilitate rapid training of robot manipulators to perform non-prehensile manipulation tasks. Project videos and supplementary material can be found at: https://sites.google.com/view/cvmpc"
Feature Extractor or Decision Maker: Rethinking the Role of Visual Encoders in Visuomotor Policies.,https://doi.org/10.1109/ICRA55743.2025.11127332,"An end-to-end (E2E) visuomotor policy is typically treated as a unified whole, but recent approaches using out-of-domain (OOD) data to pretrain the visual encoder have cleanly separated the visual encoder from the network, with the remainder referred to as the policy. We propose Visual Alignment Testing, an experimental framework designed to evaluate the validity of this functional separation. Our results indicate that in E2E-trained models, visual encoders actively contribute to decision-making resulting from motor data supervision, contradicting the assumed functional separation. In contrast, OOD-pretrained models, where encoders lack this capability, experience an average performance drop of 42% in our benchmark results, compared to the state-of-the-art performance achieved by E2E policies. We believe this initial exploration of visual encoders' role can provide a first step towards guiding future pretraining methods to address their decision-making ability, such as developing task-conditioned or context-aware encoders."
JRN-Geo: A Joint Perception Network Based on RGB and Normal Images for Cross-View Geo-Localization.,https://doi.org/10.1109/ICRA55743.2025.11127591,"Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle (UAV) localization and navigation. However, significant challenges arise from the drastic viewpoint differences and appearance variations between images. Existing methods predominantly rely on semantic features from RGB images, often neglecting the importance of spatial structural information in capturing viewpoint-invariant features. To address this issue, we incorporate geometric structural information from normal images and introduce a Joint perception network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a dual-branch feature extraction framework, leveraging a Difference-Aware Fusion Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to enable deep fusion and joint-constrained semantic and structural information representation. Furthermore, we propose a 3D geographic augmentation technique to generate potential viewpoint variation samples, enhancing the network's ability to learn viewpoint-invariant features. Extensive experiments on the University-1652 and SUES-200 datasets validate the robustness of our method against complex viewpoint variations, achieving state-of-the-art performance."
COLA: Characterizing and Optimizing the Tail Latency for Safe Level-4 Autonomous Vehicle Systems.,https://doi.org/10.1109/ICRA55743.2025.11127679,"Autonomous vehicles (AVs) systems are envisioned to revolutionize our life by providing safe, relaxing, and convenient ground transportation. To ensure safety, AV systems need to make timely driving decisions in response to complicated and highly dynamic real-world driving environments. We present a systematic study to understand the causes of tail latency in AV systems and their impact on safety. We empirically analyze the design of two open-source industrial AV systems, Baidu Apollo and Autoware. We explore how pipelined computation design (such as module dependency and execution patterns), traffic factors (surrounding environments of AV), and system factors (such as cache contention) impact AV systems' tail latency. Inspired by the insights, We propose a set of systematic designs that lead to performance and safety improvements of up to <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1.65 \times$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$14 \times$</tex>, respectively."
MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-Target Domain While Preserving Performance on Real-Source Domain.,https://doi.org/10.1109/ICRA55743.2025.11127978,"Deep neural network (DNN) based perception models are indispensable in the development of autonomous vehicles (AVs). However, their reliance on large-scale, high-quality data is broadly recognized as a burdensome necessity due to the substantial cost of data acquisition and labeling. Further, the issue is not a one-time concern as AVs might need a new dataset if they are to be deployed to another region (real-target domain) that the in-hand dataset within the real-source domain cannot incorporate. To mitigate this burden, we propose leveraging synthetic environments as an auxiliary domain where the characteristics of real domains are reproduced. This approach could enable indirect experience about the real-target domain in a time- and cost-effective manner. As a practical demonstration of our methodology, nuScenes and South Korea are employed to represent real-source and real-target domains, respectively. That means we construct digital twins for several regions of South Korea, and the data-acquisition framework of nuScenes is reproduced. Blending the aforementioned components within a simulator allows us to obtain a synthetic-fusion domain in which we forge our novel driving dataset, MORDA: Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation. To verify the value of synthetic features that MORDA provides in learning about driving environments of South Korea, 2D/3D detectors are trained solely on a combination of nuScenes and MORDA. Afterward, their performance is evaluated on the unforeseen real-world dataset (AI-Hub <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>This research (paper) used datasets from High-precision data collection vehicle daytime city road data. All data information can be accessed through AI-Hub (http://www.aihub.or.kr).) collected in South Korea. Our experiments present that MORDA can significantly improve mean Average Precision (mAP) on AI-Hub dataset while that on nuScenes is retained or slightly enhanced. Details on MORDA can be accessed at https://morda-e8d07e.gitlab.io."
Towards Latency-Aware 3D Streaming Perception for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128729,"Although existing 3D perception algorithms have demonstrated significant improvements in performance, their deployment on edge devices continues to encounter critical challenges due to substantial runtime latency. We propose a new benchmark tailored for online evaluation by considering runtime latency. Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP) framework that addresses the latency issue through two primary components: 1) latency-aware history integration, which extends query propagation into a continuous process, ensuring the integration of historical feature regardless of varying latency; 2) latency-aware predictive detection, a module that compensates the detection results with the predicted trajectory and the posterior accessed latency. By incorporating the latency-aware mechanism, our method shows generalization across various latency levels, achieving an online performance that closely aligns with 80% of its offline evaluation on the Jetson AGX Orin without any acceleration techniques."
DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127884,"Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we introduce DRIVE  Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models. Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decisionmaking processes. We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output. These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations. Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models. Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models. These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications. We can only see a short distance ahead, but we can see plenty there that needs to be done.  Alan Turing"
Dur360BEV: A Real-World 360-Degree Single Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128609,"We present Dur360BEV, a novel spherical camera autonomous driving dataset equipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS system, along with a benchmark architecture designed to generate Bird-Eye-View (BEV) maps using only a single spherical camera. This dataset and benchmark address the challenges of BEV generation in autonomous driving, particularly by reducing hardware complexity through the use of a single 360-degree camera instead of multiple perspective cameras. Within our benchmark architecture, we propose a novel spherical-image-to-BEV module that leverages spherical imagery and a refined sampling strategy to project features from 2D to 3D. Our approach also includes an innovative application of focal loss, specifically adapted to address the extreme class imbalance often encountered in BEV segmentation tasks, that demonstrates improved segmentation performance on the Dur360BEV dataset. The results show that our benchmark not only simplifies the sensor setup but also achieves competitive performance. Code + Dataset: https://github.com/Tom-E-DurhamJDur360BEV"
MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual Cues.,https://doi.org/10.1109/ICRA55743.2025.11128337,"3D single object tracking is essential in autonomous driving and robotics. Existing methods often struggle with sparse and incomplete point cloud scenarios. To address these limitations, we propose a Multimodal-guided Virtual Cues Projection (MVCP) scheme that generates virtual cues to enrich sparse point clouds. Additionally, we introduce an enhanced tracker MVCTrack based on the generated virtual cues. Specifically, the MVCP scheme seamlessly integrates RGB sensors into LiDAR-based systems, leveraging a set of 2D detections to create dense 3D virtual cues that significantly improve the sparsity of point clouds. These virtual cues can naturally integrate with existing LiDAR-based 3D trackers, yielding substantial performance gains. Extensive experiments demonstrate that our method achieves competitive performance on the NuScenes dataset. Code is available at code and video."
Chameleon: Fast-Slow Neuro-Symbolic Lane Topology Extraction.,https://doi.org/10.1109/ICRA55743.2025.11127684,"Lane topology extraction involves detecting lanes and traffic elements and determining their relationships, a key perception task for mapless autonomous driving. This task requires complex reasoning, such as determining whether it is possible to turn left into a specific lane. To address this challenge, we introduce neuro-symbolic methods powered by vision-language foundation models (VLMs). Existing approaches have notable limitations: (1) Dense visual prompting with VLMs can achieve strong performance but is costly in terms of both financial resources and carbon footprint, making it impractical for robotics applications. (2) Neuro-symbolic reasoning methods for 3D scene understanding fail to integrate visual inputs when synthesizing programs, making them ineffective in handling complex corner cases. To this end, we propose a fast-slow neuro-symbolic lane topology extraction algorithm, named Chameleon, which alternates between a fast system that directly reasons over detected instances using synthesized programs and a slow system that utilizes a VLM with a chain-of-thought design to handle corner cases. Chameleon leverages the strengths of both approaches, providing an affordable solution while maintaining high performance. We evaluate the method on the OpenLane-V2 dataset, showing consistent improvements across various baseline detectors. Our code, data, and models are publicly available at https://github.com/XR-Lee/neural-symbolic"
ERetinex: Event Camera Meets Retinex Theory for Low-Light Image Enhancement.,https://doi.org/10.1109/ICRA55743.2025.11127340,"Low-light image enhancement aims to restore the under-exposure image captured in dark scenarios. Under such scenarios, traditional frame-based cameras may fail to capture the structure and color information due to the exposure time limitation. Event cameras are bio-inspired vision sensors that respond to pixel-wise brightness changes asynchronously. Event cameras' high dynamic range is pivotal for visual perception in extreme low-light scenarios, surpassing traditional cameras and enabling applications in challenging dark environments. In this paper, inspired by the success of the retinex theory for traditional frame-based low-light image restoration, we introduce the first methods that combine the retinex theory with event cameras and propose a novel retinex-based lowlight image restoration framework named ERetinex. Among our contributions, the first is developing a new approach that leverages the high temporal resolution data from event cameras with traditional image information to estimate scene illumination accurately. This method outperforms traditional image-only techniques, especially in low-light environments, by providing more precise lighting information. Additionally, we propose an effective fusion strategy that combines the high dynamic range data from event cameras with the color information of traditional images to enhance image quality. Through this fusion, we can generate clearer and more detailrich images, maintaining the integrity of visual information even under extreme lighting conditions. The experimental results indicate that our proposed method outperforms state-of-theart (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while reducing FLOPS by 84.28 %. The code is available at https://github.com/lodew920/ERetinex."
ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-Based Refinement.,https://doi.org/10.1109/ICRA55743.2025.11127815,"We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots. Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map. To refine this map, we design a novel channel and spatial attention module. Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands. Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments. Our code will be released on https://github.com/SJTU-ViSYS-team/ThermoStereoRT."
Tool-Mediated Robot Perception of Granular Substances Using Multiple Sensory Modalities.,https://doi.org/10.1109/ICRA55743.2025.11127366,"People use tools to interact with and perceive the world, with multimodal sensory inputs forming the basis of how we understand our environment. For example, a blind person uses a walking cane to tap the road and detect obstacles, and a builder uses a hammer to strike a wall to assess its structural integrity. Using tools extends our sensory capabilities during exploratory behaviors, enabling us to perceive object properties that are otherwise inaccessible. Inspired by this cognitive process, we propose a framework in which a multisensory robot employs exploratory behaviors using various tools to recognize granular substances. Our framework effectively integrates multiple non-visual sensory inputs (e.g., audio, haptic, and tactile) gathered through multiple tools (e.g., spoon, fork) and behaviors (e.g., stirring, poking) to perceive object properties. The framework segments interactions into time windows and aligns different modalities, enhancing data efficiency and interactive perception. Additionally, we conducted tool-transfer experiments to evaluate similarities between tools. Our experiments demonstrate that combining multiple tools and behaviors outperforms single-tool and singlebehavior approaches. While the audio modality dominates the non-visual multimodal system, other modalities contribute. We further demonstrate that tool similarities vary depending on the behavior, and notably, the robot does not need to complete entire interactions to achieve optimal recognition accuracy."
FisheyeDepth: A Real Scale Self-Supervised Depth Estimation Model for Fisheye Camera.,https://doi.org/10.1109/ICRA55743.2025.11127840,"Accurate depth estimation is crucial for 3D scene comprehension in robotics and autonomous vehicles. Fisheye cameras, known for their wide field of view, have inherent geometric benefits. However, their use in depth estimation is restricted by a scarcity of ground truth data and image distortions. We present FisheyeDepth, a self-supervised depth estimation model tailored for fisheye cameras. We incorporate a fisheye camera model into the projection and reprojection stages during training to handle image distortions, thereby improving depth estimation accuracy and training stability. Furthermore, we incorporate real-scale pose information into the geometric projection between consecutive frames, replacing the poses estimated by the conventional pose network. Essentially, this method offers the necessary physical depth for robotic tasks, and also streamlines the training and inference procedures. Additionally, we devise a multi-channel output strategy to improve robustness by adaptively fusing features at various scales, which reduces the noise from real pose data. We demonstrate the superior performance and robustness of our model in fisheye image depth estimation through evaluations on public datasets and real-world scenarios. The project website is available at: https://github.com/guoyangzhaolFisheyeDepth."
Geometry-Aware Volumetric Data Stitching Using Local Surface Mapping and Robot Optical Coherence Tomography.,https://doi.org/10.1109/ICRA55743.2025.11128184,"Optical coherence tomography (OCT) has been widely used for high-fidelity biological tissue scanning but is traditionally limited to small lateral fields of view that preclude large-area scanning. To overcome this problem, we propose an integration of an OCT sensor to a 6-DOF robot arm end-effector combined with a geometry-aware stitching model for surface and volumetric data stitching. We firstly develop a simple but efficient Robot-OCT calibration method by using a three-marker calibration pattern and implement an optimization solver. Given a pre-defined trajectory, a local planner is developed to update the sensor pose by using the OCT point cloud information in order to maintain the effective imaging depth based on the distance and orientation constraints. The system calibration method is verified through repeated experiments with the three-marker targets and the result shows an average testing error of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$0.132 \pm 0.071 ~\text{mm}$</tex>. The geometry-aware OCT stitching framework is demonstrated based on the experiments of different scanning trajectories and 3D-printed phantoms for large-area scanning. The OCT stitched point cloud is compared with the ground truth from the phantom CAD model and the result show an average surface alignment error of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$0.441 \pm 0.241 ~\text{mm}$</tex> for the path following tasks."
MAC-VO: Metrics-Aware Covariance for Learning-Based Stereo Visual Odometry mac-vo.github.io.,https://doi.org/10.1109/ICRA55743.2025.11128482,"We propose MAC-VO, a novel learning-based stereo visual odometry (VO) framework that trains a metrics-aware uncertainty model to serve two critical functions: selecting keypoints and weighting residuals in pose graph optimization. Unlike traditional geometric methods that favor texture-rich features like edges, our keypoint selector leverages this learned uncertainty model to eliminate low-quality features based on global inconsistency. In contrast to learning-based approaches that rely on scale-agnostic weight matrices for covariance, our metrics-aware covariance modelderived from the learned uncertaintycaptures spatial errors in keypoint registration and inter-axis correlations. By embedding this co-variance model into pose graph optimization, MAC-VO achieves superior robustness and accuracy in pose estimation, excelling in challenging environments with varying illumination, feature density, and motion patterns. Evaluations on public benchmark datasets demonstrate that MAC-VO surpasses existing VO algorithms and even some SLAM systems in difficult scenarios. Additionally, the uncertainty map offers valuable insights for decision-making."
Ground-Optimized 4D Radar-Inertial Odometry Via Continuous Velocity Integration Using Gaussian Process.,https://doi.org/10.1109/ICRA55743.2025.11127572,"Radar ensures robust sensing capabilities in adverse weather conditions, yet challenges remain due to its high inherent noise level. Existing radar odometry has overcome these challenges with strategies such as filtering spurious points, exploiting Doppler velocity, or integrating with inertial measurements. This paper presents two novel improvements beyond the existing radar-inertial odometry: ground-optimized noise filtering and continuous velocity preintegration. Despite the widespread use of ground planes in LiDAR odometry, imprecise ground point distributions of radar measurements cause naive plane fitting to fail. Unlike plane fitting in LiDAR, we introduce a zone-based uncertainty-aware ground modeling specifically designed for radar. Secondly, we note that radar velocity measurements can be better combined with IMU for a more accurate preintegration in radar-inertial odometry. Existing methods often ignore temporal discrepancies between radar and IMU by simplifying the complexities of asynchronous data streams with discretized propagation models. Tackling this issue, we leverage GP and formulate a continuous preintegration method for tightly integrating 3-DOF linear velocity with IMU, facilitating full 6-DOF motion directly from the raw measurements. Our approach demonstrates remarkable performance (less than 1 % vertical drift) in public datasets with meticulous conditions, illustrating substantial improvement in elevation accuracy. The code will be released as open source for the community: https://github.com/wooseongY/Go-RIO."
UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128868,"Understanding fine-grained object affordances is imperative for robots to manipulate objects in unstructured environments given open-ended task instructions. However, existing methods of visual affordance predictions often rely on manually annotated data or conditions only on a predefined set of tasks. We introduce Unsupervised Affordance Distillation (UAD), a method for distilling affordance knowledge from foundation models into a task-conditioned affordance model without any manual annotations. By leveraging the complementary strengths of large vision models and vision-language models, UAD automatically annotates a large-scale dataset with detailed <instruction, visual affordance> pairs. Training only a lightweight task-conditioned decoder atop frozen features, UAD exhibits notable generalization to in-the-wild robotic scenes and to various human activities, despite only being trained on rendered objects in simulation. Using affordance provided by UAD as the observation space, we show an imitation learning policy that demonstrates promising generalization to unseen object instances, object categories, and even variations in task instructions after training on as few as 10 demonstrations. Project website with Appendix: unsup-affordance.github.io/."
Bat-VUFN: Bat-Inspired Visual-and-Ultrasound Fusion Network for Robust Perception in Adverse Conditions.,https://doi.org/10.1109/ICRA55743.2025.11128308,"Environmental factors like weather and road conditions significantly impact object recognition in autonomous vehicles. While cameras provide rich semantic information, their reliance on electromagnetic waves makes them vulnerable to performance degradation in adverse conditions such as low light and rain. In contrast, ultrasonic sensors offer reliable short-range detection, unaffected by such conditions. We introduce Bat-VUFN, a bio-inspired multi-sensory system that merges camera and ultrasonic data using an Input Quality Score (IQS)-based fusion technique to enhance near-field perception in challenging environments. Bat-VUFN dynamically adjusts sensor contributions based on prevailing conditions, achieving impressive results on the K-Bat dataset (average precision: 0.95, MAE: 0.52m, RMSE: 0.55m), demonstrating its robustness in adverse scenarios."
TinySense: A Lighter Weight and More Power-Efficient Avionics System for Flying Insect-Scale Robots.,https://doi.org/10.1109/ICRA55743.2025.11128223,"In this paper, we introduce advances in the sensor suite of an autonomous flying insect robot (FIR) weighing less than a gram. FIRs, because of their small weight and size, offer unparalleled advantages in terms of material cost and scalability. However, their size introduces considerable control challenges, notably high-speed dynamics, restricted power, and limited payload capacity. While there have been advancements in developing lightweight sensors, often drawing inspiration from biological systems, no sub-gram aircraft has been able to attain sustained hover without relying on feedback from external sensing such as a motion capture system. The lightest vehicle capable of sustained hovering-the first level of sensor autonomy-is the much larger 28 g Crazyflie. Previous work reported a reduction in size of that vehicle's avionics suite to 187 mg and 21 mW. Here, we report a further reduction in mass and power to only 78.4 mg and 15 mW. We replaced the laser rangefinder with a lighter and more efficient pressure sensor, and built a smaller optic flow sensor around a global-shutter imaging chip. A Kalman Filter (KF) fuses these measurements to estimate the state variables that are needed to control hover: pitch angle, translational velocity, and altitude. Our system achieved performance comparable to that of the Crazyflie's estimator while in flight, with root mean squared errors of 1.573 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\text{deg}, 0.186 \mathrm{m} / \mathrm{s}$</tex>, and 0.136 m, respectively, relative to motion capture."
TSCLIP: Robust CLIP Fine-Tuning for Worldwide Cross-Regional Traffic Sign Recognition.,https://doi.org/10.1109/ICRA55743.2025.11127563,"Traffic sign is a critical map feature for navigation and traffic control. Nevertheless, current methods for traffic sign recognition rely on traditional deep learning models, which typically suffer from significant performance degradation considering the variations in data distribution across different regions. In this paper, we propose TSCLIP, a robust fine-tuning approach with the contrastive language-image pre-training (CLIP) model for worldwide cross-regional traffic sign recognition. We first curate a cross-regional traffic sign benchmark dataset by combining data from ten different sources. Then, we propose a prompt engineering scheme tailored to the characteristics of traffic signs, which involves specific scene descriptions and corresponding rules to generate targeted text descriptions. During the TSCLIP fine-tuning process, we implement adaptive dynamic weight ensembling (ADWE) to seamlessly incorporate outcomes from each training iteration with the zero-shot CLIP model. This approach ensures that the model retains its ability to generalize while acquiring new knowledge about traffic signs. To the best knowledge of authors, TSCLIP is the first contrastive language-image model used for the worldwide cross-regional traffic sign recognition task. The project website is available at: https://github.com/guoyangzhao/TSCLIP."
OPPA: Online Planner's Parameter Adaptation for Enhanced Mobile Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128768,"Autonomous navigation in mobile robots has made significant advancements; however, traditional methods often struggle to adapt in real-time to dynamic or unstructured environments. This paper presents the Online Planner's Parameter Adaptation (OPPA) framework, which enhances both adaptability and safety in mobile robot navigation by dynamically adjusting planner parameters. OPPA integrates a rule-based system for estimating tunnel width using 2D LiDAR and path data with a learning-based approach utilizing a shallow transformer model. By incorporating a human-in-the-loop process to refine training data, OPPA improves accuracy and reliability in complex environments. Designed for real-time efficiency on resource-constrained platforms, OPPA has been validated through simulation and real-world experiments, demonstrating its ability to enhance both safety and performance. These results highlight OPPA as a viable solution for dynamic and complex robotic applications."
Learning to Refine Input Constrained Control Barrier Functions via Uncertainty-Aware Online Parameter Adaptation.,https://doi.org/10.1109/ICRA55743.2025.11128840,"Control Barrier Functions (CBFs) have become powerful tools for ensuring safety in nonlinear systems. How-ever, finding valid CBFs that guarantee persistent safety and feasibility remains an open challenge, especially in systems with input constraints. Traditional approaches often rely on manually tuning the parameters of the class <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$K$</tex> functions of the CBF conditions a priori. The performance of CBF-based controllers is highly sensitive to these fixed parameters, potentially leading to overly conservative behavior or safety violations. To overcome these issues, this paper introduces a learning-based optimal control framework for online adaptation of Input Constrained CBF (ICCBF) parameters in discrete-time nonlinear systems. Our method employs a probabilistic ensemble neural network to predict the performance and risk metrics, as defined in this work, for candidate parameters, accounting for both epistemic and aleatoric uncertainties. We propose a two-step verification process using Jensen-Rnyi Divergence and distributionally-robust Conditional Value at Risk to identify valid parameters. This enables dynamic re-finement of ICCBF parameters based on current state and nearby environments, optimizing performance while ensuring safety within the verified parameter set. Experimental results demonstrate that our method outperforms both fixed-parameter and existing adaptive methods in robot navigation scenarios across safety and performance metrics. [Project Page]<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>Project page: https://www.taekyung.me/online-adaptive-cbf [Code] [Video]"
GA-TEB: Goal-Adaptive Framework for Efficient Navigation Based on Goal Lines.,https://doi.org/10.1109/ICRA55743.2025.11128136,"In crowd navigation, the local goal plays a crucial role in trajectory initialization, optimization, and evaluation. Recognizing that when the global goal is distant, the robot's primary objective is avoiding collisions, making it less critical to pass through the exact local goal point, this work introduces the concept of goal lines, which extend the traditional local goal from a single point to multiple candidate lines. Coupled with a topological map construction strategy that groups obstacles to be as convex as possible, a goal-adaptive navigation framework is proposed to efficiently plan multiple candidate trajectories. Simulations and experiments demonstrate that the proposed GA-TEB framework effectively prevents deadlock situations, where the robot becomes frozen due to a lack of feasible trajectories in crowded environments. Additionally, the framework greatly increases planning frequency in scenarios with numerous non-convex obstacles, enhancing both robustness and safety."
Reinforcement Learning for Adaptive Planner Parameter Tuning: A Perspective on Hierarchical Architecture.,https://doi.org/10.1109/ICRA55743.2025.11128541,"Automatic parameter tuning methods for planning algorithms, which integrate pipeline approaches with learning-based techniques, are regarded as promising due to their stability and capability to handle highly constrained environments. While existing parameter tuning methods have demonstrated considerable success, further performance improvements require a more structured approach. In this paper, we propose a hierarchical architecture for reinforcement learning-based parameter tuning. The architecture introduces a hierarchical structure with low-frequency parameter tuning, mid-frequency planning, and high-frequency control, enabling concurrent enhancement of both upper-layer parameter tuning and lower-layer control through iterative training. Experimental evaluations in both simulated and real-world environments show that our method surpasses existing parameter tuning approaches. Furthermore, our approach achieves first place in the Benchmark for Autonomous Robot Navigation (BARN) Challenge."
Forward Invariance in Trajectory Spaces for Safety-Critical Control.,https://doi.org/10.1109/ICRA55743.2025.11127715,"Useful robot control algorithms should not only achieve performance objectives but also adhere to hard safety constraints. Control Barrier Functions (CBFs) have been developed to provably ensure system safety through forward invariance. However, they often unnecessarily sacrifice performance for safety since they are purely reactive. Receding horizon control (RHC), on the other hand, consider planned trajectories to account for the future evolution of a system. This work provides a new perspective on safety-critical control by introducing Forward Invariance in Trajectory Spaces (FITS). We lift the problem of safe RHC into the trajectory space and describe the evolution of planned trajectories as a controlled dynamical system. Safety constraints defined over states can be converted into sets in the trajectory space which we render forward invariant via a CBF framework. We derive an efficient quadratic program (QP) to synthesize trajectories that provably satisfy safety constraints. Our experiments support that FITS improves the adherence to safety specifications without sacrificing performance over alternative CBF and NMPC methods."
Scalable Multi-Robot Task Allocation and Coordination Under Signal Temporal Logic Specifications.,https://doi.org/10.1109/ICRA55743.2025.11128245,"Motion planning with simple objectives, such as collision-avoidance and goal-reaching, can be solved efficiently using modern planners. However, the complexity of the allowed tasks for these planners is limited. On the other hand, signal temporal logic (STL) can specify complex requirements, but STL-based motion planning and control algorithms often face scalability issues, especially in large multi-robot systems with complex dynamics. In this paper, we propose an algorithm that leverages the best of the two worlds. We first use a single-robot motion planner to efficiently generate a set of alternative reference paths for each robot. Then coordination requirements are specified using STL, which is defined over the assignment of paths and robots' progress along those paths. We use a Mixed Integer Linear Program (MILP) to compute task assignments and robot progress targets over time such that the STL specification is satisfied. Finally, a local controller is used to track the target progress. Simulations demonstrate that our method can handle tasks with complex constraints and scales to large multi-robot teams and intricate task allocation scenarios."
Planning with Linear Temporal Logic Specifications: Handling Quantifiable and Unquantifiable Uncertainty.,https://doi.org/10.1109/ICRA55743.2025.11127333,"This work studies the planning problem for robotic systems under both quantifiable and unquantifiable uncertainty. The objective is to enable the robotic systems to optimally fulfill high-level tasks specified by Linear Temporal Logic (LTL) formulas. To capture both types of uncertainty in a unified modelling framework, we utilise Markov Decision Processes with Set-valued Transitions (MDPSTs). We introduce a novel solution technique for optimal robust strategy synthesis of MDPSTs with LTL specifications. To improve efficiency, our work leverages limit-deterministic Bchi automata (LDBAs) as the automaton representation for LTL to take advantage of their efficient constructions. To tackle the inherent nondeterminism in MDPSTs, which presents a significant challenge for reducing the LTL planning problem to a reachability problem, we introduce the concept of a Winning Region (WR) for MDPSTs. Additionally, we propose an algorithm for computing the WR over the product of the MDPST and the LDBA. Finally, a robust value iteration algorithm is invoked to solve the reachability problem. We validate the effectiveness of our approach through a case study involving a mobile robot operating in the hexagonal world, demonstrating promising efficiency gains."
Lyapunov-Certified Trajectory Tracking for Mobile Robot With a Tail Wheel: Differential-Flatness and Adaptive Backstepping Design.,https://doi.org/10.1109/ICRA55743.2025.11127405,"This paper proposes a trajectory tracking control law for a mobile robot with two front differential wheels and a tail wheel. The dynamics is given by mimicking Ackerman steering model for the dynamics of position and orientation, associated with the actuator dynamics of the tail wheel's angle modeled by a first-order response with respect to the robot's angular velocity. First we develop a nominal trajectory tracking control law to track a given desired trajectory by applying differential-flatness property of the unicycle model and backstepping approach to handle the actuator dynamics. The effectiveness of the trajectory tracking is demonstrated by conducting hardware robot experiment after performing system identification, which illustrates the superior performance over a benchmark method. The design is also extended to an adaptive tracking control under parameter uncertainty in the tail wheel dynamics through introducing the adaptation law of the parameters, and the performance is demonstrated in numerical simulation."
OPRNet: Object-Centric Point Reconstruction Network for Multimodal 3D Object Detection in Adverse Weathers.,https://doi.org/10.1109/ICRA55743.2025.11128044,"The development of a multimodal fusion technique utilizing LiDAR-camera data has enabled precise 3D object detection for self-driving vehicles, particularly in ideal conditions with clear weather. Nevertheless, adverse weathers such as fog, snow, and rain remain a challenge for existing multimodal methods. These conditions lead to a reduced density of point clouds as a result of laser signal occlusion and attenuation. Additionally, as the distance grows, the point cloud becomes sparser, further challenging object detection tasks. To address these problems, we introduce a point reconstruction network employing equirectangular projection tailored for multimodal 3D object detection. This network incorporates a range-constrained noise filter to remove noise caused by adverse weather and an object-centric point generator designed to flexibly generate points for distant objects. Moreover, we propose a dual 2D auxiliary module to enhance image features and support the point reconstruction. Experimental evaluations conducted on adverse weather datasets demonstrate that the suggested approach surpasses current techniques. The implementation can be accessed at https://github.com/jhyoon964/oprnet."
Hierarchical Spatiotemporal Fusion for Event-Visible Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11127670,"Traditional visible light cameras are prone to performance degradation under varying weather and lighting conditions. To address this challenge, we introduce an eventbased camera and propose a novel hierarchical spatiotemporal fusion approach for event-visible object detection. Our method enhances detection performance by integrating data from both event-based and visible light cameras. We have designed three key modules: The Gated Event Accumulation Representation module (GEAR), the Temporal Feature Selection module (TFS), and the Adaptive Fusion module (AF). GEAR and TFS enhance temporal feature fusion at both image and feature levels, while AF effectively integrates multi-modal features with low computational complexity. Our approach has been trained and validated on the publicly available DSEC-Detection dataset, achieving mAP50 and mAP50-95 scores of 67.2% and 45.6%, respectively, demonstrating superior detection performance and validating the effectiveness of the proposed method."
CubeDN: Real-Time Drone Detection in 3D Space from Dual mmWave Radar Cubes.,https://doi.org/10.1109/ICRA55743.2025.11127766,"As drone use has become more widespread, there is a critical need to ensure safety and security. A key element of this is robust and accurate drone detection and localization. While cameras and other optical sensors like LiDAR are commonly used for object detection, their performance degrades under adverse lighting and environmental conditions. Therefore, this has generated interest in finding more reliable alternatives, such as millimeter-wave (mmWave) radar. Recent research on mmWave radar object detection has predominantly focused on 2D detection of road users. Although these systems demonstrate excellent performance for 2D problems, they lack the sensing capability to measure elevation, which is essential for 3D drone detection. To address this gap, we propose CubeDN, a single-stage end-to-end radar object detection network specifically designed for flying drones. CubeDN overcomes challenges such as poor elevation resolution by utilizing a dual radar configuration and a novel deep learning pipeline. It simultaneously detects, localizes, and classifies drones of two sizes, achieving decimeter-level tracking accuracy at closer ranges with overall 95% average precision (AP) and 85% average recall (AR). Furthermore, CubeDN completes data processing and inference at 10Hz, making it highly suitable for practical applications."
CA-IoU: Central-Gaussian Angle-IoU for Robust Bounding Box Regression.,https://doi.org/10.1109/ICRA55743.2025.11127790,"Accurate object detection depends on the precise refinement of bounding box regression. Recent advancements in bounding box regression have introduced a variety of methodologies aimed at reducing the disparity between predicted and ground truth bounding boxes. The prevailing objective functions for bounding box regression typically encompass three key perspectives: i) Intersection over Union (IoU), ii) distance between central points, and iii) aspect ratio alignment. Nonetheless, these existing loss functions encounter two primary challenges including slow convergence of the distance term and aspect ratio variation irrelevant to bounding box localization. This paper presents two novel loss terms to address these challenges. Firstly, we introduce the concept of the Integral of Central-Gaussian, a novel approach that leverages the cumulative distribution function (CDF) derived from a closed-form Gaussian distribution based on the central points of bounding boxes. Secondly, we introduce an alternative aspect ratio representation by minimizing the angle between two bounding boxes in direct proportion to their IoU. We term this comprehensive loss function Central-Gaussian Angle-IoU (CA-IoU), seamlessly incorporating the Integral of Central-Gaussian with angle-based IoU. Extensive experiments on various models and benchmarks for object detection highlight the superior performance of CA-IoU loss compared to existing bounding box regression methods. The source code and the corresponding trained models will be made available."
"Design, Contact Modeling, and Collision-Inclusive Planning of a Dual-Stiffness Aerial RoboT (DART).",https://doi.org/10.1109/ICRA55743.2025.11128035,"Collision-resilient quadrotors have gained significant attention given their potential for operating in cluttered environments and leveraging impacts to perform agile maneuvers. However, existing designs are typically single-mode: either safeguarded by propeller guards that prevent deformation or deformable but lacking rigidity, which is crucial for stable flight in open environments. This paper introduces DART, a Dual-stiffness Aerial RoboT, that adapts its post-collision response by either engaging a locking mechanism for a rigid mode or disengaging it for a flexible mode, respectively. Comprehensive characterization tests highlight the significant difference in post-collision responses between its rigid and flexible modes, with the rigid mode offering seven times higher stiffness compared to the flexible mode. To understand and harness the collision dynamics, we propose a novel collision response prediction model based on the linear complementarity system theory. We demonstrate the accuracy of predicting collision forces for both the rigid and flexible modes of DART. Experimental results confirm the accuracy of the model and underscore its potential to advance collision-inclusive trajectory planning in aerial robotics."
Learning Quadrotor Control from Visual Features Using Differentiable Simulation.,https://doi.org/10.1109/ICRA55743.2025.11128641,
Remote: Real-Time Ego-Motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning.,https://doi.org/10.1109/ICRA55743.2025.11128055,"Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here: remote-bmxs.netlify.app"
Intraoperative Trocar-Based Eyeball Rotation Estimation Using Only 2D Microscope Images.,https://doi.org/10.1109/ICRA55743.2025.11127673,"In ophthalmic surgery, surgeons or robots manipulate a light probe and an instrument around two separated trocars following sclerotomy to achieve orbital control for eyeball pose adjustment and subsequent surgical tasks referring to microscope frames. However, current methods face significant challenges in directly extracting the eyeball pose from real-time microscope frames due to the limited microscope perspective and the darkened operating room (OR). This paper decomposes eyeball rotations only along the x and y axes. Then, a method of calculating eyeball poses using eyeball geometry and microscopic trocar positions is presented. This method is tested by simulation and a phantom system with current [2.0, 2.8] degree error, providing assistant intraoperative eyeball status in the dark OR with extended method discussions."
Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots.,https://doi.org/10.1109/ICRA55743.2025.11128076,"Robot-assisted surgery has profoundly influenced current forms of minimally invasive surgery. However, in transurethral urological surgical robots, they need to work in a liquid environment. This causes vaporization of the liquid when shearing and heating is performed, resulting in bubble atomization that affects the visual perception of the robot. This can lead to the need for uninterrupted pauses in the surgical procedure, which makes the surgery take longer. To address the atomization characteristics of liquids under urological surgical robotic vision, we propose an unsupervised zero-shot dehaze method (RSF-Dehaze). Specifically, the proposed Region Similarity Filling Module (RSFM) of RSF-Dehaze significantly improves the recovery of blurred region tissues. In addition, we organize and propose a dehaze dataset for robotic vision in urological surgery (USRobot-Dehaze dataset). In particular, this dataset contains the three most common urological surgical robot operation scenarios. To the best of our knowledge, we are the first to organize and propose a publicly available dehaze dataset for urological surgical robot vision. The proposed RSF-Dehaze proves the effectiveness of our method in three urological surgical robot operation scenarios with extensive comparative experiments with 20 most classical and advanced dehazing and image recovery algorithms. The proposed source code and dataset are available at https://github.com/wurenkai/RSF-Dehaze."
Sim2real Within 5 Minutes: Efficient Domain Transfer with Stylized Gaussian Splatting for Endoscopic Images.,https://doi.org/10.1109/ICRA55743.2025.11127744,"Robot assisted endoluminal intervention is an emerging technique for both benign and malignant luminal lesions. With vision-based navigation, when combined with pre-operative imaging data as priors, it is possible to recover position and pose of the endoscope without the need of additional sensors. In practice, however, aligning pre-operative and intra-operative domains is complicated by significant texture differences. Although methods such as style transfer can be used to address this issue, they require large datasets from both source and target domains with prolonged training times. This paper proposes an efficient domain transfer method based on stylized Gaussian splatting, only requiring a few of real images (10 images) with very fast training time. Specifically, the transfer process includes two phases. In the first phase, the 3D models reconstructed from CT scans are represented as differential Gaussian point clouds. In the second phase, only color appearance related parameters are optimized to transfer the style and preserve the visual content. A novel structure consistency loss is applied to latent features and depth levels to enhance the stability of the transferred images. Detailed validation was performed to demonstrate the performance advantages of the proposed method compared to that of the current state-of-the-art, highlighting the potential for intra-operative surgical navigation."
Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-Driven Surface Normal-Aware Tracking and Mapping.,https://doi.org/10.1109/ICRA55743.2025.11128637,"Simultaneous Localization and Mapping (SLAM) is essential for precise surgical interventions and robotic tasks in minimally invasive procedures. While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM with high-quality novel view synthesis and fast rendering, these systems struggle with accurate depth and surface reconstruction due to multi-view inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between the reconstructed frames. In this work, we present Endo-2DTAM, a real-time endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which consists of tracking, mapping, and bundle adjustment modules for geometrically accurate reconstruction. Our robust tracking module combines point-topoint and point-to-plane distance metrics, while the mapping module utilizes normal consistency and depth distortion to enhance surface reconstruction quality. We also introduce a pose-consistent strategy for efficient and geometrically coherent keyframe sampling. Extensive experiments on public endoscopic datasets demonstrate that Endo-2DTAM achieves an RMSE of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1.87 \pm 0.63 \mathbf{m m}$</tex> for depth reconstruction of surgical scenes while maintaining computationally efficient tracking, high-quality visual appearance, and real-time rendering. Our code will be released at github.com/lastbasket/Endo-2DTAM."
HFUS-NeRF: Hybrid Representation for Fast Ultrasound Reconstruction in Robotic Ultrasound System.,https://doi.org/10.1109/ICRA55743.2025.11128742,"Telemedicine is promising in digital healthcare management, such as supporting the coronavirus disease 2019 (COVID-19) pandemic. Three-dimensional (3D) ultrasound reconstruction and new view image synthesis, which can assist in diagnosis and reexamine, have significant potential in teleultrasound, especially integrating robotic ultrasound systems (RUSS). Neural Radiance Field (NeRF), an impressive reconstruction method, requires long training times, limiting its practicality in ultrasound. Despite NeRF variants achieving faster optimization, their performance remains confined to natural scene reconstructions. To address this limitation, we propose HFUS-NeRF, a hybrid representation method designed for fast and accurate ultrasound reconstruction. HFUS-NeRF integrates multi-resolution hash-grid and tri-plane representations to represent each sampling point of the ultrasonic wave. A unified model for sampling points from different ultrasonic probes is presented to simulate the wave's propagation through tissues, and the final ultrasound image is rendered using volume rendering. Compared with NeRF-based ultrasound reconstruction, both the hash grid and triplane resolutions can be scaled up more efficiently, improving reconstruction speed. Experimental results demonstrate that HFUS-NeRF enhances reconstruction quality while significantly reducing reconstruction time to mere minutes. Furthermore, we validated HFUS-NeRF's adaptability by reconstruction using images from different types of ultrasound probes, and real-world experiments confirmed its feasibility and transferability, enabling fast ultrasound reconstruction on human subjects."
Word2Wave: Language Driven Mission Programming for Efficient Subsea Deployments of Marine Robots.,https://doi.org/10.1109/ICRA55743.2025.11128181,"This paper explores the design and development of a language-based interface for dynamic mission programming of autonomous underwater vehicles (AUVs). The proposed 'Word2Wave' (W2W) framework enables interactive programming and parameter configuration of AUVs for remote subsea missions. The W2W framework includes: (i) a set of novel language rules and command structures for efficient language-to-mission mapping; (ii) a GPT-based prompt engineering module for training data generation; (iii) a small language model (SLM)-based sequence-to-sequence learning pipeline for mission command generation from human speech or text; and (iv) a novel user interface for 2D mission map visualization and human-machine interfacing. The proposed learning pipeline adapts an SLM named T5-Small that can learn language-to-mission mapping from processed language data effectively, providing robust and efficient performance. In addition to a benchmark evaluation with state-of-the-art, we conduct a user interaction study to demonstrate the effectiveness of W2W over commercial AUV programming interfaces. Across participants, W2W-based programming required less than 10% time for mission programming compared to traditional interfaces; it is deemed to be a simpler and more natural paradigm for subsea mission programming with a usability score of 76.25. W2W opens up promising future research opportunities on hands-free AUV mission programming for efficient subsea deployments."
A Data-Driven Velocity Estimator for Autonomous Underwater Vehicles Experiencing Unmeasurable Flow and Wave Disturbance.,https://doi.org/10.1109/ICRA55743.2025.11128636,"Autonomous Underwater Vehicles (AUVs) encounter significant challenges in confined spaces like ports and testing tanks, where vehicle-environment interactions, such as wave reflections and unsteady flows, introduce complex, time-varying disturbances. Model-based state estimation methods can struggle to handle these dynamics, leading to localization errors. To address this, we propose a data-driven velocity estimation approach using Inertial Measurement Units (IMUs) and a Gated Recurrent Unit (GRU) neural network, capturing temporal dependencies and rejecting external disturbances. This velocity estimator is then integrated into a sensor fusion framework using an asynchronous Kalman filter to improve localization by fusing on-board and off-board sensor information. Experimental validation on miniature AUVs demonstrates the effectiveness of the proposed method in enhancing accuracy for velocity and position estimation in environments with significant disturbances due to interactions between the vehicle and the environment."
"Dynamic End Effector Trajectory Tracking for Small-Scale Underwater Vehicle-Manipulator Systems (UVMS): Modeling, Control, and Experimental Validation.",https://doi.org/10.1109/ICRA55743.2025.11128667,"With the ongoing miniaturization, recently, lightweight, commercial underwater vehicle-manipulator systems (UVMSs) have emerged that massively lower the entry barrier into underwater manipulation. Within this research field, dynamic and accurate end effector trajectory tracking is a crucial first step in developing autonomous capabilities. In this context, coupling effects between the manipulator and vehicle dynamics are expected to pose a considerable challenge. However, UVMS control strategies analyzed in detailed experimental studies are particularly rare. We present a holistic approach based on task-priority control that we describe and discuss from modeling towards extensive experimental studies, which are crucial for the notoriously hard-to-simulate underwater domain. We demonstrate this framework on the widely used platform of a BlueROV2 and an Alpha 5 manipulator. The end effector trajectory tracking is shown to be highly accurate, with <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$&lt;4 \text{cm}$</tex> median position error. Moreover, our experimental findings on the consideration of dynamic coupling within UVMS control motivate further research. The code is available at https://github.com/HippoCampusRobotics/uvms. A video of the results is available at https://youtu.be/IDM1I5KqlVI."
P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies.,https://doi.org/10.1109/ICRA55743.2025.11128755,"Developing generalizable robot policies that can robustly handle varied environmental conditions and object instances remains a fundamental challenge in robot learning. While considerable efforts have focused on collecting large robot datasets and developing policy architectures to learn from such data, navely learning from visual inputs often results in brittle policies that fail to transfer beyond the training data. This work presents Prescriptive Point Priors for Policies or P3-PO, a novel framework that constructs a unique state representation of the environment leveraging recent advances in computer vision and robot learning to achieve improved out-of-distribution generalization for robot manipulation. This representation is obtained through two steps. First, a human annotator prescribes a set of semantically meaningful points on a single demonstration frame. These points are then propagated through the dataset using off-the-shelf vision models. The derived points serve as an input to state-of-the-art policy architectures for policy learning. Our experiments across four real-world tasks demonstrate an overall 43% absolute improvement over prior methods when evaluated in identical settings as training. Further, P3-PO exhibits 58% and 80% gains across tasks for new object instances and more cluttered environments respectively. Videos illustrating the robot's performance are best viewed at point-priors.github.io."
APA-BI: Adaptive Partition Aggregation and Bidirectional Integration for UAV-View Geo-Localization.,https://doi.org/10.1109/ICRA55743.2025.11128402,"The task of UAV-view geo-localization is to match a query image with database images to estimate the current geographic location of the query image. This is particularly useful in environments where GPS is not available or when the device fails. Although deep learning methods make sufficient progress in UAV-view geo-localization, they still face challenges in improving the distinguishability of features. For instance, some feature aggregation methods do not consider semantic integrity, and robust elements in the image are not given enough attention. This paper proposes a UAV-view geo-localization method (APA-BI) to tackle the above issues. Specifically, we propose an adaptive partition aggregation method to ensure feature integrity at the semantic level by increasing the receptive field of the classifier module. At the same time, we design a bidirectional integration module to further enhance feature distinguishability by extracting robust tubular topological structures from images. Experimental results on public datasets demonstrate that APA-BI achieves impressive retrieval accuracy and outperforms most state-of-the-art methods. Moreover, the test results of APA-BI in real-world scenarios also show excellent performance."
Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning.,https://doi.org/10.1109/ICRA55743.2025.11127531,"Multimodal task specification is essential for enhanced robotic performance, where Cross-modality Alignment enables the robot to holistically understand complex task instructions. Directly annotating multimodal instructions for model training proves impractical, due to the sparsity of paired multimodal data. In this study, we demonstrate that by leveraging unimodal instructions abundant in real data, we can effectively teach robots to learn multimodal task specifications. First, we endow the robot with strong Crossmodality Alignment capabilities, by pretraining a robotic multimodal encoder using extensive out-of-domain data. Then, we employ two Collapse and Corrupt operations to further bridge the remaining modality gap in the learned multimodal representation. This approach projects different modalities of identical task goal as interchangeable representations, thus enabling accurate robotic operations within a well-aligned multimodal latent space. Evaluation across more than 130 tasks and 4000 evaluations on both simulated LIBERO benchmark and real robot platforms showcases the superior capabilities of our proposed framework, demonstrating significant potential in overcoming data constraints in robotic learning. Website: zh1hao.wang/Robo_MUTUAL"
Trajectory Planning with Signal Temporal Logic Costs Using Deterministic Path Integral Optimization.,https://doi.org/10.1109/ICRA55743.2025.11127582,"Formulating the intended behavior of a dynamic system can be challenging. Signal temporal logic (STL) is frequently used for this purpose due to its suitability in formalizing comprehensible, modular, and versatile spatiotemporal specifications. Due to scaling issues with respect to the complexity of the specifications and the potential occurrence of non-differentiable terms, classical optimization methods often solve STL-based problems inefficiently. Smoothing and approximation techniques can alleviate these issues but require changing the optimization problem. This paper proposes a novel sampling-based method based on model predictive path integral control to solve optimal control problems with STL cost functions. We demonstrate the effectiveness of our method on benchmark motion planning problems and compare its performance with state-of-the-art methods. The results show that our method efficiently solves optimal control problems with STL costs."
Multi-Agent Path Finding Using Conflict-Based Search and Structural-Semantic Topometric Maps.,https://doi.org/10.1109/ICRA55743.2025.11128758,"As industries increasingly adopt large robotic fleets, there is a pressing need for computationally efficient, practical, and optimal conflict-free path planning for multiple robots. Conflict-Based Search (CBS) is a popular method for multi-agent path finding (MAPF) due to its completeness and optimality; however, it is often impractical for real-world applications, as it is computationally intensive to solve and relies on assumptions about agents and operating environments that are difficult to realize. This article proposes a solution to overcome computational challenges and practicality issues of CBS by utilizing structural-semantic topometric maps. Instead of running CBS over large grid-based maps, the proposed solution runs CBS over a sparse topometric map containing structural-semantic cells representing intersections, pathways, and dead ends. This approach significantly accelerates the MAPF process and reduces the number of conflict resolutions handled by CBS while operating in continuous time. In the proposed method, robots are assigned time ranges to move between topometric regions, departing from the traditional CBS assumption that a robot can move to any connected cell in a single time step. The approach is validated through real-world multi-robot path-finding experiments and benchmarking simulations. The results demonstrate that the proposed MAPF method can be applied to real-world non-holonomic robots and yields significant improvement in computational efficiency compared to traditional CBS methods while improving conflict detection and resolution in cases of corridor symmetries."
Enabling Multi-Robot Collaboration from Single-Human Guidance.,https://doi.org/10.1109/ICRA55743.2025.11127982,
Fan-Out Revisited: The Impact of the Human Element on Scalability of Human Multi-Robot Teams.,https://doi.org/10.1109/ICRA55743.2025.11128358,"This paper introduces a novel fan-out model that improves accuracy over previous models. The commonly used models rely on neglect time, the time an agent operates independently, which confounds both human and robot abilities. The proposed model separates neglect time into two functionally distinct concepts: the time a robot can operate self-sufficiently, and the time a human estimates the robot can do so. Previous research indicates fan-out is often overestimated. This work explains why robot ability provides an upper bound to fan-out, but that actual achieved fan-out is influenced by both the human and robot abilities. We conduct a study to validate this new model and show improved performance over the two most common fan-out models. The results show that both previous models overestimate as predicted. Using the new fan-out model, we show that as the difference between human estimation and robot abilities grows, the actual fan-out will fall further from the upper bound potential fan-out. By including assessments of both the robotic and human elements, the new model provides a more nuanced understanding of the dynamics at play and the factors involved in scaling Human Multi-Robot Teams."
HARP: Human-Assisted Regrouping With Permutation Invariant Critic for Multi-Agent Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127727,"Human-in-the-loop reinforcement learning integrates human expertise to accelerate agent learning and provide critical guidance and feedback in complex fields. However, many existing approaches focus on single-agent tasks and require continuous human involvement during the training process, significantly increasing the human workload and limiting scalability. In this paper, we propose HARP (HumanAssisted Regrouping with Permutation Invariant Critic), a multi-agent reinforcement learning framework designed for group-oriented tasks. HARP integrates automatic agent regrouping with strategic human assistance during deployment, enabling and allowing non-experts to offer effective guidance with minimal intervention. During training, agents dynamically adjust their groupings to optimize collaborative task completion. When deployed, they actively seek human assistance and utilize the Permutation Invariant Group Critic to evaluate and refine human-proposed groupings, allowing non-expert users to contribute valuable suggestions. In multiple collaboration scenarios, our approach is able to leverage limited guidance from non-experts and enhance performance. The project can be found at https://github.com/huawen-hu/HARP."
Training Human-Robot Teams by Improving Transparency Through a Virtual Spectator Interface.,https://doi.org/10.1109/ICRA55743.2025.11127625,"After-action reviews (AARs) are professional discussions that help operators and teams enhance their task performance by analyzing completed missions with peers and professionals. Previous studies comparing different formats of AARs have focused mainly on human teams. However, the inclusion of robotic teammates brings along new challenges in understanding teammate intent and communication. Traditional AAR between human teammates may not be satisfactory for human-robot teams. To address this limitation, we propose a new training review (TR) tool, called the Virtual Spectator Interface (VSI), to enhance human-robot team performance and situational awareness (SA) in a simulated search mission. The proposed VSI primarily utilizes visual feedback to review subjects' behavior. To examine the effectiveness of VSI, we took elements from AAR to conduct our own TR, and designed a 1 <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\times 3$</tex> between-subjects experiment with experimental conditions: TR with (1) VSI, (2) screen recording, and (3) non-technology (only verbal descriptions). The results of our experiments demonstrated that the VSI did not result in significantly better team performance than other conditions. However, the TR with VSI led to more improvement in the subjects' SA over the other conditions."
Development of Contactless Delivery Service Robot with Modular Working Platform in Isolation Wards.,https://doi.org/10.1109/ICRA55743.2025.11128317,
RACCOON: Grounding Embodied Question-Answering with State Summaries from Existing Robot Modules.,https://doi.org/10.1109/ICRA55743.2025.11127843,"Explainability is vital for establishing user trust, also in robotics. Recently, foundation models (e.g. vision-language models, VLMs) fostered a wave of embodied agents that answer arbitrary queries about their environment and their interactions with it. However, naively prompting VLMs to answer queries based on camera images does not take into account existing robot architectures which represent the robot's tasks, skills, and beliefs about the state of the world. To overcome this limitation, we propose RACCOON, a framework that combines foundation models' responses with a robot's internal knowledge. Inspired by Retrieval-Augmented Generation (RAG), RACCOON selects relevant context, retrieves information from the robot's state, and utilizes it to refine prompts for an LLM to answer questions accurately. This bridges the gap between the model's adaptability and the robot's domain expertise."
GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and Human Explanations.,https://doi.org/10.1109/ICRA55743.2025.11127826,"When operating in human environments, robots need to handle complex tasks while both adhering to social norms and accommodating individual preferences. For instance, based on common sense knowledge, a household robot can pre-dict that it should avoid vacuuming during a social gathering, but it may still be uncertain whether it should vacuum before or after having guests. In such cases, integrating common-sense knowledge with human preferences, often conveyed through human explanations, is fundamental yet a challenge for existing systems. In this paper, we introduce GRACE, a novel approach addressing this while generating socially appropriate robot actions. GRACE leverages common sense knowledge from LLMs, and it integrates this knowledge with human explanations through a generative network. The bidirectional structure of GRACE enables robots to refine and enhance LLM predictions by utilizing human explanations and makes robots capable of generating such explanations for human-specified actions. Our evaluations show that integrating human explanations boosts GRACE's performance, where it outperforms several baselines and provides sensible explanations."
Robi Butler: Multimodal Remote Interaction with a Household Robot Assistant.,https://doi.org/10.1109/ICRA55743.2025.11128329,"Imagine a future when we can Zoom-call a robot to manage household chores remotely. This work takes one step in this direction. Robi Butler is a new household robot assistant that enables seamless multimodal remote interaction. It allows the human user to monitor its environment from a first-person view, issue voice or text commands, and specify target objects through hand-pointing gestures. At its core, a high-level behavior module, powered by Large Language Models (LLMs), interprets multimodal instructions to generate multistep action plans. Each plan consists of open-vocabulary primitives supported by vision-language models, enabling the robot to process both textual and gestural inputs. Zoom provides a convenient interface to implement remote interactions between the human and the robot. The integration of these components allows Robi Butler to ground remote multimodal instructions in real-world home environments in a zero-shot manner. We evaluated the system on various household tasks, demonstrating its ability to execute complex user commands with multimodal inputs. We also conducted a user study to examine how multimodal interaction influences user experiences in remote human-robot interaction. These results suggest that with the advances in robot foundation models, we are moving closer to the reality of remote household robot assistants."
AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement.,https://doi.org/10.1109/ICRA55743.2025.11128062,"An embodied agent assisting humans is often asked to complete new tasks, and there may not be sufficient time or labeled examples to train the agent to perform these new tasks. Large Language Models (LLMs) trained on considerable knowledge across many domains can be used to predict a sequence of abstract actions for completing such tasks, although the agent may not be able to execute this sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation in the context of cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM. Project website<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""></sup>Project supported in part by TCS Research India: https://sssshivvvv.github.io/adaptbot/"
Multi-Agent Path Planning in Complex Environments using Gaussian Belief Propagation with Global Path Finding.,https://doi.org/10.1109/ICRA55743.2025.11128006,"Multi-agent path planning is a critical challenge in robotics, requiring agents to navigate complex environments while avoiding collisions and optimizing travel efficiency. This work addresses the limitations of existing approaches by combining Gaussian belief propagation with path integration and introducing a novel tracking factor to ensure strict adherence to global paths. The proposed method is tested with two different global path-planning approaches: rapidly exploring random trees and a structured planner, which leverages predefined lane structures to improve coordination. A simulation environment was developed to validate the proposed method across diverse scenarios, each posing unique challenges in navigation and communication. Simulation results demonstrate that the tracking factor reduces path deviation by 28% in single-agent and 16% in multi-agent scenarios, highlighting its effectiveness in improving multi-agent coordination, especially when combined with structured global planning."
Olympus: A Jumping Quadruped for Planetary Exploration Utilizing Reinforcement Learning for In-Flight Attitude Control.,https://doi.org/10.1109/ICRA55743.2025.11127737,"Exploring planetary bodies with lower gravity, such as the moon and Mars, allows legged robots to utilize jumping as an efficient form of locomotion thus giving them a valuable advantage over traditional rovers for exploration. Motivated by this fact, this paper presents the design, simulation, and learning-based in-flight attitude control of Olympus, a jumping legged robot tailored to the gravity of Mars. First, the design requirements are outlined followed by detailing how simulation enabled optimizing the robot's design - from its legs to the overall configuration - towards high vertical jumping, forward jumping distance, and in-flight attitude reorientation. Subsequently, the reinforcement learning policy used to track desired in-flight attitude maneuvers is presented. Successfully crossing the sim2real gap, extensive experimental studies of attitude reorientation tests are demonstrated."
THAMP-3D: Tangent-Based Hybrid A* Motion Planning for Tethered Robots in Sloped 3D Terrains.,https://doi.org/10.1109/ICRA55743.2025.11128469,"This paper introduces a novel motion planning algorithm designed for a team of curvature-constrained tethered robots operating on sloped 3D terrains. Our approach addresses the critical issues of tether-terrain interaction, robot stability, and tether entanglement avoidance. The study focuses on a two-robot system, where stability is primarily dependent on tether tension, which is in turn limited by wheel traction. We propose a path-planning method that strategically utilizes terrain features (e.g., rocks) to augment tether tension through additional friction, thereby enhancing overall system stability. Our algorithm employs a modified tangent graph as the underlying structure for a hybrid A* search, incorporating stability constraints throughout the planning process. The proposed method is extensively evaluated through various simulation experiments, demonstrating its effectiveness in planning safe and efficient paths."
Deep Learning Based Topography Aware Gas Source Localization with Mobile Robot.,https://doi.org/10.1109/ICRA55743.2025.11128134,"Gas source localization in complex environments is critical for applications such as environmental monitoring, industrial safety, and disaster response. Traditional methods often struggle with the challenges posed by a lack of environmental topography integration, especially when interactions between wind and obstacles distort gas dispersion patterns. In this paper, we propose a deep learning-based approach, which leverages spatial context and environmental mapping to enhance gas source localization. By integrating Simultaneous Localization and Mapping (SLAM) with a U-Net-based model, our method predicts the likelihood of gas source locations by analyzing gas sensor data, wind flow, and topography of the environment represented by a 2D occupancy map. We demonstrate the efficacy of our approach using a wheeled robot equipped with a photoionization detector, a LIDAR, and an anemometer, in various scenarios with dynamic wind fields and multiple obstacles. The results show that our approach can robustly locate gas sources, even in challenging environments with fluctuating wind directions, outperforming conventional methods by utilizing topography contextual information. This study underscores the importance of topographical context in gas source localization and offers a flexible and robust solution for real-world applications. Data and code are publicly available."
Online Design Optimization of Passive Exoskeletons Using Fast Biomechanics Simulation and Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128497,"Exoskeletons are being adopted as assistive devices in industries such as manufacturing, logistics, and construction, aimed at reducing musculoskeletal loads in workers. Presently, their design process assumes the user to be quasi-static, optimizing the design parameters for reduction of human joint torques followed by fine-tuning through usability studies and physical prototyping. We present a method for optimizing passive exoskeleton designs before the physical prototyping stage for muscle effort reduction in dynamic tasks such as arm reaching and walking. We employ fast MuJoCo-based simulations of human biomechanics to compute the joint torques, muscle forces and muscle activations while executing task trajectories using pre-trained reinforcement learning models from the literature. We train another set of reinforcement learning models that minimize joint torques and muscle effort rates by varying the exoskeleton's design parameters online during the task motions. Baselines for comparison include the default designs of shoulder and walking assist exoskeletons from the literature, and designs obtained through conventional optimization techniques. In terms of muscle effort rates, the RL-based designs improved upon these baselines by an average of 3.42% and 1.96% respectively in the arm reaching task, and 6.28% and 5.81% in the walking task. Our method can be adapted to evaluate exoskeletons in real-time through motion capture, and for muscle-aware online control of powered exoskeletons."
Accurately Modeling the Output Torque and Stiffness of Ankle-Foot Orthoses with a Compliant Linkage Model.,https://doi.org/10.1109/ICRA55743.2025.11128177,"The stiffness of passive lower-limb exoskeletons and orthoses governs their assistance. A common practice in the design of these systems is to assume the stiffness of the device is determined only by the intended elastic element (e.g., spring), while the structural components, human attachments, and soft tissues are considered rigid. In practice, the mechanical behavior of orthoses is significantly affected by the compliance of these elements, which drastically impacts the assistance provided. In this work, we present a linkage model with compliant elements that can accurately predict the applied stiffness of ankle-foot orthoses, and retroactively estimate the stiffness of unintended spring elements from published data. The compliant model accurately predicted the torque trajectories of two published passive orthoses with modeled peak torques within 4 % to 7 % of measured values. In contrast, the rigid model greatly overestimated the peak torques, predicting 203 % to 376 % of the measured values. The compliant model also indicated that an onboard joint encoder could only measure 52 % to 69 % of the peak ankle angle recorded with motion capture. The compliant model was also used to reassess the stiffness range of a variable-stiffness orthosis, indicating that its adjustable range is likely 69 % of rigid model predictions. Overall, this work highlights the need to consider how unmodeled compliance affects the mechanical behavior of orthoses and provides a foundation for further exploration."
Towards Neurorobotic Interface for Finger Joint Angle Estimation: A Multi-Stage CNN-LSTM Network with Transfer Learning.,https://doi.org/10.1109/ICRA55743.2025.11127944,"To maximize the autonomy of individuals with upper limb amputations in daily activities, leveraging forearm muscle information to infer movement intent is a promising research direction. While current prosthetic hand technologies can utilize forearm muscle data to achieve basic movements such as grasping, accurately estimating finger joint angles remains a significant challenge. Therefore, we propose a Multi-Stage Cascade Convolutional Neural Network with Long Short-Term Memory Network, where an upsampling module is introduced before the downsampling module to enhance model generalization. Additionally, we designed a transfer learning (TL) framework based on parameter freezing, where the pre-trained downsampling module is fixed, and only the upsampling module is updated with a small amount of out-ofdistribution data to achieve TL. Furthermore, we compared the performance of unimodal and multimodal models, collecting surface electromyography (sEMG) signals, brightness mode ultrasound images (B-mode US images), and motion capture data simultaneously. The results show that on the validation set, the US image had the lowest error, while on the prediction set, the four-channel sEMG achieved the lowest error. The performance of the multimodal model in both datasets was intermediate between the unimodal models. On the prediction set, the average normalized root mean square error values for the four-channel sEMG, US images, and sensor fusion models across three subjects were 0.170,0.203, and 0.186, respectively. By utilizing advanced sensor fusion techniques and TL, our approach can reduce the need for extensive data collection and training for new users, making prosthetic control more accessible and adaptable to individual needs."
Here's your PDDL Problem File! On Using VLMs for Generating Symbolic PDDL Problem Files.,https://doi.org/10.1109/ICRA55743.2025.11127832,"Large Language Models (LLMs) excel at generating contextually relevant text but lack logical reasoning abilities. They rely on statistical patterns rather than logical inference, making them unreliable for structured decision-making. Integrating LLMs with task planning can address this limitation by combining their natural language understanding with the precise, goal-oriented reasoning of planners. This paper introduces ViPlan, a hybrid system that leverages Vision Language Models (VLMs) to extract high-level semantic information from visual and textual inputs while integrating classical planners for logical reasoning. ViPlan utilizes VLMs to generate syntactically correct and semantically meaningful PDDL problem files from images and natural language instructions, which are then processed by a task planner to generate an executable plan. The entire process is embedded within a behavior tree framework, enhancing efficiency, reactivity, replanning, modularity, and flexibility. The generation and planning capabilities of ViPlan are empirically evaluated with simulated and real-world experiments."
MuST: Multi-Head Skill Transformer for Long-Horizon Dexterous Manipulation with Skill Progress.,https://doi.org/10.1109/ICRA55743.2025.11127280,"Robot picking and packing tasks require dexterous manipulation skills, such as rearranging objects to establish a good grasping pose, or placing and pushing items to achieve tight packing. These tasks are challenging for robots due to the complexity and variability of the required actions. To tackle the difficulty of learning and executing long-horizon tasks, we propose a novel framework called the Multi-Head Skill Transformer (MuST). This model is designed to learn and sequentially chain together multiple motion primitives (skills), enabling robots to perform complex sequences of actions effectively. MuST introduces a progress value for each skill, guiding the robot on which skill to execute next and ensuring smooth transitions between skills. Additionally, our model is capable of expanding its skill set and managing various sequences of sub-tasks efficiently. Extensive experiments in both simulated and real-world environments demonstrate that MuST significantly enhances the robot's ability to perform long-horizon dexterous manipulation tasks."
CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills Using Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128783,"Curriculum learning is a training mechanism in reinforcement learning (RL) that facilitates the achievement of complex policies by progressively increasing the task difficulty during training. However, designing effective curricula for a specific task often requires extensive domain knowledge and human intervention, which limits its applicability across various domains. Our core idea is that large language models (LLMs), with their extensive training on diverse language data and ability to encapsulate world knowledge, present significant potential for efficiently breaking down tasks and decomposing skills across various robotics environments. Additionally, the demonstrated success of LLMs in translating natural language into executable code for RL agents strengthens their role in generating task curricula. In this work, we propose CurricuLLM, which leverages the high-level planning and programming capabilities of LLMs for curriculum design, thereby enhancing the efficient learning of complex target tasks. CurricuLLM consists of: (Step 1) Generating a sequence of subtasks that aid target task learning in natural language form, (Step 2) Translating natural language description of subtasks in executable task code, including the reward code and goal distribution code, and (Step 3) Evaluating trained policies based on trajectory rollout and subtask description. We evaluate Cur-ricuLLM in various robotics simulation environments, ranging from manipulation, navigation, and locomotion, to show that CurricuLLM can aid learning complex robot control tasks. In addition, we validate humanoid locomotion policy learned through CurricuLLM in the real-world. Project website is https://iconlab.negarmehr.com/CurricuLLM/"
PUGS: Zero-Shot Physical Understanding with Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11128683,"Current robotic systems can understand the categories and poses of objects well. But understanding physical properties like mass, friction, and hardness, in the wild, remains challenging. We propose a new method that reconstructs 3D objects using the Gaussian splatting representation and predicts various physical properties in a zero-shot manner. We propose two techniques during the reconstruction phase: a geometryaware regularization loss function to improve the shape quality and a region-aware feature contrastive loss function to promote region affinity. Two other new techniques are designed during inference: a feature-based property propagation module and a volume integration module tailored for the Gaussian representation. Our framework is named as zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS achieves new state-of-the-art results on the standard benchmark of ABO-500 mass prediction. We provide extensive quantitative ablations and qualitative visualization to demonstrate the mechanism of our designs. We show the proposed methodology can help address challenging real-world grasping tasks. Our codes, data, and models are available at https://github.com/EverNorif/PUGS"
Image-Guided Surgical Planning for Percutaneous Nephrolithotomy Using CTRs: A Phantom-Based Study.,https://doi.org/10.1109/ICRA55743.2025.11127423,"In this paper, we validate the effectiveness of the optimal planning algorithms we have developed for devising surgical plans for Percutaneous Nephrolithotomy (PCNL) using patient-specific Concentric-Tube Robots (CTRs). To do so, we built a life-sized phantom model of the right hemithorax, replicating the anatomy of a patient who suffered from kidney stone and underwent conventional PCNL. Two-dimensional CT scans of the phantom model and its 3D reconstruction enabled the creation of a surgical plan using our planning algorithms based on a puncture into the mid-pole of the kidney. This was compared with two other percutaneous tracts involving punctures into the lower and upper calyces for comparison. The optimal mid-pole plan achieved 84% stone coverage, significantly outperforming the lower pole (58 %) and upper pole (45 %) plans. These results validate the effectiveness of the algorithms and align with simulation-based findings from previous studies, which reported an average volume coverage of 81.619.6 % in clinical cases."
ETSM: Automating Dissection Trajectory Suggestion and Confidence Map-Based Safety Margin Prediction for Robot-Assisted Endoscopic Submucosal Dissection.,https://doi.org/10.1109/ICRA55743.2025.11128241,"Robot-assisted Endoscopic Submucosal Dissection (ESD) improves the surgical procedure by providing a more comprehensive view through advanced robotic instruments and bimanual operation, thereby enhancing dissection efficiency and accuracy. Accurate prediction of dissection trajectories is crucial for better decision-making, reducing intraoperative errors, and improving surgical training. Nevertheless, predicting these trajectories is challenging due to variable tumor margins and dynamic visual conditions. To address this issue, we create the ESD Trajectory and Confidence Map-based Safety Margin (ETSM) dataset with 1849 short clips, focusing on submucosal dissection with a dual-arm robotic system. We also introduce a framework that combines optimal dissection trajectory prediction with a confidence map-based safety margin, providing a more secure and intelligent decision-making tool to minimize surgical risks for ESD procedures. Additionally, we propose the Regression-based Confidence Map Prediction Network (RCMNet), which utilizes a regression approach to predict confidence maps for dissection areas, thereby delineating various levels of safety margins. We evaluate our RCMNet using three distinct experimental setups: in-domain evaluation, robustness assessment, and out-of-domain evaluation. Experimental results show that our approach excels in the confidence map-based safety margin prediction task, achieving a mean absolute error (MAE) of only 3.18. To the best of our knowledge, this is the first study to apply a regression approach for visual guidance concerning delineating varying safety levels of dissection areas. Our approach bridges gaps in current research by improving prediction accuracy and enhancing the safety of the dissection process, showing great clinical significance in practice. The dataset and code are available at https://github.com/FrankMOWJ/RCMNet."
Partial-to-Full Registration based on Gradient-SDF for Computer-Assisted Orthopedic Surgery.,https://doi.org/10.1109/ICRA55743.2025.11127578,"In computer-assisted orthopedic surgery (CAOS), accurate pre-operative to intra-operative bone registration is an essential and critical requirement for providing navigational guidance. This registration process is challenging since the intra-operative 3D points are sparse, only partially overlapped with the pre-operative model, and disturbed by noise and outliers. The commonly used method in current state-of-the-art orthopedic robotic system is bony landmarks based registration, but it is very time-consuming for the surgeons. To address these issues, we propose a novel partial-to-full registration framework based on gradient-SDF for CAOS. The simulation experiments using bone models from publicly available datasets and the phantom experiments performed under both optical tracking and electromagnetic tracking systems demonstrate that the proposed method can provide more accurate results than standard benchmarks and be robust to 90% outliers. Importantly, our method achieves convergence in less than 1 second in real scenarios and mean target registration error values as low as 2.198 mm for the entire bone model. Finally, it only requires random acquisition of points for registration by moving a surgical probe over the bone surface without the need for correspondences, thus showing significant potential clinical value. The code of the framework is available*."
Sampling-Based Model Predictive Control for Volumetric Ablation in Robotic Laser Surgery.,https://doi.org/10.1109/ICRA55743.2025.11127624,"Laser-based surgical ablation relies heavily on surgeon involvement, restricting precision to the limits of human error and perception. The interaction between laser and tissue is governed by various laser parameters that control the laser irradiance on the tissue, including the power, distance, spot size, orientation, and exposure time. This complex interaction lends itself to robotic automation, allowing the surgeon to focus on high-level tasks, such as choosing the region and method of ablation, while the lower-level ablation plan can be handled autonomously. This paper describes a sampling-based model predictive control (MPC) scheme to plan ablation sequences for arbitrary tissue volumes. Using a steady-state point ablation model to simulate a single laser-tissue interaction, a random search technique explores the reachable state space while preserving sensitive tissue regions. The sampled MPC strategy provides an ablation sequence that accounts for parameter uncertainty without violating constraints, such as avoiding nerve bundles."
SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks.,https://doi.org/10.1109/ICRA55743.2025.11127797,"Behavior cloning facilitates the learning of dexterous manipulation skills, yet the complexity of surgical environments, the difficulty and expense of obtaining patient data, and robot calibration errors present unique challenges for surgical robot learning. We provide an enhanced surgical digital twin with photorealistic human anatomical organs, integrated into a comprehensive simulator designed to generate high-quality synthetic data to solve fundamental tasks in surgical autonomy. We present SuFIA-BC: visual Behavior Cloning policies for Surgical First Interactive Autonomy Assistants. We investigate visual observation spaces including multi-view cameras and 3D visual representations extracted from a single endoscopic camera view. Through systematic evaluation, we find that the diverse set of photorealistic surgical tasks introduced in this work enables a comprehensive evaluation of prospective behavior cloning models for the unique challenges posed by surgical environments. We observe that current state-of-the-art behavior cloning techniques struggle to solve the contact-rich and complex tasks evaluated in this work, regardless of their underlying perception or control architectures. These findings highlight the importance of customizing perception pipelines and control architectures, as well as curating larger-scale synthetic datasets that meet the specific demands of surgical tasks. Project website: orbit-surgical.github.io/sufia-bc/"
Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127224,"Recent studies have made significant progress in addressing dexterous manipulation problems, particularly in inhand object reorientation. However, there are few existing works that explore the potential utilization of developed dexterous manipulation controllers for downstream tasks. In this study, we focus on constrained dexterous manipulation for food peeling. Food peeling presents various constraints on the reorientation controller, such as the requirement for the hand to securely hold the object after reorientation for peeling. We propose a simple system for learning a reorientation controller that facilitates the subsequent peeling task. Videos are available at: https://taochenshh.github.io/projects/veg-peeling."
Prompt-Responsive Object Retrieval with Memory-Augmented Student-Teacher Learning.,https://doi.org/10.1109/ICRA55743.2025.11128512,"Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io"
Implicit Articulated Robot Morphology Modeling with Configuration Space Neural Signed Distance Functions.,https://doi.org/10.1109/ICRA55743.2025.11127575,"In this paper, we introduce a novel approach to implicitly encode precise robot morphology using forward kinematics based on a configuration space signed distance function. Our proposed Robot Neural Distance Function (RNDF) optimizes the balance between computational efficiency and accuracy for signed distance queries conditioned on the robot's configuration for each link. Compared to the baseline method, the proposed approach achieves an 81.1% reduction in distance error while utilizing only 47.6% of model parameters. Its parallelizable and differentiable nature provides direct access to joint-space derivatives, enabling a seamless connection between robot planning in Cartesian task space and configuration space. These features make RNDF an ideal surrogate model for general robot optimization and learning in 3D spatial planning tasks. Specifically, we apply RNDF to robotic arm-hand modeling and demonstrate its potential as a core platform for wholearm, collision-free grasp planning in cluttered environments. The code and model are available at https://github.com/roboticmanipulation/RNDF."
A Data-Efficient Progressive Learning Framework for Robot Scooping Task.,https://doi.org/10.1109/ICRA55743.2025.11127485,"Robot scooping is a challenging and important task in robotic tool manipulation research due to the complex relationship between the robot, the tool, and target objects/environment. Taking into account different tools, different target objects and varying environments, the required scooping manipulation strategy usually varies greatly. Even considering a specific type of spoon, the question of how to obtain a policy model that requires less demonstration data but shows better generalization capabilities deserves further exploration. In this paper, we propose a progressive learning framework for general robot scooping tasks, which requires a limited number of demonstrations but shows promising generalization capability. We first learn a scooping policy via human demonstrations with a specific setup. We then use this as a pre-train model for reinforcement learning in a curriculum manner to achieve a scooping strategy that is generalizable to different task setups. Finally, we evaluate the capabilities of the policy with a series of experiments both in simulation and on a real robot."
Manipulability Transfer and Tracking Control: Bridging Domain Adaptation with Predictive Feasibility.,https://doi.org/10.1109/ICRA55743.2025.11127841,"This paper introduces a novel framework for improving human-to-robot manipulability transfer and tracking in Learning by Demonstration. Our approach addresses key challenges, including manipulability ellipsoid (ME) domain adaptation between different kinematic structures, ME-IK feasibility checks and optimization across trajectories accounting for the robot's redundancy, and introducing a manipulability-aware control strategy. Leveraging a unified quadratic programming control with vector-field inequalities, our method enables robust tracking and optimization of manipulability, accommodating multiple demonstrations and the inherent variability in task execution. Experimental results demonstrate superior performance in precise tracking and force generation compared to traditional methods, highlighting the advantages of incorporating human implicit information for more effective robot control."
GS-EVT: Cross-Modal Event Camera Tracking Based on Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11128190,"Reliable self-localization is a foundational skill for many intelligent mobile platforms. This paper explores the use of event cameras for motion tracking thereby providing a solution with inherent robustness under difficult dynamics and illumination. In order to circumvent the challenge of event camera-based mapping, the solution is framed in a cross-modal way. It tracks a map representation that comes directly from frame-based cameras. Specifically, the proposed method operates on top of gaussian splatting, a state-of-the-art representation that permits highly efficient and realistic novel view synthesis. The key of our approach consists of a novel pose parametrization that uses a reference pose plus first order dynamics for local differential image rendering. The latter is then compared against images of integrated events in a staggered coarse-to-fine optimization scheme. As demonstrated by our results, the realistic view rendering ability of gaussian splatting leads to stable and accurate tracking across a variety of both publicly available and newly recorded data sequences."
A Coarse-to-Fine Event-based Framework for Camera Pose Relocalization with Spatio-Temporal Retrieval and Refinement Network.,https://doi.org/10.1109/ICRA55743.2025.11127875,"Most existing event-based camera pose relocalization (CPR) learning methods implicitly encode environmental information into network parameters to achieve end-to-end mapping from event stream to pose. However, these end-to-end CPR methods fail to utilize prior environmental information effectively. As the scale of the environment increases, the difficulty of this mapping relationship grows significantly, reducing the robustness of the end-to-end methods across different scenarios. To address the above issues, this paper proposes the first coarse-to-fine event-based CPR framework, which achieves a new paradigm from end-to-end pose regression network to a hierarchical approach. In the coarse localization stage, we effectively encode similarity features by incorporating the fine-grained temporal information, achieving accurate retrieval of nearby event stream. In the pose refinement stage, we present an Event Spatio-temporal Pose Refinement Network (ESPR-Net) based on the Recurrent Convolutional Neural Networks (RCNN) architecture, which is capable of learning more nu-anced spatio-temporal features to achieve accurate regression of the relative pose. Finally, we conducted a comprehensive comparison on the IJRR and M3ED dataset, achieving state-of-the-art (SOTA) performance on both. Notably, our method attains a significant 83 % performance improvement on the outdoor M3ED dataset."
Digital Beamforming Enhanced Radar Odometry.,https://doi.org/10.1109/ICRA55743.2025.11127292,"Radar has become an essential sensor for autonomous navigation, especially in challenging environments where camera and LiDAR sensors fail. 4D single-chip millimeter-wave radar systems, in particular, have drawn increasing attention thanks to their ability to provide spatial and Doppler information with low hardware cost and power consumption. However, most single-chip radar systems using traditional signal processing, such as Fast Fourier Transform, suffer from limited spatial resolution in radar detection, significantly limiting the performance of radar-based odometry and Simultaneous Localization and Mapping (SLAM) systems. In this paper, we develop a novel radar signal processing pipeline that integrates spatial domain beamforming techniques, and extend it to 3D Direction of Arrival estimation. Experiments using public datasets are conducted to evaluate and compare the performance of our proposed signal processing pipeline against traditional methodologies. These tests specifically focus on assessing structural precision across diverse scenes and measuring odometry accuracy in different radar odometry systems. This research demonstrates the feasibility of achieving more accurate radar odometry by simply replacing the standard FFT-based processing with the proposed pipeline. The codes are available at GitHub**https://github.com/SenseRoboticsLab/DBE-Radar."
Fast Global Localization on Neural Radiance Field.,https://doi.org/10.1109/ICRA55743.2025.11128297,"Neural Radiance Fields (NeRF) presented a novel way to represent scenes, allowing for high-quality 3D reconstruction from 2D images. Following its remarkable achievements, global localization within NeRF maps is an essential task for enabling a wide range of applications. Recently, Loc-NeRF demonstrated a localization approach that combines traditional Monte Carlo Localization with NeRF, showing promising results for using NeRF as an environment map. However, despite its advancements, Loc-NeRF encounters the challenge of a time-intensive ray rendering process, which can be a significant limitation in practical applications. To address this issue, we introduce Fast Loc-NeRF, which enhances efficiency and accuracy in NeRF map-based global localization. We propose a particle rejection weighting strategy that estimates the uncertainty of particles by leveraging NeRF's inherent characteristics and incorporates them into the particle weighting process to reject abnormal particles. Additionally, Fast Loc-NeRF employs a coarse-to-fine approach, matching rendered pixels and observed images across multiple resolutions from low to high. As a result, it speeds up the costly particle update process while enhancing precise localization results. Our Fast Loc-NeRF establishes new state-of-the-art localization performance on several bench-marks, demonstrating both its accuracy and efficiency. The code is available at this url."
HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for Multi-Session Radar SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128678,"Recently, radars have been widely featured in robotics for their robustness in challenging weather conditions. Two commonly used radar types are spinning radars and phased-array radars, each offering distinct sensor characteristics. Existing datasets typically feature only a single type of radar, leading to the development of algorithms limited to that specific kind. In this work, we highlight that combining different radar types offers complementary advantages, which can be leveraged through a heterogeneous radar dataset. Moreover, this new dataset fosters research in multi-session and multirobot scenarios where robots are equipped with different types of radars. In this context, we introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering unparalleled localization, mapping, and place recognition capabilities. The dataset covers diverse weather and lighting conditions and a range of urban traffic scenarios, enabling a comprehensive analysis across various environments. The sequence paths with multiple revisits and ground truth pose for each sensor enhance its suitability for place recognition research. We expect the HeRCULES dataset to facilitate odometry, mapping, place recognition, and sensor fusion research. The dataset and development tools are available at https://sites.google.com/view/herculesdataset."
NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments.,https://doi.org/10.1109/ICRA55743.2025.11128789,"Visual place recognition (VPR) enables autonomous robots to identify previously visited locations, which contributes to tasks like simultaneous localization and mapping (SLAM). VPR faces challenges such as accurate image neighbor retrieval and appearance change in scenery. Event cameras, also known as dynamic vision sensors, are a new sensor modality for VPR and offer a promising solution to the challenges with their unique attributes: high temporal resolution (1MHz clock), ultra-low latency (in s), and high dynamic range (>120dB). These attributes make event cameras less susceptible to motion blur and more robust in variable lighting conditions, making them suitable for addressing VPR challenges. However, the scarcity of event-based VPR datasets, partly due to the novelty and cost of event cameras, hampers their adoption. To fill this data gap, our paper introduces the NYC-Event-VPR dataset to the robotics and computer vision communities, featuring the Prophesee IMX636 HD event sensor (1280x720 resolution), combined with RGB camera and GPS module. It encompasses over 13 hours of geotagged event data, spanning 260 kilometers across New York City, covering diverse lighting and weather conditions, day/night scenarios, and multiple visits to various locations. Furthermore, our paper employs three frameworks to conduct generalization performance assessments, promoting innovation in event-based VPR and its integration into robotics applications."
ZeroSCD: Zero-Shot Street Scene Change Detection.,https://doi.org/10.1109/ICRA55743.2025.11128082,"Scene Change Detection is a challenging task in computer vision and robotics that aims to identify differences between two images of the same scene captured at different times. Traditional change detection methods rely on training models that take these image pairs as input and estimate the changes, which requires large amounts of annotated data, a costly and time-consuming process. To overcome this, we propose ZeroSCD, a zero-shot scene change detection framework that eliminates the need for training. ZeroSCD leverages pre-existing models for place recognition and semantic segmentation, utilizing their features and outputs to perform change detection. In this framework, features extracted from the place recognition model are used to estimate correspondences and detect changes between the two images. These are then combined with segmentation results from the semantic segmentation model to precisely delineate the boundaries of the detected changes. Extensive experiments on benchmark datasets demonstrate that ZeroSCD outperforms several state-of-the-art methods in change detection accuracy, despite not being trained on any of the benchmark datasets, proving its effectiveness and adaptability across different scenarios."
Shared Control for Cable Routing with Tactile Sensing.,https://doi.org/10.1109/ICRA55743.2025.11127322,"Multi-stage deformable linear object manipulation, such as cable routing, is the common and necessary part of human life and industry. However, autonomous robots still lack the dexterity and generalization required for these complex tasks. Direct teleoperation is an alternative approach, but the absence of reliable force and haptic feedback methods undermines its robustness and efficiency. This paper proposes a shared control method based on tactile sensing to address a multi-stage, contact-rich cable routing task. The proposed method allows human and robotic autonomy to share control of the robot platform. An action primitive vocabulary is constructed, incorporating adaptive authority allocation between human and autonomy, to generate motions for specific task stages. These allocations modulate the control weights of human and autonomy in accordance with the requirements of task stages. The method selects primitives from this vocabulary based on the tactile data and human intention. The effectiveness of our approach is demonstrated through a task involving straightening a cable and slotting it into a clip. We compare its performance with alternative methods and present that our method has a higher success rate and takes less time than direct teleoperation."
Whisker-Based Active Tactile Perception for Contour Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128114,"Perception using whisker-inspired tactile sensors currently faces a major challenge: the lack of active control in robots based on direct contact information from the whisker. To accurately reconstruct object contours, it is crucial for the whisker sensor to continuously follow and maintain an appropriate relative touch pose on the surface. This is especially important for localization based on tip contact, which has a low tolerance for sharp surfaces and must avoid slipping into tangential contact. In this paper, we first construct a magnetically transduced whisker sensor featuring a compact and robust suspension system composed of three flexible spiral arms. We develop a method that leverages a characterized whisker deflection profile to directly extract the tip contact position using gradient descent, with a Bayesian filter applied to reduce fluctuations. We then propose an active motion control policy to maintain the optimal relative pose of the whisker sensor against the object surface. A B-Spline curve is employed to predict the local surface curvature and determine the sensor orientation. Results demonstrate that our algorithm can effectively track objects and reconstruct contours with sub-millimeter accuracy. Finally, we validate the method in simulations and real-world experiments where a robot arm drives the whisker sensor to follow the surfaces of three different objects."
CDM: Contact Diffusion Model for Multi-Contact Point Localization.,https://doi.org/10.1109/ICRA55743.2025.11127780,"In this paper, we propose a Contact Diffusion Model (CDM), a novel learning-based approach for multi-contact point localization. We consider a robot equipped with joint torque sensors and a force/torque sensor at the base. By leveraging a diffusion model, CDM addresses the singularity where multiple pairs of contact points and forces produce identical sensor measurements. We formulate CDM to be conditioned on past model outputs to account for the time-dependent characteristics of the multi-contact scenarios. Moreover, to effectively address the complex shape of the robot surfaces, we incorporate the signed distance field in the denoising process. Consequently, CDM can localize contacts at arbitrary locations with high accuracy. Simulation and real-world experiments demonstrate the effectiveness of the proposed method. In particular, CDM operates at 15.97ms and, in the real world, achieves an error of 0.44cm in single-contact scenarios and 1.24cm in dual-contact scenarios."
Force Admittance Control of an Underactuated Gripper with Full-State Feedback.,https://doi.org/10.1109/ICRA55743.2025.11127415,"We present admittance control and fingertip contact detection with a linkage gripper remotely driven by a pneumatic rolling diaphragm actuator. The gripper is driven by underactuated mechanisms sensorized by joint encoders in order to fully determine the gripper state. We present the modelling of the linkage and fluidic transmission, validate its ability to regulate pinch force via admittance control within an RMS error well under 0.5 Newtons, and show the ability to detect contact at targeted locations on the linkage. In addition, we demonstrate simple grasping behaviors: blindly searching for an unobstructed object and detecting object loss. Our results show that an integrative approach of instrumenting underactuated gripper mechanisms can result in a lightweight gripper that is not only mechanically adaptive but sensitive enough to react to contact events without distal sensors or vision."
GenTact Toolbox: A Computational Design Pipeline to Procedurally Generate Context-Driven 3D Printed Whole-Body Artificial Skins.,https://doi.org/10.1109/ICRA55743.2025.11128675,"Developing whole-body tactile skins for robots remains a challenging task, as existing solutions often prioritize modular, one-size-fits-all designs, which, while versatile, fail to account for the robot's specific shape and the unique demands of its operational context. In this work, we introduce GenTact Toolbox, a computational pipeline for creating versatile wholebody tactile skins tailored to both robot shape and application domain. Our method includes procedural mesh generation for conforming to a robot's topology, task-driven simulation to refine sensor distribution, and multi-material 3D printing for shape-agnostic fabrication. We validate our approach by creating and deploying six capacitive sensing skins on a Franka Research 3 robot arm in a human-robot interaction scenario. This work represents a shift from one-size-fits-all tactile sensors toward context-driven, highly adaptable designs that can be customized for a wide range of robotic systems and applications. The project website is available at https://hiro-group.ronc.one/gentacttoolbox"
Interactive Motion Planning for a 7-DOF Robot.,https://doi.org/10.1109/ICRA55743.2025.11128865,"The use of robots in high-risk and extreme environments is crucial for tasks that are dangerous or inaccessible to humans and require high precision. Particularly in scenarios where the cost of failure is high, remote human teleoperation can be the preferred method of robot control due to the adaptability and high-level decision making of humans. Teleoperation brings many challenges including lack of accurate prior knowledge about the environment, limited views of the environment by on-board sensors, and especially inconsistent latency. 7-DOF (degrees of freedom) manipulators provide redundancy which can be utilized for increased flexibility in manipulation, and may be preferred to 6-DOF manipulators in many scenarios. The redundancy, however, must be considered by the teleoperation system. We present an extension to an existing Interactive Planning and Supervised Execution (IPSE) system that enables full teleoperation of a 7-DOF robot by encoding the redundant degree of freedom with a Shoulder-Elbow-Wrist (SEW) angle, which is user-manipulable via an SEW angle graph. Additionally, we introduce a novel user interface feature that encodes robot state information into a 2D image which is displayed directly on the SEW angle graph. We conduct a user-study which demonstrates that the addition of this SEW graph significantly reduces task completion time."
"A Hybrid User Interface Combining AR, Desktop, and Mobile Interfaces for Enhanced Industrial Robot Programming.",https://doi.org/10.1109/ICRA55743.2025.11128291,"Robot programming for complex assembly tasks is challenging and demands expert knowledge. With Augmented Reality (AR), immersive 3D visualization can be placed in the robot's intrinsic coordinate system to support robot programming. However, AR interfaces introduce usability challenges. To address these, we introduce a hybrid user interface (HUI) that combines a 2D desktop, a smartphone, and an AR head-mounted display (HMD) application, enabling operators to choose the most suitable device for each sub-task. The evaluation with an expert user study shows that an HUI can enhance efficiency and user experience by selecting the appropriate device for each sub-task. Generally, the HMD is preferred for tasks involving 3D content, the desktop for creating the program structure and parametrization, and the smartphone for mobile parametrization. However, the device selection depends on individual user characteristics and their familiarity with the devices."
Enhancing AR-to-Robot Registration Accuracy: A Comparative Study of Marker Detection Algorithms and Registration Parameters.,https://doi.org/10.1109/ICRA55743.2025.11128039,"Augmented Reality (AR) offers potential for enhancing human-robot collaboration by enabling intuitive interaction and real-time feedback. A crucial aspect of AR-robot integration is accurate spatial registration to align virtual content with the physical robotic workspace. This paper systematically investigates the effects of different tracking techniques and registration parameters on AR-to-robot registration accuracy, focusing on paired-point methods. We evaluate four marker detection algorithms - ARToolkit, Vuforia, ArUco, and retroreflective tracking - analyzing the influence of viewing distance, angle, marker size, point distance, distribution, and quantity. Our results show that ARToolkit provides the highest registration accuracy. While larger markers and positioning registration point centroids close to target locations consistently improved accuracy, other factors such as point distance and quantity were highly dependent on the tracking techniques used. Additionally, we propose an effective refinement method using point cloud registration, significantly improving accuracy by integrating data from points recorded between registration locations. These findings offer practical guidelines for enhancing AR-robot registration, with future work needed to assess the transferability to other AR devices and robots."
Sketch-MoMa: Teleoperation for Mobile Manipulator via Interpretation of Hand-Drawn Sketches.,https://doi.org/10.1109/ICRA55743.2025.11128261,"To use assistive robots in everyday life, a remote control system with common devices, such as 2D devices, is helpful to control the robots anytime and anywhere as intended. Hand-drawn sketches are one of the intuitive ways to control robots with 2D devices. However, since similar sketches have different intentions from scene to scene, existing work requires additional modalities to set the sketches' semantics. This requires complex operations for users and leads to decreasing usability. In this paper, we propose Sketch-MoMa, a teleoperation system using user-given hand-drawn sketches as instructions to control a robot. We use Vision-Language Models (VLMs) to understand the user-given sketches superimposed on an observation image and infer drawn shapes and low-level tasks of the robot. We utilize sketches and the generated shapes for recognition and motion planning of the generated low-level tasks for precise and intuitive operations. We validate our approach using state-of-the-art VLMs with 7 tasks and 5 sketch shapes. We also demonstrate that our approach effectively specifies more detailed intentions, such as how to grasp and how much to rotate. Moreover, we show the competitive usability of our approach compared with the existing 2D interface through a user experiment with 14 participants. Our videos and results are available at this link."
Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V.,https://doi.org/10.1109/ICRA55743.2025.11127975,"Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\sim 35 \%$</tex>) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning."
Optimizing Robot Programming: Mixed Reality Gripper Control.,https://doi.org/10.1109/ICRA55743.2025.11128349,"Conventional robot programming methods are complex and time-consuming for users. In recent years, alternative approaches such as mixed reality have been explored to address these challenges and optimize robot programming. While the findings of the mixed reality robot programming methods are convincing, most existing methods rely on gesture interaction for robot programming. Since controller-based interactions have proven to be more reliable, this paper examines three controller-based programming methods within a mixed reality scenario: 1) Classical Jogging, where the user positions the robot's end effector using the controller's thumbsticks, 2) Direct Control, where the controller's position and orientation directly corresponds to the end effector's, and 3) Gripper Control, where the controller is enhanced with a 3D-printed gripper attachment to grasp and release objects. A within-subjects study <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(n = 30)$</tex> was conducted to compare these methods. The findings indicate that the Gripper Control condition outperforms the others in terms of task completion time, user experience, mental demand, and task performance, while also being the preferred method. Therefore, it demonstrates promising potential as an effective and efficient approach for future robot programming. Video available at https://youtu.be/83kWr8zUFIQ."
FLEX: A Framework for Learning Robot-Agnostic Force-Based Skills Involving Sustained Contact Object Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127866,"Learning to manipulate objects efficiently, particularly those involving sustained contact (e.g., pushing, sliding) and articulated parts (e.g., drawers, doors), presents significant challenges. Traditional methods, such as robot-centric reinforce-ment learning (RL), imitation learning, and hybrid techniques, require massive training and often struggle to generalize across different objects and robot platforms. We propose a novel framework for learning object-centric manipulation policies in force space, decoupling the robot from the object. By directly applying forces to selected regions of the object, our method simplifies the action space, reduces unnecessary exploration, and decreases simulation overhead. This approach, trained in simulation on a small set of representative objects, captures ob-ject dynamicssuch as joint configurationsallowing policies to generalize effectively to new, unseen objects. Decoupling these policies from robot-specific dynamics enables direct transfer to different robotic platforms (e.g., Kinova, Panda, URS) with-out retraining. Our evaluations demonstrate that the method significantly outperforms baselines, achieving over an order of magnitude improvement in training efficiency compared to other state-of-the-art methods. Additionally, operating in force space enhances policy transferability across diverse robot plat-forms and object types. We further showcase the applicability of our method in a real-world robotic setting. Link: https://tufts-ai-robotics-group.github.io/FLEX/"
Flora: Sample-Efficient Preference-Based Rl Via Low-Rank Style Adaptation of Reward Functions.,https://doi.org/10.1109/ICRA55743.2025.11127633,"Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks. We provide videos of our results and source code at https://sites.google.com/view/preflora/."
On-Robot Reinforcement Learning with Goal-Contrastive Rewards.,https://doi.org/10.1109/ICRA55743.2025.11128466,"Reinforcement Learning (RL) has the potential to enable robots to learn from their own actions in the real world. Unfortunately, RL can be prohibitively expensive, in terms of on-robot runtime, due to inefficient exploration when learning from a sparse reward signal. Designing dense reward functions is labour-intensive and requires domain expertise. In our work, we propose Goal-Contrastive Rewards (GCR), a dense reward function learning method that can be trained on passive video demonstrations. By using videos without actions, our method is easier to scale, as we can use arbitrary videos. GCR combines two loss functions, an implicit value loss function that models how the reward increases when traversing a successful trajectory, and a goal-contrastive loss that discriminates between successful and failed trajectories. We perform experiments in simulated manipulation environments across RoboMimic and MimicGen tasks, as well as in the real world using a Franka arm and a Spot quadruped. We find that GCR leads to a more-sample efficient RL, enabling model-free RL to solve about twice as many tasks as our baseline reward learning methods. We also demonstrate positive cross-embodiment transfer from videos of people and of other robots performing a task. Website: https://gcr-robot.github.io/."
"Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control.",https://doi.org/10.1109/ICRA55743.2025.11128365,"Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer (Fig. 1). To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84% success rate in the real world via extensive experiments with various unseen objects. Webpage: https://watch-less-feel-more.github.io/"
Fast Policy Synthesis with Variable Noise Diffusion Models.,https://doi.org/10.1109/ICRA55743.2025.11127858,"Diffusion models have seen rapid adoption in robotic imitation learning, enabling autonomous execution of complex dexterous tasks. However, action synthesis is often slow, requiring many steps of iterative denoising, limiting the extent to which models can be used in tasks that require fast reactive policies. To sidestep this, recent works have explored how the distillation of the diffusion process can be used to accelerate policy synthesis. However, distillation is computationally expensive and can hurt both the accuracy and diversity of synthesized actions. We propose SDP (Streaming Diffusion Policy), an alternative method to accelerate policy synthesis, leveraging the insight that generating a partially denoised action trajectory is substantially faster than a full output action trajectory. At each observation, our approach outputs a partially denoised action trajectory with variable levels of noise corruption, where the immediate action to execute is noise-free, with subsequent actions having increasing levels of noise and uncertainty. The partially denoised action trajectory for a new observation can then be quickly generated by applying a few steps of denoising to the previously predicted noisy action trajectory (rolled over by one timestep). We illustrate the efficacy of this approach, dramatically speeding up policy synthesis while preserving performance across both simulated and real-world settings. Project website: https://streaming-diffusion-policy.github.io."
Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control.,https://doi.org/10.1109/ICRA55743.2025.11128452,"Compliance plays a crucial role in manipulation, as it balances between the concurrent control of position and force under uncertainties. Yet compliance is often overlooked by today's visuomotor policies that solely focus on position control. This paper introduces Adaptive Compliance Policy (ACP), a novel framework that learns to dynamically adjust system com-pliance both spatially and temporally for given manipulation tasks from human demonstrations, improving upon previous approaches that rely on pre-selected compliance parameters or assume uniform constant stiffness. However, computing full compliance parameters from human demonstrations is an ill- defined problem. Instead, we estimate an approximate compli-ance profile with two useful properties: avoiding large contact forces and encouraging accurate tracking. Our approach en-ables robots to handle complex contact-rich manipulation tasks and achieves over 50% performance improvement compared to state-of-the-art visuomotor policy methods. Project website with result videos: adaptive-compliance.github.io."
Learning Wheelchair Tennis Navigation from Broadcast Videos with Domain Knowledge Transfer and Diffusion Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127909,"In this paper, we propose a novel and generalizable zero-shot knowledge transfer framework that distills expert sports navigation strategies from web videos into robotic systems with adversarial constraints and out-of-distribution image trajectories. Our pipeline enables diffusion-based imitation learning by reconstructing the full 3D task space from multiple partial views, warping it into 2D image space, closing the planning loop within this 2D space, and transfer constrained motion of interest back to task space. Additionally, we demonstrate that the learned policy can serve as a local planner in conjunction with position control. We apply this framework in the wheelchair tennis navigation problem to guide the wheelchair into the ball-hitting region. Our pipeline achieves a navigation success rate of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{9 7. 6 7 \%}$</tex> in reaching real-world recorded tennis ball trajectories with a physical robot wheelchair, and achieve a success rate of 68.49% in a real-world, real-time experiment on a full-sized tennis court<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup><sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>Code is at https://github.gatech.edu/MCG-Lab/tennis_gameplay_learning."
Diff-Dagger: Uncertainty Estimation With Diffusion Policy for Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127730,"Recently, diffusion policy has shown impressive results in handling multi-modal tasks in robotic manipulation. However, it has fundamental limitations in out-of-distribution failures that persist due to compounding errors and its limited capability to extrapolate. One way to address these limitations is robot-gated DAgger, an interactive imitation learning with a robot query system to actively seek expert help during policy rollout. While robot-gated DAgger has high potential for learning at scale, existing methods like Ensemble-DAgger struggle with highly expressive policies: They often misinterpret policy disagreements as uncertainty at multi-modal decision points. To address this problem, we introduce Diff-DAgger, an efficient robot-gated DAgger algorithm that leverages the training objective of diffusion policy. We evaluate Diff-DAgger across different robot tasks including stacking, pushing, and plugging, and show that Diff-DAgger improves the task failure prediction by 39.0 %, the task completion rate by 20.6 %, and reduces the wall-clock time by a factor of 7.8. We hope that this work opens up a path for efficiently incorporating expressive yet data-hungry policies into interactive robot learning settings. The project website is available at: https://diffdagger.github.io."
SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127562,"We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the SE(3) object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as crossembodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually-crafted rules. To guide the robot in executing the task, the object trajectory is used to condition a diffusion policy. We systematically evaluate our method on simulation and real-world tasks. In real-world evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/object_centric_diffusion"
Imitation Learning with Limited Actions via Diffusion Planners and Deep Koopman Controllers.,https://doi.org/10.1109/ICRA55743.2025.11128735,"Recent advances in diffusion-based robot policies have demonstrated significant potential in imitating multi-modal behaviors. However, these approaches typically require large quantities of demonstration data paired with corresponding robot action labels, creating a substantial data collection burden. In this work, we propose a plan-then-control framework aimed at improving the action-data efficiency of inverse dynamics controllers by leveraging observational demonstration data. Specifically, we adopt a Deep Koopman Operator framework to model the dynamical system and utilize observation-only trajectories to learn a latent action representation. This latent representation can then be effectively mapped to real high-dimensional continuous actions using a linear action decoder, requiring minimal action-labeled data. Through experiments on simulated robot manipulation tasks and a real robot experiment with multi-modal expert demonstrations, we demonstrate that our approach significantly enhances action-data efficiency and achieves high task success rates with limited action data."
H3O: Hyper-Efficient 3D Occupancy Prediction with Heterogeneous Supervision.,https://doi.org/10.1109/ICRA55743.2025.11128732,"3D occupancy prediction has recently emerged as a new paradigm for holistic 3D scene understanding and provides valuable information for downstream planning in autonomous driving. Most existing methods, however, are computationally expensive, requiring costly attention-based 2D- 3D transformation and 3D feature processing. In this paper, we present a novel 3D occupancy prediction approach, H30, which features highly efficient architecture designs that incur a significantly lower computational cost as compared to the current state-of-the-art methods. In addition, to compensate for the ambiguity in ground-truth 3D occupancy labels, we advocate leveraging auxiliary tasks to complement the direct 3D supervision. In particular, we integrate multi-camera depth estimation, semantic segmentation, and surface normal estimation via differentiable volume rendering, supervised by corresponding 2D labels that introduces rich and heterogeneous supervision signals. We conduct extensive experiments on the Occ3D-nuScenes and SemanticKITTI benchmarks that demonstrate the superiority of our proposed H30."
TrackOcc: Camera-Based 4D Panoptic Occupancy Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128437,"Comprehensive and consistent dynamic scene understanding from camera input is essential for advanced autonomous systems. Traditional camera-based perception tasks like 3D object tracking and semantic occupancy prediction lack either spatial comprehensiveness or temporal consistency. In this work, we introduce a brand-new task, Camera-based 4D Panoptic Occupancy Tracking, which simultaneously addresses panoptic occupancy segmentation and object tracking from camera-only input. Furthermore, we propose TrackOcc, a cutting-edge approach that processes image inputs in a streaming, end-to-end manner with 4D panoptic queries to address the proposed task. Leveraging the localization-aware loss, TrackOcc enhances the accuracy of 4D panoptic occupancy tracking without bells and whistles. Experimental results demonstrate that our method achieves state-of-the-art performance on the Waymo dataset. The source code will be released at https://github.com/Tsinghua-MARS-Lab/TrackOcc."
RadarMask: A Novel End-to-End Sparse Millimeter-Wave Radar Sequence Panoptic Segmentation and Tracking Method.,https://doi.org/10.1109/ICRA55743.2025.11128555,"In the realms of autonomous driving and robotics, radar sensors are garnering growing interest. Scene understanding is crucial for the safe navigation of autonomous systems. Panoptic segmentation and tracking tasks enable the dynamic, semantic multilevel description of the environment surrounding vehicles and different instances. However, previous panoptic segmentation and tracking methods have primarily focused on LiDAR. To tackle the complex challenge of panoptic segmentation and tracking for radar data, we introduce RadarMask, an innovative method that addresses this issue for the first time within the radar domain. Our approach is end-to-end, requiring no post-processing. We also introduce simple and effective point cloud feature modules and target motion estimation modules tailored to the unique characteristics of radar points. Finally, we demonstrate the effectiveness of our algorithm on the RadarScenes dataset, achieving state-of-the-art performance in comparisons. The implementation of our method can be found at: https://github.com/yb-guo/RadarMask."
Enhancing Autonomous Navigation by Imaging Hidden Objects Using Single-Photon LiDAR.,https://doi.org/10.1109/ICRA55743.2025.11128292,"Robust autonomous navigation in environments with limited visibility remains a critical challenge in robotics. We present a novel approach that leverages Non-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve visibility and enhance autonomous navigation. Our method enables mobile robots to see around corners by utilizing multi-bounce light information, effectively expanding their perceptual range without additional infrastructure. We propose a three-module pipeline: (1) Sensing, which captures multi-bounce histograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy maps of hidden regions from these histograms using a convolutional neural network; and (3) Control, which allows a robot to follow safe paths based on the estimated occupancy. We evaluate our approach through simulations and real-world experiments on a mobile robot navigating an L-shaped corridor with hidden obstacles. Our work represents the first experimental demonstration of NLOS imaging for autonomous navigation, paving the way for safer and more efficient robotic systems operating in complex environments. We also contribute a novel dynamics-integrated transient rendering framework for simulating NLOS scenarios, facilitating future research in this domain."
Visual-Based Forklift Learning System Enabling Zero-Shot Sim2Real Without Real-World Data.,https://doi.org/10.1109/ICRA55743.2025.11127682,"Forklifts are used extensively in various industrial settings and are in high demand for automation. In particular, counterbalance forklifts are highly versatile and are employed in diverse scenarios. However, efforts to automate these processes are lacking, primarily owing to the absence of a safe and performance-verifiable development environment. This study proposes a learning system that combines a photorealistic digital learning environment with a <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1 / 14$</tex>-scale robotic forklift environment to address this challenge. Inspired by the training-based learning approach adopted by forklift operators, we employ an end-to-end vision-based deep reinforcement learning approach. The learning is conducted in a digitalized environment created from CAD data, making it safe and eliminating the need for real-world data. In addition, we safely validate the method in a physical setting using a <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$1 / 14$</tex>-scale robotic forklift with a configuration similar to that of a real forklift. We achieved a 60% success rate in pallet loading tasks in real experiments using a robotic forklift. Our approach demonstrates zero-shot sim2real with a simple method that does not require heuristic additions. This learning-based approach is considered a first step towards the automation of counterbalance forklifts."
Strategic System Design for High Precision in Assembly Processes of CPU.,https://doi.org/10.1109/ICRA55743.2025.11128746,"Robotic picking and placing played an essential role in Industrial 4.0 and have long been recognized as significant contributions to industrial processes. Various scenarios involve picking and placing parts for assembly in industrial production, such as assembling different electronic components in the manufacturing process. Those tasks require high precision to complete. However, achieving high precision in the assembly of CPUs poses a significant challenge, particularly when dealing with reflective surfaces. This paper presents a strategic system design tailored to address these challenges effectively. We focus on system device choice and optimizing the key parameters of the sensor system to strike a balance between device cost and the required precision. We use methods to construct the whole robot manipulation system, such as geometric segmentation, binocular vision with structure light projection and, based on 3D information, 6D pose estimation to construct the system. The results of our study demonstrate the practical applicability and benefits of this strategic system design in industrial settings. By meeting strict system accuracy requirements, our approach contributes to advancing industry practices and growing its impact on society."
The Influence of Counterbalance System on the Dynamic Characterization of Heavy Industrial Robots.,https://doi.org/10.1109/ICRA55743.2025.11128332,"The precision of industrial robots is often limited by the relatively low stiffness of their joints, leading to positioning errors influenced by factors such as the mass and inertia of robotic links, external forces, and the counterbalance system (CBS). Counterbalance systems, typically consisting of hydropneumatic cylinders, are designed to reduce motor torque and assist in supporting heavier links. Traditionally, positioning errors in industrial robots have been corrected statically by determining pose-dependent stiffness values. However, recent numerical models incorporate inertial effects to improve positioning error correction, making accurate inertial parameter identification essential. These parameters are typically unknown and must be determined experimentally. While methodologies for inertial parameter estimation have been extensively studied, none have accounted for the effect of the counterbalance system in this process. To address this gap, a methodology for estimating inertial parameters was applied to a heavy industrial robot, considering the influence of the counterbalance system. A comparative analysis with and without the counterbalance system showed that its inclusion improved joint torque calculation accuracy, showing the necessity of considering it in dynamic parameter characterization methodologies."
Deep Learning-Based Friction Compensation in Low Velocity for Enhanced Direct Teaching in Collaborative Manipulators.,https://doi.org/10.1109/ICRA55743.2025.11128191,"Direct teaching in collaborative manipulators, an essential method for intuitive trajectory control, faces significant challenges due to friction in robot joints. To address this, we present a novel friction compensation framework to improve direct teaching methods for robots. Our approach focuses on mitigating friction in the joints most susceptible to frictional effects, ensuring smoother and more precise motion. The proposed framework uses deep neural networks (DNN) to model the complex friction behavior. This approach circumvents the difficulties associated with traditional friction compensation model selection. We develop specific data input preprocessing algorithms that optimize friction estimation when paired with standard encoders commonly used in collaborative robots. In addition, our custom loss function is specifically designed to improve DNN training in these low-velocity regions. To evaluate the effectiveness of our framework, we conduct comprehensive ablation studies assessing the impact of two critical components: the preprocessing algorithms and the custom loss function. These studies provide insight into the contributions of each element to overall performance. Experimental validation using two 6-DoF collaborative robots demonstrates the practical applicability and effectiveness of our approach."
Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing.,https://doi.org/10.1109/ICRA55743.2025.11127320,"Due to high dimensionality and non-convexity, real-time optimal control using full-order dynamics models for legged robots is challenging. Therefore, Nonlinear Model Predictive Control (NMPC) approaches are often limited to reduced-order models or local approximations. Sampling-based MPC has shown potential in nonconvex even discontinuous problems, but often yields suboptimal solutions with high variance, which limits its applications in high-dimensional locomotion. This work introduces DIAL-MPC (Diffusion-Inspired Annealing for Legged MPC), a sampling-based MPC framework with a novel diffusion-style annealing process. Such a process is supported by the theoretical landscape analysis of Model Predictive Path Integral Control (MPPI) and the connection between MPPI and single-step diffusion. Algorithmically, DIALMPC iteratively refines solutions online and achieves both global coverage and local convergence. In quadrupedal torquelevel control tasks, DIAL-MPC reduces the tracking error of standard MPPI by 13.4 times and outperforms reinforcement learning (RL) policies by 50 % in challenging climbing tasks without any training. In particular, DIAL-MPC enables precise real-world quadrupedal jumping with payload. To the best of our knowledge, DIAL-MPC is the first training-free method that optimizes over full-order legged dynamics in real-time."
"$\mathcal{D}(\mathcal{R}, \mathcal{O})$ Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping.",https://doi.org/10.1109/ICRA55743.2025.11127754,"Dexterous grasping is a fundamental yet challenging skill in robotic manipulation, requiring precise interaction between robotic hands and objects. In this paper, we present <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathcal{D}(\mathcal{R}, \mathcal{O})$</tex> Grasp, a novel framework that models the interaction between the robotic hand in its grasping pose and the object, enabling broad generalization across various robot hands and object geometries. Our model takes the robot hand's description and object point cloud as inputs and efficiently predicts kinematically valid and stable grasps, demonstrating strong adaptability to diverse robot embodiments and object geometries. Extensive experiments conducted in both simulated and real-world environments validate the effectiveness of our approach, with significant improvements in success rate, grasp diversity, and inference speed across multiple robotic hands. Our method achieves an average success rate of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$87.53\%$</tex> in simulation in less than one second, tested across three different dexterous robotic hands. In real-world experiments using the LeapHand, the method also demonstrates an average success rate of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{8 9\%}. \mathcal{D}(\mathcal{R}, \mathcal{O})$</tex> Grasp provides a robust solution for dexterous grasping in complex and varied environments. The code, appendix, and videos are available on our project website at https://nus-lins-lab.github.io/drograspweb/."
TrofyBot: A Transformable Rolling and Flying Robot with High Energy Efficiency.,https://doi.org/10.1109/ICRA55743.2025.11128779,"Terrestrial and aerial bimodal vehicles have gained significant interest due to their energy efficiency and versatile maneuverability across different domains. However, most existing passive-wheeled bimodal vehicles rely on attitude regulation to generate forward thrust, which inevitably results in energy waste on producing lifting force. In this work, we propose a novel passive-wheeled bimodal vehicle called TrofyBot that can rapidly change the thrust direction with a single servo motor and a transformable parallelogram linkage mechanism (TPLM). Cooperating with a bidirectional force generation module (BFGM) for motors to produce bidirectional thrust, the robot achieves flexible mobility as a differential driven rover on the ground. This design achieves 95.37% energy saving efficiency in terrestrial locomotion, allowing the robot continuously move on the ground for more than two hours in current setup. Furthermore, the design obviates the need for attitude regulation and therefore provides a stable sensor field of view (FoV). We model the bimodal dynamics for the system, analyze its differential flatness property, and design a controller based on hybrid model predictive control for trajectory tracking. A prototype is built and extensive experiments are conducted to verify the design and the proposed controller, which achieves high energy efficiency and seamless transition between modes."
Geometric Design and Gait Co-Optimization for Soft Continuum Robots Swimming at Low and High Reynolds Numbers.,https://doi.org/10.1109/ICRA55743.2025.11128096,"Recent advancements in soft actuators have enabled soft continuum swimming robots to achieve higher efficiency and more closely mimic the behaviors of real marine animals. However, optimizing the design and control of these soft continuum robots remains a significant challenge. In this paper, we present a practical framework for the co-optimization of the design and control of soft continuum robots, approached from a geometric locomotion analysis perspective. This framework is based on the principles of geometric mechanics, accounting for swimming at both low and high Reynolds numbers. By generalizing geometric principles to continuum bodies, we achieve efficient geometric variational co-optimization of designs and gaits across different power consumption metrics and swimming environments. The resulting optimal designs and gaits exhibit greater efficiencies at both low and high Reynolds numbers compared to three-link or serpenoid swimmers with the same degrees of freedom, approaching or even surpassing the efficiencies of infinitely flexible swimmers and those with higher degrees of freedom."
ShadowTac: Dense Measurement of Shear and Normal Deformation of a Tactile Membrane from Colored Shadows.,https://doi.org/10.1109/ICRA55743.2025.11128441,"To robustly handle objects, robots must perceive mechanical interactions through touch with sufficient richness. New tactile sensors leverage miniature cameras to provide dense measurements of these interactions, allowing for the extraction of material properties and frictional information. Among the plethora of solutions, retrographic sensing is popular for its ability to finely resolve the shape of the object being touched. These sensors use a reflective membrane, illuminated at a shallow angle by three RGB lights from which fine details of the surface can be recovered. However, these retrographic sensors are unable to detect the lateral displacement of the membrane and, therefore overlook frictional information, which is crucial for grasping and manipulation. Embedding and tracking opaque markers has been a makeshift solution, but these markers occlude the membrane and are difficult to manufacture. In this paper, we introduce ShadowTac, a tactile sensor that combines retrographic illumination with non-intrusive markers created by colored shadows. We patterned the retrographic surface with a dense array of submillimeter dimples, which are small enough not to obstruct the view yet cast shadows large enough to be visible to the camera. ShadowTac captures a dense image of both the normal displacement field with fine details and a precise lateral displacement field by tracking the markers. Additionally, our sensor is easy to manufacture, as the dimple pattern can simply be molded. We evaluated the measurement reliability of ShadowTac and its effectiveness in estimating the incipient slip of arbitrary objects. The dense measurement of both the normal and shear deformation that the sensor captures makes it ideal for tracking dynamic interactions between robotic fingertips and manipulated objects."
Occlusion-Aware 6D Pose Estimation with Depth-Guided Graph Encoding and Cross-Semantic Fusion for Robotic Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128248,"Reliable 6D pose estimation is crucial for robotic tasks but presents significant challenges in environments with occlusion. Recent approaches tend to directly predict pose parameters of object with deep neural networks, lacking the modeling ability of non-adjacent and complex relationships of surface points in occluded scenarios. To solve this problem, we propose a novel occlusion-aware 6D pose estimation framework, which uses depth-guided graph neural network (GNN) to model potential relationships from RGBD input. Two semantic information, which are mask and binary code of object, are adaptively fused to extract 2D-3D correspondence related features in an effective manner. Both enhanced graph features and fused semantic information contribute to the performance improvement of pose estimation with occlusion. Extensive experiments indicate that our approach outperforms comparative methods by 1.2% and 1.9% on LMO and YCBV datasets (up to 30% for certain objects) and its validity is also verified under real-world pose estimation test."
JPG-SLAM: Joint Point-Gaussian Splatting Representation for Dense Dynamic SLAM.,https://doi.org/10.1109/ICRA55743.2025.11127881,"This paper presents a simultaneous localization and mapping (SLAM) system to provide accurate pose estimation and dynamic scene reconstruction. Our approach proposes a Joint Point-Gaussian Splatting representation, which fully integrates the robustness of isotropic feature points in pose estimation and the flexibility of anisotropic 3D Gaussians in scene representation. This system does not need to suppress the anisotropic representation of Gaussian elements, which enables the mapping module to achieve finer scene representation with lower memory consumption. Additionally, in order to enhance the adaptability of the system in dynamic environments, we introduced a dynamic region recognition module and utilized 3D Gaussian Splatting and 4D Gaussian Splatting representations to represent static and dynamic regions respectively. Furthermore, we developed a local map management strategy for Gaussian Splatting mapping, effectively reducing the memory and computational resource usage in the mapping process. Experiments on public datasets demonstrate that our system achieves state-of-the-art tracking and mapping accuracy compared to existing baselines."
Submodular Optimization for Keyframe Selection & Usage in SLAM.,https://doi.org/10.1109/ICRA55743.2025.11127770,"Keyframes are LiDAR scans saved for future reference in Simultaneous Localization And Mapping (SLAM), but despite their central importance most algorithms leave choices of which scans to save and how to use them to wasteful heuristics. This work proposes two novel keyframe selection strategies for localization and map summarization, as well as a novel approach to submap generation which selects keyframes that best constrain localization. Our results show that online keyframe selection and submap generation reduce the number of saved keyframes and improve per scan computation time without compromising localization performance. We also present a map summarization feature for quickly capturing environments under strict map size constraints."
Equivariant Filter Design for Range-Only SLAM.,https://doi.org/10.1109/ICRA55743.2025.11127384,"Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due to its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy (BLE) localisation in terrestrial and aerial applications and acoustic beacon localisation in submarine applications. In this work, we consider a mobile robot equipped with an inertial measurement unit (IMU) and a range sensor that measures distances to a collection of fixed landmarks. We derive an equivariant filter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is compatible with the range measurements. The proposed filter does not require bootstrapping or initialisation of landmark positions, and demonstrates robustness to the noprior situation. The filter is demonstrated on a real-world dataset, and it is shown to significantly outperform a state-of-the-art EKF alternative in terms of both accuracy and robustness."
Tension Dependent Twisted String Actuator Modelling and Efficacy Benchmarking in Force and Impedance Control.,https://doi.org/10.1109/ICRA55743.2025.11127389,"This study presents a comprehensive experimental analysis of Twisted String Actuators (TSA), focused on enhancing contraction modelling accuracy and establishing a baseline for TSA tension and impedance control efficacy. A novel TSA string radius function is introduced, computing effective radii for multi-strand bundles based on axial actuator tension. The proposed model was validated in physical experiments, resulting in a reduction of maximal errors between measured and simulated actuator contraction trajectories from up to 60 % in established models to around 10% in our work. Additionally, the tension-dependent radius modification effectively reduced errors between the estimated and the measured bundle tension by an order of magnitude, marking an essential step towards TSA control independent of bundle tension measurements. TSA tension control was assessed based on four metrics: accu-racy, precision, impact stability, and bandwidth, following ISO 9283:1998 standards. The quality of tension control was found to be dependent on bundle tension, twisting angle and strand quantity, whereas impact stability was maintained in all config-urations. Joint impedance control with TSA was evaluated for perturbation stability and position control bandwidth, where the latter was enhanced with increasing joint stiffness. The presented analysis informs designers about the capabilities of TSAs in different configurations, and their respective suitability for desired applications."
A Novel Twisted-Winching String Actuator for Robotic Applications: Design and Validation.,https://doi.org/10.1109/ICRA55743.2025.11127747,"This paper presents a novel actuator system combining a twisted string actuator (TSA) with a winch mechanism. Relative to traditional hydraulic and pneumatic systems in robotics, TSAs are compact and lightweight but face limitations in stroke length and force-transmission ratios. Our integrated TSA-winch system overcomes these constraints by providing variable transmission ratios through dynamic adjustment. It increases actuator stroke by winching instead of overtwisting, and it improves force output by twisting. The design features a rotating turret that houses a winch, which is mounted on a bevel gear assembly driven by a through-hole drive shaft. Mathematical models are developed for the combined displacement and velocity control of this system. Experimental validation demonstrates the actuator's ability to achieve a wide range of transmission ratios and precise movement control. We present performance data on movement precision and generated forces, discussing the results in the context of existing literature. This research contributes to the development of more versatile and efficient actuation systems for advanced robotic applications and improved automation solutions."
Design and Evaluation of High-Performance Motion-Decoupled Cable Transmission Modules.,https://doi.org/10.1109/ICRA55743.2025.11128390,"Cable transmissions are commonly used in robotics for remote force transmission, offering a lightweight, compact, and efficient solution for transmitting high forces between input and output. However, cables in flexible compression housings (Bowden cables), exhibit high static friction, which increases exponentially with total bend angle. Alternatively, internally routed ball-bearing supported cable capstan transmissions are low friction, but complex and present challenges in routing multiple sets of cables. In this paper, we propose motion-decoupled cable transmission modules that address these challenges, occupying the middle ground, functioning as discrete-joint ball-bearing supported Bowden cables. Our rolling-plus-twist joint design decouples pairs of routed cables from changing significantly in tension, length, or friction during large angle motion of the linked transmission. Using sub-1 mm diameter high-strength synthetic cable, the transmission exhibits a maximum coupling motion of only 0.15 mm over the full range of motion of the cable-transmission mechanism, approximately 10% of pretension in combined hysteresis and friction, a transmission stiffness of 10 N/mm, weighing just 9 g per rolling joint and 5 g per twist joint. Two applications are demonstrated: cable routing alongside a robot arm for, say, gripper remote actuation, and remote needle advancement for an MRI-safe needle biopsy robot."
Advanced $X \theta$ Reluctance Electromagnetic Micropositioning System for Precision Motion Control.,https://doi.org/10.1109/ICRA55743.2025.11128138,"This study examines a novel setup of a micropositioning trajectory manipulator in <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$X \theta$</tex>, energized by a reluctance actuator (RA) and two accompanying moving magnet actuators (MMA). The design is characterized by a C -core RA, which features asymmetrical air gaps between the mover and the stator elements when under angular <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\theta$</tex> rotation. When the stator coil is energized, a magnetic flux induces a force in the mover. Two MMAs can add force and torque dynamics to the system via solenoid and permanent magnet (PM) pairs to offer additional corrective actions. Facilitating control of a translational (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$x$</tex>) and rotational (<tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\theta$</tex>) two-degree-of-freedom (2DOF) actuation system. Flexure hinges aid in the retraction force of the mover element and provide needed stiffness to the system without frictional effects. This was modeled analytically and optimized to achieve outlined performance objectives. The system was validated experimentally through triangle, and sinusoidal trajectories in open loop control. The most relevant application is scanning mirror systems where specific targeted rotational and translational trajectories can benefit light beam positioning. This system allows both translation and rotation specifications of a selected trajectory to be realized in one actuation unit, opening up more design possibilities for controlling precision positioning systems."
Automated Video Object Detection of Motile Cells Under Microscopy.,https://doi.org/10.1109/ICRA55743.2025.11128014,"Video object detection (VOD) of motile cells (e.g., bacteria and sperm) under microscopy is challenging due to motion blur, sporadic out-of-focus, and pose variations. Compared with VOD in generic scenes, the lower contrast and smaller color space of microscopy imaging further introduce feature overlap between the foreground objects and the background objects (e.g., impurity cells and contaminants). Transformer-based methods have achieved great success in the VOD of generic scenes by utilizing object queries to model the inner-frame objects and the inter-frame objects. However, the appearance overlap problem in microscopy video frames significantly compromises the inter-frame query aggregation by introducing background features into the object query. To tackle this challenge, this paper reports a static-dynamic query-based VOD network that treats object queries of the current video frame and reference video frames differently. Specifically, a two-stage framework is implemented that first generates high-quality object queries of reference frames with a static Transformer decoder pre-trained on a still image dataset. The network is then trained on a per-frame annotated dataset using a dynamic Transformer decoder to model the object queries of the current frame. A Reference Query Relation Module is further proposed to enhance the reference queries for more effective aggregation with the current query. Experiments on clinically collected biopsied sperm datasets validated the effectiveness of the proposed method."
Vision-Based Movement Primitives for Lunar Hazard Avoidance.,https://doi.org/10.1109/ICRA55743.2025.11128448,"To support sustainable infrastructure on the Moon, NASA is developing the In-Situ Resource Utilization (ISRU) Pilot Excavator (IPEx) to extract and transport lunar regolith for processing and construction. During its mission, IPEx will execute various driving patterns, primarily cycling between excavation and unloading sites, with additional ma-neuvers such as circular traverses around the lander and raster scans for environmental mapping. In this work, dynamic move-ment primitives (DMPs) are used to represent these patterns. We augment the DMPs with a vision-based real-time obstacle avoidance system to navigate surface hazards, such as rocks, encountered during traversal. Our approach is evaluated in a high-fidelity simulation replicating the challenging environment of the lunar south pole to demonstrate IPEx's ability to adapt to surface hazards while fulfilling its operational tasks."
LAFNET: Lightweight Aerial Fire Detection Model for Onboard Edge Computing.,https://doi.org/10.1109/ICRA55743.2025.11128176,"Fire poses significant threats to life and property, necessitating efficient inspection and accurate identification. Although aerial computer vision algorithms hold great promise, the deployment and computational limitations of onboard platforms prevent existing algorithms from meeting high standards of accuracy and real-time performance. To address these challenges, we propose an lightweight aerial fire detection model, LAFNET. This model incorporates the EffiDarknetLight backbone, optimized for both lightweight design and ease of deployment, integrates specially designed LightGhost(LG) block components within the LightGhost-Path Aggregation Network(LG-PAN) neck, resulting in a model Params of only 1.3 M. Experimental results demonstrate that our method attains a good trade-off between lightweight design and detection accuracy. Compared to the smallest standard YOLO series' model YOLOv5n, LAFNET improves MAP by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{2. 1 \%}$</tex>, while reducing Params and FLOPs by <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{2 7. 8 \%}$</tex> and <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathbf{2 9. 3 \%}$</tex>, the inference speed on Nvidia Orin Nano edge computing side improves 24.8 %. These experiments indicate that LAFNET offers a highly efficient solution for aerial fire detection, combining speed and accuracy."
UDSV: Unsupervised Deep Stitching for Tractor-Trailer Surround View.,https://doi.org/10.1109/ICRA55743.2025.11127831,"In recent years, with the rapid development of Advanced Driver Assistance Systems (ADAS), the demand for the precise and efficient surround view stitching system has significantly increased. Traditional stitching methods perform well in small single-unit vehicles with stable camera poses. However, the stitching quality sharply degrades when applied to large tractor-trailers due to the continuous pose changes caused by the non-rigid connection between the tractor and trailer. In detail, first, the extended length of tractor-trailers results in low overlap between cameras, making feature extraction and matching challenging. Additionally, the stitched images often appear irregular, detracting from visual quality. Besides, even if static stitching looks natural, it causes jitter in dynamic scenarios due to random feature extraction. In this paper, we propose an unsupervised deep stitching method for tractor-trailer surround view system. We introduce a feature extraction module for tractor-trailer scenarios (FMT) to enhance feature extraction in low-overlap situations. Besides, we design a spatio-temporally consistent control point constraint strategy (STCC) to achieve spatial shape preservation and temporal smoothing effects, resulting in visually consistent and stable stitched sequences. Experimental results from both public and real dataset show that our method efficiently completes tractor-trailer surround view stitching, producing well-aligned and natural panoramic images compared to previous methods."
Aerial Grasping by Multi-Limbed Flying Robot SPIDAR Based on Vectored Thrust Control.,https://doi.org/10.1109/ICRA55743.2025.11128075,"Delivery by aerial robots is an emerging topic in many scenarios, such as logistics, construction industry, and disaster response. Compared to the standard styles that deploy cage or sling, grasping style by gripper can handle objects in various shapes. A multi-limbed structure with distributed vectorable rotors called SPIDAR shows a higher potential to grasp large object in a three-dimensional manner. Therefore, in this paper, we focus on the advanced usage of the vectored thrust forces to achieve aerial grasping by this robot. First, a vectored thrust control to avoid the aerointerference on the underwind segments (e.g., grasped object) during flight is proposed. Then, an optimization-based planning method that utilizes redundant vectored thrust forces for firm grasping is developed. Finally, we demonstrate the feasibility of the proposed flight control and grasp planning by performing challenging grasping and transporting motion with a spherical object of which the diameter is 0.6 m. To the best of our knowledge, this work is the first to achieve multi-finger-like grasping to carry a large object in midair."
Hook-Based Aerial Payload Grasping from a Moving Platform.,https://doi.org/10.1109/ICRA55743.2025.11127483,"This paper investigates payload grasping from a moving platform using a hook-equipped aerial manipulator. First, a computationally efficient trajectory optimization based on complementarity constraints is proposed to determine the optimal grasping time. To enable application in complex, dynamically changing environments, the future motion of the payload is predicted using a physics simulator-based model. The success of payload grasping under model uncertainties and external disturbances is formally verified through a robustness analysis method based on integral quadratic constraints. The proposed algorithms are evaluated in a high-fidelity physical simulator, and in real flight experiments using a customdesigned aerial manipulator platform."
VLN-KHVR: Knowledge-And-History Aware Visual Representation for Continuous Vision-and-Language Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127961,"Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate with lowlevel actions following natural language instructions in 3D environments. Most existing approaches utilize observation features from the current step to represent the viewpoint. However, these representations often conflate redundant and essential information for navigation, introducing ambiguity into the agent's action prediction. To address the problem of inadequate representation, we propose a Knowledge-andHistory Aware Visual Representation for Continuous Vision-and-Language Navigation (VLN-KHVR). The proposed approach constructs enriched visual representations tailored to navigation instructions, enhancing agents' navigation performance. Specifically, VLN-KHVR extracts image features from the current observation, retrieves relevant knowledge in the knowledge base, and obtains the history of the navigation episode. Subsequently, the knowledge and history features are filtered to eliminate the information irrelevant to navigation instruction. These refined features are integrated with the instruction for further interaction. Finally, the aggregated features are used to guide navigation. Our model outperforms previous methods on the VLN-CE benchmark, demonstrating the effectiveness of the proposed method."
LiteVLoc: Map-Lite Visual Localization for Image Goal Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128511,"This paper presents Lite VLoc, a hierarchical vi-sual localization framework that uses a lightweight topo-metric map to represent the environment. The method consists of three sequential modules that estimate camera poses in a coarse-to-fine manner. Unlike dense 3D mapping methods, LiteVLoc reduces storage by avoiding geometric reconstruction. It uses a learning-based feature matcher to establish dense correspondences between sparse keyframes and observations, and then refines poses with a geometric solver, enabling robustness to viewpoint changes. The system assumes depth sensors or stereo camera for deployment. A novel dataset for the map-free relocalization task is also introduced. Extensive experiments including localization and navigation in both simulated and real-world scenarios have validate the system's performance and demonstrated its precision and efficiency for large-scale deployment. Code and data will be made publicly available at the webpage:https://rpl-cs-ucl.github.io/LiteVLoc."
BEINGS: Bayesian Embodied Image-Goal Navigation With Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127919,"Image-goal navigation enables a robot to reach the location where a target image was captured, using visual cues for guidance. However, current methods either rely heavily on data and computationally expensive learning-based approaches or lack efficiency in complex environments due to insufficient exploration strategies. To address these limitations, we propose Bayesian Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that formulates ImageNav as an optimal control problem within a model predictive control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to predict future observations, enabling efficient, real-time navigation decisions grounded in the robot's sensory experiences. By integrating Bayesian updates, our method dynamically refines the robot's strategy without requiring extensive prior experience or data. Our algorithm is validated through extensive simulations and physical experiments, showcasing its potential for embodied robot systems in visually complex scenarios. Project Page: www.mwg.ink/BEINGS-web."
FLAF: Focal Line and Feature-Constrained Active View Planning for Visual Teach and Repeat.,https://doi.org/10.1109/ICRA55743.2025.11128785,"This paper presents FLAF, a focal line and feature-constrained active view planning method for autonomous orientation adjustment of a rotatable active camera during mobile robot navigation. FLAF is built on a visual teach-and-repeat (VT&R) system, which enables robots to cruise various paths that fulfill many daily autonomous navigation requirements. The VT&R system integrates Visual Simultaneous Localization and Mapping (VSLAM) with trajectory following. However, tracking failures in feature-based VSLAM, particularly in textureless regions common in human-made environments, poses a significant challenge to real-world VT&R deployment. To address this, the proposed view planner is integrated into a feature-based VSLAM system, creating an active camerabased VSLAM (AC-SLAM) solution that mitigates tracking failures. Our system features a Pan-Tilt Unit (PTU)-based active camera mounted on a mobile robot. FLAF actively directs the camera toward more map points during path learning and toward more feature-identifiable map points while following the learned trajectory. Using FLAF, the AC-SLAM system constructs a complete path map during teaching and maintains stable localization during repeating. Experimental results in real scenarios show that FLAF significantly outperforms existing methods by accounting for feature identifiability, particularly the view angle of the features. During effectively dealing with low-texture regions in active view planning, considering feature identifiability enables our active VT&R system to perform well in challenging environments."
Ground-Level Viewpoint Vision-and-Language Navigation in Continuous Environments.,https://doi.org/10.1109/ICRA55743.2025.11127275,"Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment is still challenging. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a lowheight field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Groundlevel Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots."
ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics.,https://doi.org/10.1109/ICRA55743.2025.11128133,"Oysters are a vital keystone species in coastal ecosystems, providing significant economic, environmental, and cultural benefits. As the importance of oysters grows, so does the relevance of autonomous systems for their detection and monitoring. However, current monitoring strategies often rely on destructive methods. While manual identification of oysters from video footage is non-destructive, it is time-consuming, requires expert input, and is further complicated by the challenges of the underwater environment. To address these challenges, we propose a novel pipeline using stable diffusion to augment a collected real dataset with photorealistic synthetic data. This method enhances the dataset used to train a YOLOv10-based vision model. The model is then deployed and tested on an edge platform; Aqua2, an Autonomous Underwater Vehicle (AUV), achieving a state-of-the-art 0.657 mAP@50 for oyster detection."
IBURD: Image Blending for Underwater Robotic Detection.,https://doi.org/10.1109/ICRA55743.2025.11127997,"We present an image blending pipeline, IBURD, that creates realistic synthetic images to assist in the training of deep detectors for use on underwater autonomous vehicles (AUVs) for marine debris detection tasks. Specifically, IBURD generates both images of underwater debris and their pixel-level annotations, using source images of debris objects, their annotations, and target background images of marine environments. With Poisson editing and style transfer techniques, IBURD is even able to robustly blend transparent objects into arbitrary backgrounds and automatically adjust the style of blended images using the blurriness metric of target background images. These generated images of marine debris in actual underwater backgrounds address the data scarcity and data variety problems faced by deep-learned vision algorithms in challenging underwater conditions, and can enable the use of AUVs for environmental cleanup missions. Both quantitative and robotic evaluations of IBURD demonstrate the efficacy of the proposed approach for robotic detection of marine debris."
3DSSDF: Underwater 3D Sonar Reconstruction Using Signed Distance Functions.,https://doi.org/10.1109/ICRA55743.2025.11128101,"Underwater autonomous robotic operations require online localization and 3D mapping. Because of the absence of absolute positioning underwater, these tasks strongly rely on embedded sensors, including proprioceptive or navigation sensors - which can be fused for an odometry, - and exteroceptive sensors. One of the most popular exteroceptive sensors for underwater is the imaging sonar, which emits a large fan-shaped acoustic signal and estimates the position of the surrounding obstacles from a measure of the reflected signal. This paper addresses underwater online localization and 3D mapping using a forward looking, wide-aperture imaging sonar and vehicle's intrinsic navigation estimates. We introduce 3DSSDF (3D Sonar Reconstruction Using Signed Distance Functions), a new localization and 3D mapping algorithm based on signed distance functions, which is evaluated in simulation and on real data, in man-made and natural environments. Comparisons to reference trajectories and maps demonstrate that, in our tests, 3DSSDF efficiently corrects navigation drift and that trajectory and map accuracy is always below 1 m and below 1% of the distanced travelled, which can be sufficient for the safe inspection of natural or artificial underwater structures."
Cascade IPG Observer for Underwater Robot State Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128435,"This paper presents a novel cascade nonlinear observer framework for inertial state estimation. It tackles the problem of intermediate state estimation when external localization is unavailable or in the event of a sensor outage. The proposed observer comprises two nonlinear observers based on a recently developed iteratively preconditioned gradient descent (IPG) algorithm. It takes the inputs via an IMU preintegration model where the first observer is a quaternion-based IPG. The output for the first observer is the input for the second observer, estimating the velocity and, consequently, the position. The proposed observer is validated on a public underwater dataset and a real-world experiment using our robot platform. The estimation is compared with an extended Kalman filter (EKF) and an invariant extended Kalman filter (InEKF). Results demonstrate that our method outperforms these methods regarding better positional accuracy and lower variance."
Bipedal Walking with Continuously Compliant Robotic Legs.,https://doi.org/10.1109/ICRA55743.2025.11127488,"In biomechanics and robotics, elasticity plays a crucial role in enhancing locomotion efficiency and stability. Traditional approaches in legged robots often employ series elastic actuators (SEA) with discrete rigid components, which, while effective, add weight and complexity. This paper presents an innovative alternative by integrating continuously compliant structures into the lower legs of a bipedal robot, fundamentally transforming the SEA concept. Our approach replaces traditional rigid segments with lightweight, deformable materials, reducing overall mass and simplifying the actuation design. This novel design introduces unique challenges in modeling, sensing, and control, due to the infinite dimensionality of continuously compliant elements. We address these challenges through effective approximations and control strategies. The paper details the design and modeling of the compliant leg structure, presents low-level force and kinematics controllers, and introduces a high-level posture controller with a gait scheduler. Experimental results demonstrate successful bipedal walking using this new design."
Optimal Torque Distribution via Dynamic Adaptation for Quadrupedal Locomotion on Slippery Terrains.,https://doi.org/10.1109/ICRA55743.2025.11128680,"As legged robots continue to evolve, new control methods are being developed to provide fast, robust, accurate and computationally efficient algorithms for traversing challenging environments. This paper presents a realtime adaptive locomotion controller for quadrupeds, designed to maintain stability and controllability on various surfaces, including highly slippery terrains. The proposed approach optimizes control effort distribution based on the probability of slippage by utilizing a surface-independent adaptation layer. By balancing the robot's redundant kinematic system through rank relaxation-similar to loosening constraints in optimization problems-this method demonstrates significant performance improvements. Unlike Reinforcement Learning (RL) approaches, which depend on pre-trained policies and may struggle to adapt velocity tracking control across different terrains, our method rapidly adjusts to changing conditions, as validated by extensive simulation experiments."
Adaptive Energy Regularization for Autonomous Gait Transition and Energy-Efficient Quadruped Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128812,"In reinforcement learning for legged robot locomotion, crafting effective reward strategies is crucial. Predefined gait patterns and complex reward systems are widely used to stabilize policy training. Drawing from the natural locomotion behaviors of humans and animals, which adapt their gaits to minimize energy consumption, we investigate the impact of incorporating an energy-efficient reward term that prioritizes distance-averaged energy consumption into the reinforcement learning framework. Our findings demonstrate that this simple addition enables quadruped robots to autonomously select appropriate gaits-such as four-beat walking at lower speeds and trotting at higher speeds-without the need for explicit gait regularizations. Furthermore, we provide a guideline for tuning the weight of this energy-efficient reward, facilitating its application in real-world scenarios. The effectiveness of our approach is validated through simulations and on a real Unitree Gol robot. This research highlights the potential of energy-centric reward functions to simplify and enhance the learning of adaptive and efficient locomotion in quadruped robots. Videos and more details are at https://sites.google.com/berkeley.edu/efficient-locomotion"
Music-Driven Legged Robots: Synchronized Walking to Rhythmic Beats.,https://doi.org/10.1109/ICRA55743.2025.11127347,"We address the challenge of effectively controlling the locomotion of legged robots by incorporating precise frequency and phase characteristics, which is often ignored in locomotion policies that do not account for the periodic nature of walking. We propose a hierarchical architecture that integrates a low-level phase tracker, oscillators, and a high-level phase modulator. This controller allows quadruped robots to walk in a natural manner that is synchronized with external musical rhythms. Our method generates diverse gaits across different frequencies and achieves real-time synchronization with music in the physical world. This research establishes a foundational framework for enabling real-time execution of accurate rhythmic motions in legged robots. The video and code are available at https://music-walker.github.io/."
Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control.,https://doi.org/10.1109/ICRA55743.2025.11128652,"Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system re-mains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks."
"Hybrid Decision Making for Scalable Multi-Agent Navigation: Integrating Semantic Maps, Discrete Coordination, and Model Predictive Control.",https://doi.org/10.1109/ICRA55743.2025.11128657,"This paper presents a framework for multi-agent navigation in structured but dynamic environments, integrating three key components: a shared semantic map encoding metric and semantic environmental knowledge, a claim policy for coordinating access to areas within the environment, and a Model Predictive Controller for generating motion trajectories that respect environmental and coordination constraints. The main advantages of this approach include: (i) enforcing area occupancy constraints derived from specific task requirements; (ii) enhancing computational scalability by eliminating the need for collision avoidance constraints between robotic agents; and (iii) the ability to anticipate and avoid deadlocks between agents. The paper includes both simulations and physical experiments demonstrating the framework's effectiveness in various representative scenarios."
Decentralized Nonlinear Model Predictive Control for Safe Collision Avoidance in Quadrotor Teams with Limited Detection Range.,https://doi.org/10.1109/ICRA55743.2025.11128389,"Multi-quadrotor systems face significant challenges in decentralized control, particularly with safety and coordination under sensing and communication limitations. State-of-the-art methods leverage Control Barrier Functions (CBFs) to provide safety guarantees but often neglect actuation constraints and limited detection range. To address these gaps, we propose a novel decentralized Nonlinear Model Predictive Control (NMPC) that integrates Exponential CBFs (ECBFs) to enhance safety and optimality in multi-quadrotor systems. We provide both conservative and practical minimum bounds of the range that preserve the safety guarantees of the ECBFs. We validate our approach through extensive simulations with up to 10 quadrotors and 20 obstacles, as well as real-world experiments with 3 quadrotors. Results demonstrate the effectiveness of the proposed framework in realistic settings, highlighting its potential for reliable quadrotor teams operations."
An Efficient NSGA-II-Based Algorithm for Multi-Robot Coverage Path Planning.,https://doi.org/10.1109/ICRA55743.2025.11128792,"This work presents an algorithm based on the Nondominated Sorting Genetic Algorithm II (NSGA-II) to solve multi-objective offline Multi-Robot Coverage Path Planning (MCPP) problems. The proposed algorithm embeds a donation-mutation operator and a multiple-parent crossover that generates solutions which maintain the longest path while minimizing the average path length. The algorithm also uses a library of elitism-selected high-fitness robot paths, and tournament-selected high min-max fitness paths, to construct high multi-objective fitness offspring. We evaluate the performance of our proposed algorithm against the state-of-the-art NSGA-II extended with an improved Heuristic Genetic Algorithm Crossover, and we demonstrate that for different instances of the MCPP problem, the Pareto-fronts of our proposed algorithm are not dominated by any of the points of the fronts generated by the state-of-the-art NSGA-II. A comparison has also been performed in a virtual environment simulating five drones inspecting three wind turbines. Results show that our approach exhibits a higher convergence rate for higher values of the ratio between the number of points to visit and the number of drones."
Fully Differentiable Adaptive Informative Path Planning.,https://doi.org/10.1109/ICRA55743.2025.11127801,"Autonomous robots can survey and monitor large environments. However, these robots often have limited compu-tational and power resources, making it crucial to develop an ef-ficient and adaptive informative path planning (IPP) algorithm. Such an algorithm must quickly adapt to environmental data to maximize the information collected while accommodating path constraints, such as distance budgets and boundary limitations. Current approaches to this problem often rely on maximizing mutual information using methods such as greedy algorithms, Bayesian optimization, and genetic algorithms. These methods can be slow and do not scale well to large or 3D environments. We present an adaptive IPP approach that is fully differentiable, significantly faster than previous methods, and scalable to 3D spaces. Our approach also supports continuous sensing robots, which collect data continuously along the entire path, by leveraging streaming sparse Gaussian processes. Benchmark results on two real-world datasets demonstrate that our approach yields solutions that are on par with or better than baseline methods while being up to two orders of magnitude faster. Additionally, we showcase our adaptive IPP approach in a 3D space using a system-on-chip embedded computer with minimal computational resources. Our code is available in the SGP- Tools Python library with a companion ROS 2 package for deployment on ArduPilot-based robots."
Online Informative Motion Planning for Active Information Gathering of a Non-Stationary Gaussian Process.,https://doi.org/10.1109/ICRA55743.2025.11127505,"Information gathering focuses on designing strategies for a robot to collect data about a physical process, aiming for accurate field reconstruction. While many recent methods have been proposed to address this problem, they often assume the model of the physical process is a priori known and stationary-assumptions that rarely hold in practice. This paper presents a novel informative motion planning approach for online information gathering of a non-stationary Gaussian process. Our approach comprises two key components: an informative path planner that explores the physical field and an adaptive velocity planner that adjusts the robot's velocity profile exploiting the field's spatial variability. Additionally, we propose a path smoothing and tracking strategy to ensure continuous robot motion. Extensive simulations on a bathymetric mapping task demonstrate the effectiveness of our approach, showing superior performance in reconstructing non-stationary physical fields compared to several baseline methods."
REACT: Multi Robot Energy-Aware Orchestrator for Indoor Search and Rescue Critical Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127906,"Smart factories enhance production efficiency and sustainability, but emergencies like human errors, machinery failures and natural disasters pose significant risks. In critical situations, such as fires or earthquakes, collaborative robots can assist first-responders by entering damaged buildings and locating missing persons, mitigating potential losses. Unlike previous solutions that overlook the critical aspect of energy management, in this paper we propose REACT, a smart energy-aware orchestrator that optimizes the exploration phase, ensuring prolonged operational time and effective area coverage. Our solution leverages a fleet of collaborative robots equipped with advanced sensors and communication capabilities to explore and navigate unknown indoor environments, such as smart factories affected by fires or earthquakes, with high density of obstacles. By leveraging real-time data exchange and cooperative algorithms, the robots dynamically adjust their paths, minimize redundant movements and reduce energy consumption. Extensive simulations confirm that our approach significantly improves the efficiency and reliability of search and rescue missions in complex indoor environments, improving the exploration rate by 10% over existing methods and reaching a map coverage of 97% under time critical operations, up to nearly 100% under relaxed time constraint."
Multi-Agent Ergodic Exploration Under Smoke-Based Time-Varying Sensor Visibility Constraints.,https://doi.org/10.1109/ICRA55743.2025.11128670,"In this work, we consider the problem of multiagent informative path planning (IPP) for robots whose sensor visibility continuously changes as a consequence of a time-varying natural phenomenon. We leverage ergodic trajectory optimization (ETO), which generates paths such that the amount of time an agent spends in an area is proportional to the expected information in that area. We focus specifically on the problem of multi-agent drone search of a wildfire, where we use the time-varying environmental process of smoke diffusion to construct a sensor visibility model. This sensor visibility model is used to repeatedly calculate an expected information distribution (EID) to be used in the ETO algorithm. Our experiments show that our exploration method achieves improved information gathering over both baseline search methods and naive ergodic search formulations."
Combined Modal Robust Cascade Control for Wheeled Self-Reconfigurable Robots Under Drive Failure and Safety Threat.,https://doi.org/10.1109/ICRA55743.2025.11128425,"Wheeled self-reconfigurable robots (WSRRs), a new type of multi-robot system with flexible configurations and task adaptability, have an extensive application prospects in unstructured mission environments. In this paper, based on the nonholonomic constraints and Lagrange method, the combinatorial modal kinematics and dynamics of WSRRs with arbitrary reconfiguration scale are established. At the kinematic level, based on the nonholonomic constraints, a smooth obstacle avoidance strategy based on the safety geofences is designed to ensure safety. At the dynamic level, an adaptive fault-tolerant mechanism is introduced to ensure reasonable torque distribution and avoid tracking performance degradation. Meanwhile, an improved extended state observer (IESO) is elaborated, through which the high-frequency ocsillation from measurement noises and peaking phenomenon from initial observer errors can be suppressed, and the robust velocity tracking control under unknown lumped disturbances is realized. Finally, a real-world WSRRs experiment is constructed to verify the proposed method's fault tolerance, robustness, and safety comparatively."
CaDRE: Controllable and Diverse Generation of Safety-Critical Driving Scenarios Using Real-World Trajectories.,https://doi.org/10.1109/ICRA55743.2025.11127319,"Simulation is an indispensable tool in the development and testing of autonomous vehicles (AVs), offering an efficient and safe alternative to road testing. An outstanding challenge with simulation-based testing is the generation of safety-critical scenarios, which are essential to ensure that AVs can handle rare but potentially fatal situations. This paper addresses this challenge by introducing a novel framework CaDRE, to generate realistic, diverse, and controllable safetycritical scenarios. Our approach optimizes for both the quality and diversity of scenarios by employing a unique formulation and algorithm that integrates real-world scenarios, domain knowledge, and black-box optimization. We validate the effectiveness of our framework through extensive testing in three representative types of traffic scenarios. The results demonstrate superior performance in generating diverse and highquality scenarios with greater sample efficiency than existing reinforcement learning (RL) and sampling-based methods."
Certificated Actor-Critic: Hierarchical Reinforcement Learning with Control Barrier Functions for Safe Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127225,"Control Barrier Functions (CBFs) have emerged as a prominent approach to designing safe navigation systems of robots. Despite their popularity, current CBF-based methods exhibit some limitations: optimization-based safe control techniques tend to be either myopic or computationally intensive, and they rely on simplified system models; conversely, the learning-based methods suffer from the lack of quantitative indication in terms of navigation performance and safety. In this paper, we present a new model-free reinforcement learning algorithm called Certificated Actor-Critic (CAC), which introduces a hierarchical reinforcement learning framework and well-defined reward functions derived from CBFs. We carry out theoretical analysis and proof of our algorithm, and propose several improvements in algorithm implementation. Our analysis is validated by two simulation experiments, showing the effectiveness of our proposed CAC algorithm."
Exact Imposition of Safety Boundary Conditions in Neural Reachable Tubes.,https://doi.org/10.1109/ICRA55743.2025.11127972,"Hamilton-Jacobi (HJ) reachability analysis is a widely adopted verification tool to provide safety and performance guarantees for autonomous systems. However, it involves solving a partial differential equation (PDE) to compute a safety value function, whose computational and memory complexity scales exponentially with the state dimension, making its direct application to large-scale systems intractable. To overcome these challenges, DeepReach, a recently proposed learning-based approach, approximates high-dimensional reachable tubes using neural networks (NNs). While shown to be effective, the accuracy of the learned solution decreases with system complexity. One of the reasons for this degradation is a soft imposition of safety constraints during the learning process, which corresponds to the boundary conditions of the PDE, resulting in inaccurate value functions. In this work, we propose ExactBC, a variant of DeepReach that imposes safety constraints exactly during the learning process by restructuring the overall value function as a weighted sum of the boundary condition and the NN output. Moreover, the proposed variant no longer needs a boundary loss term during the training process, thus eliminating the need to balance different loss terms. We demonstrate the efficacy of the proposed approach in significantly improving the accuracy of the learned value function for four challenging reachability tasks: a rimless wheel system with state resets, collision avoidance in a cluttered environment, autonomous rocket landing, and multi-aircraft collision avoidance."
RelAIBotiX: Reliability Assessment for AI-Controlled Robotic Systems.,https://doi.org/10.1109/ICRA55743.2025.11128348,"AI-controlled robotic systems can introduce significant risks to both humans and the environment. Traditional reliability assessment methods fall short in addressing the complexities of these systems, particularly when dealing with black-box or dynamically changing control policies. The traditional approaches are applied manually and do not consider frequent software updates. In this paper, we present RelAIBotiX, a new methodology that enables dynamic and continuous reliability assessment, specifically tailored for robotic systems controlled by AI algorithms. RelAIBotiX combines four methods: (i) Skill Detection that automatically identifies executed skills using deep learning techniques, (ii) Behavioral Analysis that creates an operational profile of the robotic system containing information about the skill execution sequence, active components for each skill, and their utilization intensity that influence their failure rate, (iii) Reliability Model Generation that automatically transforms the operational profile and reliability data of robotic hardware components into quantitative hybrid reliability models, and (iv) Reliability Model Solver for the numerical evaluation of the generated reliability models. Our evaluation included computing the reliability of the system, the probability of failure of individual skills, and component sensitivity analysis. We validated the applicability of the proposed framework across five simulative and real-world setups."
Adaptive Emotional Expression in Social Robots: A Multimodal Approach to Dynamic Emotion Modeling.,https://doi.org/10.1109/ICRA55743.2025.11127502,"Social robots have been extensively studied in recent decades, with many researchers exploring the use of modalities such as facial expressions to achieve more natural emotions in robots. Various methods have been attempted to generate and express robot emotions, including computational models that define an affect space and show dynamic emotion changes. However, the implementation of multimodal expression in previous models is ambiguous, and the generation of emotions in response to stimuli relies on heuristic methods. In this paper, we present a framework that enables robots to naturally express their emotions in a multimodal way, where the emotion can change over time based on the given stimulus values. By representing the robot's emotion as a position in an affect space of a computational emotion model, we consider the given stimuli values as driving forces that can shift the emotion position dynamically. In order to examine the feasibility of our proposed method, a mobile robot prototype was implemented that can recognize touch and express different emotions with facial expressions and movements. The experiment demonstrated that the emotion elicited by a given stimulus is contingent upon the robot's previous state, thereby imparting the impression that the robot possesses a distinctive emotion model. Furthermore, the Godspeed survey results indicated that our model was rated significantly higher than the baseline, which did not include a computational emotion model, in terms of anthropomorphism, animacy, and perceived intelligence. Notably, the unpredictability of emotion switching contributed to a perception of greater lifelikeness, which in turn enhanced the overall interaction experience."
"""Oh! It's Fun Chatting with You!"" a Humor-Aware Social Robot Chat Framework.",https://doi.org/10.1109/ICRA55743.2025.11127367,"Humor is a key element in human interactions, essential for building connections and rapport. To enhance human-robot communication, we developed a humor-aware chat framework that enables robots to deliver contextually appropriate humor. This framework takes into account the interaction environment, and user's profile as well as emotional state. Two GPT models are used to generate responses. The initial one, named sensor-GPT, processes contextual data from the sensor along with the user's response and conversation history to create prompts for the second one, chat-GPT. These prompts can guide the model on how to integrate appropriate humor elements into the conversation, ensuring that the dialogue is both contextually relevant and humorous. Our experiment compared the effectiveness of humor expression between our framework and the GPT-40 model. The results demonstrate that robots using our framework significantly outperform those using GPT-4o in humor expression, extending conversations, and improving overall interaction quality."
Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots.,https://doi.org/10.1109/ICRA55743.2025.11127985,"Humans are able to convey different messages using only touch. Equipping robots with the ability to under-stand social touch adds another modality in which humans and robots can communicate. In this paper, we present a social gesture recognition system using a fabric-based, large-scale tactile sensor placed onto the arms of a humanoid robot. We built a social gesture dataset using multiple participants and extracted temporal features for classification. By collecting tactile data on a humanoid robot, our system provides insights into human-robot social touch, and displays that the use of fabric based sensors could be a potential way of advancing the development of spHRI systems for more natural and effective communication."
Seeing Eye to Eye: Design and Evaluation of a Custom Expressive Eye Display Module for the Stretch Mobile Manipulator.,https://doi.org/10.1109/ICRA55743.2025.11128312,"Mobile manipulators  robots with a moving base and an arm for grasping objects  are becoming more common in human-populated environments, such as hospitals, warehouses, and even homes. Yet most mobile manipulators lack clear ways to communicate intent to human interlocutors in a continuous, socially acceptable, and easy-to-interpret way. One possible solution for improving mobile manipulator communication is the addition of expressive eyes. This paper presents the design and evaluation of a custom expressive LED eye module for mobile manipulators, which can display both gaze and emotional expressions. Our evaluation study <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(N=32)$</tex> involved a mock teamwork task alongside a Hello Robot Stretch RE2 mobile manipulator with the custom LED eye module. The results showed that both gaze and emotional expressions supported better participant performance in the task and more feelings of social closeness. Emotional eye expressions also yielded higher ratings of robot social warmth and competence. This work can inform mobile manipulator design for smoother integration into human-populated spaces."
UGotMe: An Embodied System for Affective Human-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11128819,"Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multi-party conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, real-time response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve real-time response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://lipzh5.github.io/HumanoidVLE/"
SCU-Hand: Soft Conical Universal Robotic Hand for Scooping Granular Media from Containers of Various Sizes.,https://doi.org/10.1109/ICRA55743.2025.11128802,"Automating small-scale experiments in materials science presents challenges due to the heterogeneous nature of experimental setups. This study introduces the SCU-Hand (Soft Conical Universal Robot Hand), a novel end-effector designed to automate the task of scooping powdered samples from various container sizes using a robotic arm. The SCU-Hand employs a flexible, conical structure that adapts to different container geometries through deformation, maintaining consistent contact without complex force sensing or machine learning-based control methods. Its reconfigurable mechanism allows for size adjustment, enabling efficient scooping from diverse container types. By combining soft robotics principles with a sheet-morphing design, our end-effector achieves high flexibility while retaining the necessary stiffness for effective powder manipulation. We detail the design principles, fabrication process, and experimental validation of the SCU-Hand. Experimental validation showed that the scooping capacity is about 20% higher than that of a commercial tool, with a scooping performance of more than 95% for containers of sizes between 67 mm to 110 mm. This research contributes to laboratory automation by offering a cost-effective, easily implementable solution for automating tasks such as materials synthesis and characterization processes."
VSB - Variable Stiffness Based on Bowden Cables: A Simple Mechanism for Soft Robotic Hands.,https://doi.org/10.1109/ICRA55743.2025.11127353,"Soft robotic hands compensate for uncertainty in perception and actuation by leveraging passive deformation in their intrinsically compliant hardware, facilitating robust and dexterous interactions with their environment. The ability to adjust the level of compliance during operation has the potential to further improve the performance of these hands by enabling novel interaction strategies. However, achieving variable stiffness mechanically typically requires significant engineering complexity, making these systems difficult to manufacture, prone to error, and expensive. We present a novel, very simple mechanism for achieving variable stiffness. This mechanism employs tendon-driven antagonistic actuation, with Bowden cables connecting elastic elements to servomotors. It supports compact actuator designs, while the Bowden cables facilitate flexible component placement within a robotic system. Following our approach, variable stiffness actuators can be easily manufactured at low-cost from readily available materials. Despite its simplicity, we demonstrate that our mechanism provides consistent and precise control over stiffness levels and contact torques, showcasing its potential for a broad range of applications in soft robotic systems."
Design and Experimental Validation of Woodwork-Inspired Soft Pneumatic Grippers.,https://doi.org/10.1109/ICRA55743.2025.11127378,"This paper presents a novel design concept of a pair of soft gripper hands that can establish a secure connection between them for bearing a large load with a low air pressure. The design was inspired by dovetail joints in carpentry that enable a tight, strong connection between two pieces of wood. We propose to mimic the dovetail joint mechanism by using soft robotic fingers that interlace to each other for secure connection. The work was motivated by the need for securing a connection between two soft robotic arms for holding a balance-impaired older adult in case of losing balance. First, the design principle of dovetail-like secure soft finger connection is presented, and its potential application to a portable fall prevention system is described. Details of the dovetail soft finger design, its rapid inflation method, and other implementation issues are then discussed. Through experiments of a proof-of-concept prototype, it is validated that the dovetail soft fingers can bear at least 18 kg of load with only 52 kPa of air chamber pressure filled in 250 ms of charging time. At the end, the proposed method is compared to alternative methods using a Pugh chart."
A Variable Stiffness and Transformable Entanglement Soft Robotic Gripper.,https://doi.org/10.1109/ICRA55743.2025.11127484,"For objects with complex topological and geometrical features, stochastic topological grasping can be executed without the necessity for feedback or precise planning. However, this grasping method has two significant limitations. First, the technique's effectiveness is reduced when interacting with topologically and geometrically simple objects like spheres, cubes, and cylinders, due to the inherent variability in grasping patterns. Additionally, the method's low stiffness restricts its ability to securely handling heavier objects. To address these challenges, this paper proposes an entanglement soft robotic gripper with variable stiffness and two transformed grasping modes (entanglement and clamping modes). The gripper contains three filaments, which can enhance the stiffness through the mechanism of layer jamming. Furthermore, the entanglement mode and the clamping mode, can be transformed by adjusting the working length of the filaments. The grasping performance comparison with and without variable stiffness was carried out, and the results indicated that the implementation of variable stiffness led to a 149 % increase in payload weight. Through experimental validation, we successfully employed the gripper in variable stiffness and transformed modes to grasp items with various shapes and weights. Demonstration of grasping heavier objects and transforming between two grasping modes were also conducted to showcase the adaptability and versatility of the gripper."
Ego-$A^{\mathbf{3}}$: Adaptive Fusion-Based Disentangled Transformer for Egocentric Action Anticipation.,https://doi.org/10.1109/ICRA55743.2025.11127898
A New Variable-Gain Sliding Mode Filter and Its Application to Velocity Filtering.,https://doi.org/10.1109/ICRA55743.2025.11127675
A Comparative Study Between a Virtual Wand and a One-to-One Approach for the Teleoperation of a Nearby Robotic Manipulator.,https://doi.org/10.1109/ICRA55743.2025.11127740
A Novel Telelocomotion Framework with CoM Estimation for Scalable Locomotion on Humanoid Robots.,https://doi.org/10.1109/ICRA55743.2025.11127761
Stiffness Regulation Co-Pilot in Bilateral Teleimpedance Control: A Preliminary User Study.,https://doi.org/10.1109/ICRA55743.2025.11127810
Learning Visuotactile Skills With Two Multifingered Hands.,https://doi.org/10.1109/ICRA55743.2025.11128180
Learning Coordinated Bimanual Manipulation Policies Using State Diffusion and Inverse Dynamics Models.,https://doi.org/10.1109/ICRA55743.2025.11127387
BiFold: Bimanual Cloth Folding with Language Guidance.,https://doi.org/10.1109/ICRA55743.2025.11127549
One-Shot Dual-Arm Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128338
In the Wild Ungraspable Object Picking with Bimanual Nonprehensile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128490
Efficient 7-DoF Grasp for Target-Driven Object in Dense Cluttered Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127394
Task-Oriented 6-DoF Grasp Pose Detection in Clutters.,https://doi.org/10.1109/ICRA55743.2025.11128749
Behavioral Manifolds: Representing the Landscape of Grasp Affordances in Relative Pose Space.,https://doi.org/10.1109/ICRA55743.2025.11128813
NeRF-Based Transparent Object Grasping Enhanced by Shape Priors.,https://doi.org/10.1109/ICRA55743.2025.11127580
EVLoc: Event-Based Visual Localization in LiDAR Maps via Event-Depth Registration.,https://doi.org/10.1109/ICRA55743.2025.11127535
MambaGlue: Fast and Robust Local Feature Matching with Mamba.,https://doi.org/10.1109/ICRA55743.2025.11128473
ULOC: Learning to Localize in Complex Large-Scale Environments with Ultra-Wideband Ranges.,https://doi.org/10.1109/ICRA55743.2025.11128859
Indoor Localization of UAVs Using Only Few Measurements by Output-Sensitive Preimage Intersection.,https://doi.org/10.1109/ICRA55743.2025.11127316
Text2Robot: Evolutionary Robot Design from Text Descriptions.,https://doi.org/10.1109/ICRA55743.2025.11128168
QueryCAD: Grounded Question Answering for CAD Models.,https://doi.org/10.1109/ICRA55743.2025.11128709
"HeRo: A State Machine-Based, Fault-Tolerant Framework for Heterogeneous Multi-Robot Collaboration.",https://doi.org/10.1109/ICRA55743.2025.11128674
A Kinematics Optimization Framework with Improved Computational Efficiency for Task-Based Optimum Design of Serial Manipulators in Cluttered Environments.,https://doi.org/10.1109/ICRA55743.2025.11128345
ACROSS: A Deformation-Based Cross-Modal Representation for Robotic Tactile Perception.,https://doi.org/10.1109/ICRA55743.2025.11127851
Learning to Double Guess: An Active Perception Approach for Estimating the Center of Mass of Arbitrary Objects.,https://doi.org/10.1109/ICRA55743.2025.11127607
Learning In-Hand Translation Using Tactile Skin with Shear and Normal Force Sensing.,https://doi.org/10.1109/ICRA55743.2025.11127974
Contrastive Touch-to-Touch Pretraining.,https://doi.org/10.1109/ICRA55743.2025.11127980
HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of Human Head Motion in Diverse Conditions.,https://doi.org/10.1109/ICRA55743.2025.11128564
Relevance-Driven Decision Making for Safer and More Efficient Human Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11127911
Back to the Cartesian: Pilot Study for Assessing Human Stiffness in 3D Cartesian Space by Transforming from Muscle Space in a Peg-In-Hole Scenario for Tele-Impedance.,https://doi.org/10.1109/ICRA55743.2025.11127778
Systematic Comparison of Projection Methods for Monocular 3D Human Pose Estimation on Fisheye Images.,https://doi.org/10.1109/ICRA55743.2025.11128613
Robotic-CLIP: Fine-Tuning CLIP on Action Data for Robotic Applications.,https://doi.org/10.1109/ICRA55743.2025.11127829
ICRT: In-Context Imitation Learning via Next-Token Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128272
Data Augmentation for NeRFs in the Low Data Limit.,https://doi.org/10.1109/ICRA55743.2025.11128854
Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding.,https://doi.org/10.1109/ICRA55743.2025.11127987
Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction.,https://doi.org/10.1109/ICRA55743.2025.11127955
Reservoir Computing Encodes Physical Adaptations for Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127876
Self-Supervised Meta-Learning for All-Layer DNN-Based Adaptive Control with Stability Guarantees.,https://doi.org/10.1109/ICRA55743.2025.11127548
METDrive: Multimodal End-to-End Autonomous Driving with Temporal Guidance.,https://doi.org/10.1109/ICRA55743.2025.11127671
Generalizing Motion Planners with Mixture of Experts for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127274
Low-Rank Adaptation-Based All-Weather Removal for Autonomous Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127411
Stands on Shoulders of Giants: Learning to Lift 2D Detection to 3D with Geometry-Driven Objectives.,https://doi.org/10.1109/ICRA55743.2025.11128173
LidarDM: Generative LiDAR Simulation in a Generated World.,https://doi.org/10.1109/ICRA55743.2025.11128001
Renderworld: World Model with Self-Supervised 3D Label.,https://doi.org/10.1109/ICRA55743.2025.11127609
In-Plane Manipulation of Soft Micro-Fiber with Ultrasonic Transducer Array and Microscope.,https://doi.org/10.1109/ICRA55743.2025.11127336
A Complete and Bounded-Suboptimal Algorithm for a Moving Target Traveling Salesman Problem with Obstacles in 3D.,https://doi.org/10.1109/ICRA55743.2025.11128828
Physics-Aware Robotic Palletization With Online Masking Inference.,https://doi.org/10.1109/ICRA55743.2025.11128204
Image-Based Compliance Control for Robotic Steering of a Ferromagnetic Guidewire.,https://doi.org/10.1109/ICRA55743.2025.11128160
AutoPeel: Adhesion-Aware Safe Peeling Trajectory Optimization for Robotic Wound Care.,https://doi.org/10.1109/ICRA55743.2025.11128373
In-Vivo Cable-Driven Rodent Ankle Exoskeleton System for Sensorimotor Rehabilitation.,https://doi.org/10.1109/ICRA55743.2025.11128320
Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128581
Introspective Loop Closure for SLAM with 4D Imaging Radar.,https://doi.org/10.1109/ICRA55743.2025.11127330
Range-Based 6-DoF Monte Carlo SLAM with Gradient-Guided Particle Filter on GPU.,https://doi.org/10.1109/ICRA55743.2025.11128385
Distributed Certifiably Correct Range-Aided SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128603
coVoxSLAM: GPU Accelerated Globally Consistent Dense SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128269
Radar4VoxMap: Accurate Odometry from Blurred Radar Observations.,https://doi.org/10.1109/ICRA55743.2025.11128118
Highly Dynamic Physical Interaction for Robotics: Design and Control of an Active Remote Center of Compliance.,https://doi.org/10.1109/ICRA55743.2025.11128451
Pinto: A Latched Spring Actuated Robot for Jumping and Perching.,https://doi.org/10.1109/ICRA55743.2025.11128584
"D3-ARM: High-Dynamic, Dexterous and Fully Decoupled Cable-Driven Robotic Arm.",https://doi.org/10.1109/ICRA55743.2025.11127687
Design of an Articulated Modular Caterpillar Using Spherical Linkages.,https://doi.org/10.1109/ICRA55743.2025.11127993
Generative-AI-Driven Jumping Robot Design Using Diffusion Models.,https://doi.org/10.1109/ICRA55743.2025.11128113
Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios.,https://doi.org/10.1109/ICRA55743.2025.11127354
Explore the LiDAR-Camera Dynamic Adjustment Fusion for 3D Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11128091
Bridging Spectral-Wise and Multi-Spectral Depth Estimation Via Geometry-Guided Contrastive Learning.,https://doi.org/10.1109/ICRA55743.2025.11127755
"VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal Transparent Surface Reconstruction in Indoor Scenes.",https://doi.org/10.1109/ICRA55743.2025.11127657
CDMFusion: RGB-T Image Fusion Based on Conditional Diffusion Models via Few Denoising Steps in Open Environments.,https://doi.org/10.1109/ICRA55743.2025.11128410
UniBEVFusion: Unified Radar-Vision Bevfusion for 3D Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11128067
Trajectory Planning and Control for Differentially Flat Fixed-Wing Aerial Systems.,https://doi.org/10.1109/ICRA55743.2025.11128404
Safe Quadrotor Navigation Using Composite Control Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11127368
The Spinning Blimp: Design and Control of a Novel Minimalist Aerial Vehicle Leveraging Rotational Dynamics and Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128265
One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms.,https://doi.org/10.1109/ICRA55743.2025.11128790
Modeling and Control of Aerial Robot SERPENT: A Soft Structure Incorporated Multirotor Aerial Robot Capable of In-Flight Flexible Deformation.,https://doi.org/10.1109/ICRA55743.2025.11128294
SOLVR: Submap Oriented LiDAR-Visual Re-Localisation.,https://doi.org/10.1109/ICRA55743.2025.11127845
SSF: Sparse Long-Range Scene Flow for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128770
BoxMap: Efficient Structural Mapping and Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127722
UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty.,https://doi.org/10.1109/ICRA55743.2025.11128148
Multi-Floor Zero-Shot Object Navigation Policy.,https://doi.org/10.1109/ICRA55743.2025.11128607
Towards Survivability in Complex Motion Scenarios: RGB-Event Object Tracking via Historical Trajectory Prompting.,https://doi.org/10.1109/ICRA55743.2025.11127812
Spatially Constrained and Deeply Learned Bilateral Structural Intensity-Depth Registration Autonomously Navigates a Flexible Endoscope.,https://doi.org/10.1109/ICRA55743.2025.11127777
E2B: A Single Modality Point-Based Tracker with Event Cameras.,https://doi.org/10.1109/ICRA55743.2025.11127695
$\mathbf{F}^{2} \mathbf{R}^{2}$: Frequency Filtering-Based Rectification Robustness Method for Stereo Matching.,https://doi.org/10.1109/ICRA55743.2025.11127707
SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11128339
SR-AIF: Solving Sparse-Reward Robotic Tasks From Pixels with Active Inference and World Models.,https://doi.org/10.1109/ICRA55743.2025.11127713
Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning.,https://doi.org/10.1109/ICRA55743.2025.11127692
Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models.,https://doi.org/10.1109/ICRA55743.2025.11128270
VertiCoder: Self-Supervised Kinodynamic Representation Learning on Vertically Challenging Terrain.,https://doi.org/10.1109/ICRA55743.2025.11128598
Planning-Oriented Cooperative Perception Among Heterogeneous Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11127774
TaskExp: Enhancing Generalization of Multi-Robot Exploration with Multi-Task Pre-Training.,https://doi.org/10.1109/ICRA55743.2025.11128456
Wcdt: World-Centric Diffusion Transformer for Traffic Scene Generation.,https://doi.org/10.1109/ICRA55743.2025.11127600
Hybrid Decentralization for Multi-Robot Orienteering with Mothership-Passenger Systems.,https://doi.org/10.1109/ICRA55743.2025.11127613
Communication-Aware Iterative Map Compression for Online Path-Planning.,https://doi.org/10.1109/ICRA55743.2025.11128672
DiffCP: Ultra-Low Bit Collaborative Perception via Diffusion Model.,https://doi.org/10.1109/ICRA55743.2025.11128518
ICBSS: An Improved Algorithm for Multi-Agent Combinatorial Path Finding.,https://doi.org/10.1109/ICRA55743.2025.11127663
Escaping Local Minima: Hybrid Artificial Potential Field with Wall-Follower for Decentralized Multi-Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128765
Heterogeneous Exploration and Monitoring with Online Free-Space Ellipsoid Graphs.,https://doi.org/10.1109/ICRA55743.2025.11128121
Wavelet-Based Distributed Coverage for Heterogeneous Agents.,https://doi.org/10.1109/ICRA55743.2025.11128696
Multi-Agent Obstacle Avoidance Using Velocity Obstacles and Control Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11127277
Valg: Vision-Based Adaptive Laser Gripper for Model-Free Pose Control of Floating Objects at Air-Liquid Interface.,https://doi.org/10.1109/ICRA55743.2025.11127612
Model-Based Robotic Cell Aspiration: Tackling the Impact of Air Segment.,https://doi.org/10.1109/ICRA55743.2025.11128169
Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap.,https://doi.org/10.1109/ICRA55743.2025.11128363
Next-Best-Trajectory Planning of Robot Manipulators for Effective Observation and Exploration.,https://doi.org/10.1109/ICRA55743.2025.11128465
TriHRCBot: A Robotic Architecture for Triadic Human-Robot Collaboration Through Mediated Object Alignment.,https://doi.org/10.1109/ICRA55743.2025.11128795
Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs.,https://doi.org/10.1109/ICRA55743.2025.11127584
Integrating Field of View in Human-Aware Collaborative Planning.,https://doi.org/10.1109/ICRA55743.2025.11127371
PACE: Proactive Assistance in Human-Robot Collaboration Through Action-Completion Estimation.,https://doi.org/10.1109/ICRA55743.2025.11127399
DemoStart: Demonstration-Led Auto-Curriculum Applied to Sim-to-Real with Multi-Fingered Robots.,https://doi.org/10.1109/ICRA55743.2025.11127813
Dexterous Assembly Using a Planar Hand Having Programmable Passive Compliance.,https://doi.org/10.1109/ICRA55743.2025.11127614
GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127957
Model Q-II: An Underactuated Hand with Enhanced Grasping Modes and Primitives for Dexterous Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127291
Canonical Representation and Force-Based Pretraining of 3D Tactile for Dexterous Visuo-Tactile Policy Learning.,https://doi.org/10.1109/ICRA55743.2025.11128094
Dynamic Compact Consensus Tracking for Aerial Robots.,https://doi.org/10.1109/ICRA55743.2025.11128606
CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128342
LaMOT: Language-Guided Multi-Object Tracking.,https://doi.org/10.1109/ICRA55743.2025.11128553
Real-Time UAV Tracking: A Comparative Study of YOLOv8 with Object Tracking Algorithms.,https://doi.org/10.1109/ICRA55743.2025.11128602
Hypergraph-Transformer (HGT) for Interaction Event Prediction in Laparoscopic and Robotic Surgery.,https://doi.org/10.1109/ICRA55743.2025.11127325
Robotic Flexible Magnetic Retractor for Dynamic Tissue Manipulation in Endoscopic Submucosal Dissection.,https://doi.org/10.1109/ICRA55743.2025.11127852
Leveraging Surgical Activity Grammar for Primary Intention Prediction in Laparoscopy Procedures.,https://doi.org/10.1109/ICRA55743.2025.11127338
SLAM Assisted 3D Tracking System for Laparoscopic Surgery.,https://doi.org/10.1109/ICRA55743.2025.11127408
SurgPose: Generalisable Surgical Instrument Pose Estimation Using Zero-Shot Learning and Stereo Vision.,https://doi.org/10.1109/ICRA55743.2025.11128521
Design and Effectiveness of Virtual Monitors and AR-Based Endoscope Control for Robotically Assisted Laparoscopic Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128626
MEDiC: Autonomous Surgical Robotic Assistance to Maximizing Exposure for Dissection and Cautery.,https://doi.org/10.1109/ICRA55743.2025.11128739
Deformpam: Data-Efficient Learning for Long-Horizon Deformable Object Manipulation Via Preference-Based Action Alignment.,https://doi.org/10.1109/ICRA55743.2025.11127926
Autonomous Bimanual Manipulation of Deformable Objects Using Deep Reinforcement Learning Guided Adaptive Control.,https://doi.org/10.1109/ICRA55743.2025.11127676
Embedded IPC: Fast and Intersection-Free Simulation in Reduced Subspace for Robot Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128485
A Highly Robust Contact Sensor for Precise Contact Detection of Fabric.,https://doi.org/10.1109/ICRA55743.2025.11127921
Miniature Dielectric Elastomer Actuator Probe Inspecting Confined Spaces Embedding a CMOS Sensor.,https://doi.org/10.1109/ICRA55743.2025.11128835
"Portable, High-Frequency, and High-Voltage Control Circuits for Untethered Miniature Robots Driven by Dielectric Elastomer Actuators.",https://doi.org/10.1109/ICRA55743.2025.11128434
Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in Musculoskeletal Antagonistic Joints.,https://doi.org/10.1109/ICRA55743.2025.11128116
Beyond Traversing in a Thin Pipe: Self-Sensing Odometry of a Pipeline Robot Driven by High-Frequency Dielectric Elastomer Actuators.,https://doi.org/10.1109/ICRA55743.2025.11128685
High-Force Electroadhesion Based on Unique Liquid-Solid Dielectrics for UAV Perching.,https://doi.org/10.1109/ICRA55743.2025.11128507
Multi-Scale Convolutional Networks with Class-Normalized Logit Clipping for Robust Sea State Estimation from Noisy Ship Motion Data.,https://doi.org/10.1109/ICRA55743.2025.11128616
Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention.,https://doi.org/10.1109/ICRA55743.2025.11127818
Motion Forecasting via Model-Based Risk Minimization.,https://doi.org/10.1109/ICRA55743.2025.11127460
Computational Teaching for Driving via Multi-Task Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11127621
A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation.,https://doi.org/10.1109/ICRA55743.2025.11128380
LoRD: Adapting Differentiable Driving Policies to Distribution Shifts.,https://doi.org/10.1109/ICRA55743.2025.11128750
Behav: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127890
Learning Direct Solution in Moving Horizon Estimation with Deep Learning Methods.,https://doi.org/10.1109/ICRA55743.2025.11128431
A Data-Driven Contact Estimation Method for Wheeled-Biped Robots.,https://doi.org/10.1109/ICRA55743.2025.11128254
Simultaneous Ground Reaction Force and State Estimation Via Constrained Moving Horizon Estimation.,https://doi.org/10.1109/ICRA55743.2025.11127398
Joint 3D Point Cloud Segmentation Using Real-Sim Loop: From Panels to Trees and Branches.,https://doi.org/10.1109/ICRA55743.2025.11128189
Energy Efficient Planning for Repetitive Heterogeneous Tasks in Precision Agriculture.,https://doi.org/10.1109/ICRA55743.2025.11128083
Leveraging LLMs for Mission Planning in Precision Agriculture.,https://doi.org/10.1109/ICRA55743.2025.11128633
Hierarchical Tri-Manual Planning for Vision-Assisted Fruit Harvesting with Quadrupedal Robots.,https://doi.org/10.1109/ICRA55743.2025.11127561
Capacitated Agriculture Fleet Vehicle Routing with Implements and Limited Autonomy: A Model and a Two-Phase Solution Approach.,https://doi.org/10.1109/ICRA55743.2025.11127753
Towards Closing the Loop in Robotic Pollination for Indoor Farming Via Autonomous Microscopic Inspection.,https://doi.org/10.1109/ICRA55743.2025.11127933
Embedded Robust Model Predictive Path Integral Control Using Sensitivity Tubes and GPU Acceleration.,https://doi.org/10.1109/ICRA55743.2025.11127520
Enhancing Robotic System Robustness via Lyapunov Exponent-Based Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128769
Endpoint-Explicit Differential Dynamic Programming via Exact Resolution.,https://doi.org/10.1109/ICRA55743.2025.11128064
Second-Order Stein Variational Dynamic Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128381
Application of Koopman Direct Encoding-Based Model Predictive Control to Nonlinear Electromechanical Systems.,https://doi.org/10.1109/ICRA55743.2025.11128499
Controlled Robot Language with Frame Semantics (FrameCRL) for Autonomous Context-Aware High-Level Planning.,https://doi.org/10.1109/ICRA55743.2025.11127391
Effective Tuning Strategies for Generalist Robot Manipulation Policies.,https://doi.org/10.1109/ICRA55743.2025.11127492
RM-Planner: Integrating Reinforcement Learning with Whole-Body Model Predictive Control for Mobile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127719
TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image.,https://doi.org/10.1109/ICRA55743.2025.11128239
Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial SLAM Configurable for LiDARs or Depth Cameras.,https://doi.org/10.1109/ICRA55743.2025.11128702
Parking-SG: Open-Vocabulary Hierarchical 3D Scene Graph Representation for Open Parking Environments.,https://doi.org/10.1109/ICRA55743.2025.11128767
3D Lane Detection Based on Projection-Consistent Reference Points and Intra- & Inter-lane Context.,https://doi.org/10.1109/ICRA55743.2025.11128731
Unveiling the Black Box: Independent Functional Module Evaluation for Bird's-Eye-View Perception Model.,https://doi.org/10.1109/ICRA55743.2025.11127344
Coarse-to-Fine Cross-Modality Generation for Enhancing Vehicle Re-Identification with High-Fidelity Synthetic Data.,https://doi.org/10.1109/ICRA55743.2025.11127895
Distributed Multi-Robot Source Seeking in Unknown Environments with Unknown Number of Sources.,https://doi.org/10.1109/ICRA55743.2025.11127477
Multi-Nonholonomic Robot Object Transportation with Obstacle Crossing Using a Deformable Sheet.,https://doi.org/10.1109/ICRA55743.2025.11128313
Configuration-Adaptive Visual Relative Localization for Spherical Modular Self-Reconfigurable Robots.,https://doi.org/10.1109/ICRA55743.2025.11127871
Realm: Real-Time Line-of-Sight Maintenance in Multi-Robot Navigation with Unknown Obstacles.,https://doi.org/10.1109/ICRA55743.2025.11128211
Personalizing Interfaces to Humans with User-Friendly Priors.,https://doi.org/10.1109/ICRA55743.2025.11128246
Personalization in Human-Robot Interaction Through Preference-Based Action Representation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128756
Interface Matters: Comparing First and Third-Person Perspective Interfaces for Bi-Manual Robot Behavioural Cloning.,https://doi.org/10.1109/ICRA55743.2025.11128110
Robot Policy Transfer with Online Demonstrations: An Active Reinforcement Learning Approach.,https://doi.org/10.1109/ICRA55743.2025.11127396
User-Aware Collaborative Learning in Human-Robot Interactions.,https://doi.org/10.1109/ICRA55743.2025.11128489
Data-Efficient Learning from Human Interventions for Mobile Robots.,https://doi.org/10.1109/ICRA55743.2025.11128012
Mathematical Modeling and Rolling Motion Generation of Planar Seven-link Robot That Forms Passive Closed and Active Open Chains.,https://doi.org/10.1109/ICRA55743.2025.11128346
LEVA: A High-Mobility Logistic Vehicle with Legged Suspension.,https://doi.org/10.1109/ICRA55743.2025.11128847
"Safe Decentralized Multi-Agent Control using Black-Box Predictors, Conformal Decision Policies, and Control Barrier Functions.",https://doi.org/10.1109/ICRA55743.2025.11128015
An End-to-End Learning-Based Multi-Sensor Fusion for Autonomous Vehicle Localization.,https://doi.org/10.1109/ICRA55743.2025.11128443
"Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception.",https://doi.org/10.1109/ICRA55743.2025.11127412
"VIP-Dock: Vision, Inertia, and Pressure Sensor Fusion for Underwater Docking with Optical Beacon Guidance.",https://doi.org/10.1109/ICRA55743.2025.11128231
Heterogeneous Sensor Fusion and Active Perception for Transparent Object Reconstruction with a PDM2Sensor and a Camera.,https://doi.org/10.1109/ICRA55743.2025.11128748
DA-Fusion: Deformable Attention-Based RGB-D Fusion Transformer for Unseen Object Instance Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11128151
NDOB-Based Control of a UAV with Delta-Arm Considering Manipulator Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11127414
Flapping-Wing Flying Robot with Integrated Dual-Arm Scissors-Type Flora Sampling System.,https://doi.org/10.1109/ICRA55743.2025.11128106
Reliable Aerial Manipulation: Combining Visual Tracking with Range Sensing for Robust Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128682
Safety-Critical Control for Aerial Physical Interaction in Uncertain Environment.,https://doi.org/10.1109/ICRA55743.2025.11127757
SPIBOT: A Drone-Tethered Mobile Gripper for Robust Aerial Object Retrieval in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11128830
Knowledge-Driven Visual Target Navigation: Dual Graph Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128296
Learning to Predict the Future from Monocular Vision for Efficient Human-Aware Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128372
DP-Habitat: Bridging the Gap Between Simulation and Reality for Visual Navigation in Dynamic Pedestrian Environments.,https://doi.org/10.1109/ICRA55743.2025.11128036
X-MOBILITY: End-to-End Generalizable Navigation via World Modeling.,https://doi.org/10.1109/ICRA55743.2025.11128692
Map-SemNav: Advancing Zero-Shot Continuous Vision-and-Language Navigation Through Visual Semantics and Map Integration.,https://doi.org/10.1109/ICRA55743.2025.11127282
Diver to Robot Communication Underwater.,https://doi.org/10.1109/ICRA55743.2025.11128628
SIMP: Real-Time Energy and Time-Efficient 3D Motion Planning for Bio-Inspired AUVs.,https://doi.org/10.1109/ICRA55743.2025.11127643
End-to-End Underwater Multi-View Stereo for Dense Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128539
UR-MVO: Robust Monocular Visual Odometry for Underwater Scenarios.,https://doi.org/10.1109/ICRA55743.2025.11128500
SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model.,https://doi.org/10.1109/ICRA55743.2025.11128502
MOVE: Multi-Skill Omnidirectional Legged Locomotion With Limited View in 3D Environments.,https://doi.org/10.1109/ICRA55743.2025.11127928
Generating Diverse Challenging Terrains for Legged Robots Using Quality-Diversity Algorithm.,https://doi.org/10.1109/ICRA55743.2025.11128362
Added Mass and Accuracy of the FF -SLIP Model for Legged Swimming.,https://doi.org/10.1109/ICRA55743.2025.11128491
A Virtual Gravity Controller for Efficient Underactuated Biped Robots.,https://doi.org/10.1109/ICRA55743.2025.11128793
Stair Climbing of a Transformable Robot Using Varying Leg-Wheel Contact Points.,https://doi.org/10.1109/ICRA55743.2025.11127701
"""Hierarchy of Needs"" for Robots: Control Synthesis for Compositions of Hierarchical, Complex Objectives.",https://doi.org/10.1109/ICRA55743.2025.11128842
RM4D: A Combined Reachability and Inverse Reachability Map for Common 6-/7-Axis Robot Arms by Dimensionality Reduction to 4D.,https://doi.org/10.1109/ICRA55743.2025.11128095
An Average-Distance Minimizing Motion Sweep for Bounded Spatial Objects and Its Application in Bzier-Like Freeform Motion Generation.,https://doi.org/10.1109/ICRA55743.2025.11128523
Stop-N-Go: Search-Based Conflict Resolution for Motion Planning of Multiple Robotic Manipulators.,https://doi.org/10.1109/ICRA55743.2025.11127576
Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds for Coordinated Multi-Robot Mobile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127991
Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning.,https://doi.org/10.1109/ICRA55743.2025.11128488
Heuristically Guided Compilation for Task Assignment and Path Finding.,https://doi.org/10.1109/ICRA55743.2025.11128316
Safety-Critical Control with Saliency Detection for Mobile Robots in Dynamic Multi-Obstacle Environments.,https://doi.org/10.1109/ICRA55743.2025.11128721
Safe Control of Quadruped in Varying Dynamics via Safety Index Adaptation.,https://doi.org/10.1109/ICRA55743.2025.11127362
Updating Robot Safety Representations Online From Natural Language Feedback.,https://doi.org/10.1109/ICRA55743.2025.11127680
Distributed Perception Aware Safe Leader Follower System via Control Barrier Methods.,https://doi.org/10.1109/ICRA55743.2025.11127883
Gesturing Towards Efficient Robot Control: Exploring Sensor Placement and Control Modes for Mid-Air Human-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11127519
Understanding Dynamic Human-Robot Proxemics in the Case of Four-Legged Canine-Inspired Robots.,https://doi.org/10.1109/ICRA55743.2025.11128622
Autonomous Navigation in Crowded Space Using Multi-Sensory Data Fusion.,https://doi.org/10.1109/ICRA55743.2025.11127865
Feasibility-Aware Imitation Learning from Observations Through a Hand-Mounted Demonstration Interface.,https://doi.org/10.1109/ICRA55743.2025.11127364
Design of a Novel Pneumatic Soft Gripper for Robust Adaptive Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127397
Hybrid Gripper with Passive Pneumatic Soft Joints for Grasping Deformable Thin Objects.,https://doi.org/10.1109/ICRA55743.2025.11127915
Dexterous Three-Finger Gripper based on Offset Trimmed Helicoids (OTHs).,https://doi.org/10.1109/ICRA55743.2025.11128534
Improving Grip Stability Using Passive Compliant Microspine Arrays for Soft Robots in Unstructured Terrain.,https://doi.org/10.1109/ICRA55743.2025.11128855
Improving Coverage Performance of a Size-Reconfigurable Robot Based on Overlapping and Reconfiguration Reduction Criteria.,https://doi.org/10.1109/ICRA55743.2025.11128418
Enhancing Connection Strength in Freeform Modular Reconfigurable Robots Through Holey Sphere and Gripper Mechanisms.,https://doi.org/10.1109/ICRA55743.2025.11128097
Learning Dexterous Bimanual Catch Skills Through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127559
Flat'n'Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127794
TWIN: Two-handed Intelligent Benchmark for Bimanual Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128527
Active Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128253
Force-Conditioned Diffusion Policies for Compliant Sheet Separation Tasks in Bimanual Robotic Cells.,https://doi.org/10.1109/ICRA55743.2025.11127816
Online Trajectory Replanner for Dynamically Grasping Irregular Objects.,https://doi.org/10.1109/ICRA55743.2025.11128477
Real-Time Grasp Quality in Boundary-Constrained Granular Swarm Robots.,https://doi.org/10.1109/ICRA55743.2025.11128071
Learning Dual-Arm Coordination for Grasping Large Flat Objects.,https://doi.org/10.1109/ICRA55743.2025.11127654
Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization.,https://doi.org/10.1109/ICRA55743.2025.11128781
Improving Indoor Localization Accuracy by Using an Efficient Implicit Neural Map Representation.,https://doi.org/10.1109/ICRA55743.2025.11128666
Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11127943
LiLoc: Lifelong Localization Using Adaptive Submap Joining and Egocentric Factor Graph.,https://doi.org/10.1109/ICRA55743.2025.11127560
Chemistry3D: Robotic Interaction Toolkit for Chemistry Experiments.,https://doi.org/10.1109/ICRA55743.2025.11128846
Introducing KUGE: A Simultaneous Control Co-Design Architecture and its Application to Aerial Robotics Development.,https://doi.org/10.1109/ICRA55743.2025.11127868
HEROES: Unreal Engine-based Human and Emergency Robot Operation Education System.,https://doi.org/10.1109/ICRA55743.2025.11128532
On the Necessity of Real-Time Principles in GPU-Driven Autonomous Robots.,https://doi.org/10.1109/ICRA55743.2025.11128627
HPRM: High-Performance Robotic Middleware for Intelligent Autonomous Systems.,https://doi.org/10.1109/ICRA55743.2025.11128519
Learning Optimal Design Manifolds to Design More Practical Robotic Systems.,https://doi.org/10.1109/ICRA55743.2025.11128579
Monotone Subsystem Decomposition for Efficient Multi-Objective Robot Design.,https://doi.org/10.1109/ICRA55743.2025.11128384
Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing.,https://doi.org/10.1109/ICRA55743.2025.11128474
AeroSafe: Mobile Indoor Air Purification Using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed.,https://doi.org/10.1109/ICRA55743.2025.11128600
Sampling-Based Grasp and Collision Prediction for Assisted Teleoperation.,https://doi.org/10.1109/ICRA55743.2025.11128199
Inverse Mixed Strategy Games with Generative Trajectory Models.,https://doi.org/10.1109/ICRA55743.2025.11128615
Atom: Adaptive Theory-of-Mind-Based Human Motion Prediction in Long-Term Human-Robot Interactions.,https://doi.org/10.1109/ICRA55743.2025.11127697
Learning Dynamic Weight Adjustment for Spatial-Temporal Trajectory Planning in Crowd Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128766
COLLAGE: Collaborative Human-Agent Interaction Generation Using Hierarchical Latent Diffusion and Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128777
LUMOS: Language-Conditioned Imitation Learning with World Models.,https://doi.org/10.1109/ICRA55743.2025.11127988
LIMT: Language-Informed Multi-Task Visual World Models.,https://doi.org/10.1109/ICRA55743.2025.11128817
Towards Robust Autonomous Driving: Conditional Multimodal Large Language Models for Fine-Grained Perception.,https://doi.org/10.1109/ICRA55743.2025.11128326
Automated Hybrid Reward Scheduling Via Large Language Models for Robotic Skill Learning.,https://doi.org/10.1109/ICRA55743.2025.11127726
RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127525
A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards.,https://doi.org/10.1109/ICRA55743.2025.11127585
Learning Task Specifications from Demonstrations as Probabilistic Automata.,https://doi.org/10.1109/ICRA55743.2025.11128815
Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments.,https://doi.org/10.1109/ICRA55743.2025.11127857
R+X: Retrieval and Execution from Everyday Human Videos.,https://doi.org/10.1109/ICRA55743.2025.11128322
ARCap: Collecting High-Quality Human Demonstrations for Robot Learning with Augmented Reality Feedback.,https://doi.org/10.1109/ICRA55743.2025.11128717
XMoP: Whole-Body Control Policy for Zero-Shot Cross-Embodiment Neural Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127979
KALM: Keypoint Abstraction Using Large Models for Object-Relative Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128681
AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11127564
Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128716
Interactive4D: Interactive 4D LiDAR Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127281
Robust Scene Change Detection Using Visual Foundation Models and Cross-Attention Mechanisms.,https://doi.org/10.1109/ICRA55743.2025.11128568
LaB-CL: Localized and Balanced Contrastive Learning for Improving Parking Slot Detection.,https://doi.org/10.1109/ICRA55743.2025.11127470
Dual-Conditioned Temporal Diffusion Modeling for Driving Scene Generation.,https://doi.org/10.1109/ICRA55743.2025.11128056
RL-OGM-Parking: Lidar OGM-Based Hybrid Reinforcement Learning Planner for Autonomous Parking.,https://doi.org/10.1109/ICRA55743.2025.11128703
Multi-Task Invariant Representation Imitation Learning for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128522
Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127665
DISC: Dataset for Analyzing Driving Styles in Simulated Crashes for Mixed Autonomy.,https://doi.org/10.1109/ICRA55743.2025.11127763
HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM.,https://doi.org/10.1109/ICRA55743.2025.11127551
Gassidy: Gaussian Splatting SLAM in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127678
Large-Scale Gaussian Splatting SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128359
OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding.,https://doi.org/10.1109/ICRA55743.2025.11127983
SAP-SLAM: Semantic-Assisted Perception SLAM with 3D Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127553
Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and LiDAR-Inertial-Camera Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128712
Risk-Averse Model Predictive Control for Racing in Adverse Conditions.,https://doi.org/10.1109/ICRA55743.2025.11127729
Safety Guaranteed Robust Multi-Agent Reinforcement Learning with Hierarchical Control for Connected and Automated Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128447
Does Bilevel Optimization Result in More Competitive Racing Behavior?,https://doi.org/10.1109/ICRA55743.2025.11128087
Gate-Aware Online Planning for Two-Player Autonomous Drone Racing.,https://doi.org/10.1109/ICRA55743.2025.11127454
FlatFusion: Delving Into Details of Sparse Transformer-Based Camera-LiDAR Fusion for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128084
A2DO: Adaptive Anti-Degradation Odometry with Deep Multi-Sensor Fusion for Autonomous Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128429
Tunable Virtual IMU Frame by Weighted Averaging of Multiple Non-Collocated IMUs.,https://doi.org/10.1109/ICRA55743.2025.11127527
WildFusion: Multimodal Implicit 3D Reconstructions in the Wild.,https://doi.org/10.1109/ICRA55743.2025.11127508
Steering Prediction via a Multi-Sensor System for Autonomous Racing.,https://doi.org/10.1109/ICRA55743.2025.11128135
Inverse Kinematics on Guiding Vector Fields for Robot Path Following.,https://doi.org/10.1109/ICRA55743.2025.11127300
Dragonfly Drone: A Novel Tilt-Rotor Aerial Platform with Body-Morphing Capability.,https://doi.org/10.1109/ICRA55743.2025.11127712
An Omnidirectional Non-Tethered Aerial Prototype with Fixed Uni-Directional Thrusters.,https://doi.org/10.1109/ICRA55743.2025.11128060
Dense Fixed-Wing Swarming Using Receding-Horizon NMPC.,https://doi.org/10.1109/ICRA55743.2025.11127916
ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11128823
MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model.,https://doi.org/10.1109/ICRA55743.2025.11127837
A Full-Optical Pretouch Dual-Modal and Dual-Mechanism (PDM2) Sensor for Robotic Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127586
Learning Active Tactile Perception Through Belief-Space Control.,https://doi.org/10.1109/ICRA55743.2025.11127425
Detection of Fast-Moving Objects with Neuromorphic Hardware.,https://doi.org/10.1109/ICRA55743.2025.11127960
Lightstereo: Channel Boost is All You Need for Efficient 2D Cost Aggregation.,https://doi.org/10.1109/ICRA55743.2025.11127711
"Surfaceaug: Toward Versatile, Multimodally Consistent Ground Truth Sampling.",https://doi.org/10.1109/ICRA55743.2025.11127603
Uncertainty-Guided Enhancement on Driving Perception System Via Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11127651
Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128273
Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11128498
Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow.,https://doi.org/10.1109/ICRA55743.2025.11127947
V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception.,https://doi.org/10.1109/ICRA55743.2025.11128005
IMOST: Incremental Memory Mechanism with Online Self-Supervision for Continual Traversability Learning.,https://doi.org/10.1109/ICRA55743.2025.11127949
SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation.,https://doi.org/10.1109/ICRA55743.2025.11128800
Motion Tracks: A Unified Representation for Human-Robot Transfer in Few-Shot Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128834
Discrete Policy: Learning Disentangled Action Space for Multi-Task Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127630
AnyCar to Anywhere: Learning Universal Dynamics Model for Agile and Adaptive Mobility.,https://doi.org/10.1109/ICRA55743.2025.11128396
Learning Dynamics of a Ball with Differentiable Factor Graph and Roto-Translational Invariant Representations.,https://doi.org/10.1109/ICRA55743.2025.11127887
FutureNet-LoF: Joint Trajectory Prediction and Lane Occupancy Field Prediction with Future Context Encoding.,https://doi.org/10.1109/ICRA55743.2025.11128073
Hierarchical Reinforcement Learning for Safe Mapless Navigation with Congestion Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128867
Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception with Deep Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128829
Multi-Goal Motion Memory.,https://doi.org/10.1109/ICRA55743.2025.11128438
Dual-BEV Nav: Dual-Layer BEV-Based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments.,https://doi.org/10.1109/ICRA55743.2025.11128157
Risk-Aware Integrated Task and Motion Planning for Versatile Snake Robots Under Localization Failures.,https://doi.org/10.1109/ICRA55743.2025.11127363
Scalable Multi-Agent Surveillance: a Kernel-Based Approach.,https://doi.org/10.1109/ICRA55743.2025.11127616
Contingency Formation Planning for Interactive Drone Light Shows.,https://doi.org/10.1109/ICRA55743.2025.11128249
Design of a Formation Control System to Assist Human Operators in Flying a Swarm of Robotic Blimps.,https://doi.org/10.1109/ICRA55743.2025.11128354
In-Context Learning Enables Robot Action Prediction in LLMs.,https://doi.org/10.1109/ICRA55743.2025.11128807
UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127736
ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution.,https://doi.org/10.1109/ICRA55743.2025.11128414
Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark and LLM-Guided 3D Policy.,https://doi.org/10.1109/ICRA55743.2025.11127315
Discovering Object Attributes by Prompting Large Language Models With Perception-Action Apis.,https://doi.org/10.1109/ICRA55743.2025.11127632
ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127622
Physical Simulation with Force Feedback Aids Robot Factors Design.,https://doi.org/10.1109/ICRA55743.2025.11128203
Environmental Map Learning with Multiple-Robots.,https://doi.org/10.1109/ICRA55743.2025.11128150
SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building.,https://doi.org/10.1109/ICRA55743.2025.11127798
Unified Adaptive and Cooperative Planning Using Multi-Task Coregionalized Gaussian Processes.,https://doi.org/10.1109/ICRA55743.2025.11127374
COIGAN: Controllable Object Inpainting Through Generative Adversarial Network for Defect Synthesis in Data Augmentation.,https://doi.org/10.1109/ICRA55743.2025.11127765
Diffusion Based Robust LiDAR Place Recognition.,https://doi.org/10.1109/ICRA55743.2025.11127534
Open-Loop Position Control of a Miniature Magnetic Robot Using Two-Dimensional Divergence Control of a Magnetic Force.,https://doi.org/10.1109/ICRA55743.2025.11128398
An Equilibrium Analysis of Magnetic Quadrupole Force Field With Applications to Microrobotic Swarm Coordination.,https://doi.org/10.1109/ICRA55743.2025.11127638
Ensemble Control of a 2-DOF Parallel Link Arm in a Capsule Robot Using Oscillating External Magnetic Fields.,https://doi.org/10.1109/ICRA55743.2025.11127696
Deep Reinforcement Learning-Based Semi-Autonomous Control for Magnetic Micro-Robot Navigation with Immersive Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128848
OMASTAR Optimal Magnetic Actuation System Arrangement.,https://doi.org/10.1109/ICRA55743.2025.11128287
Measuring DNA Microswimmer Locomotion in Complex Flow Environments.,https://doi.org/10.1109/ICRA55743.2025.11127908
From Cognition to Precognition: A Future-Aware Framework for Social Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127910
OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128004
Arena 4.0: a Comprehensive Ros2 Development and Benchmarking Platform for Human-Centric Navigation Using Generative-Model-Based Environment Generation.,https://doi.org/10.1109/ICRA55743.2025.11127635
Enhancing Adaptivity of Two-Fingered Object Reorientation Using Tactile-Based Online Optimization of Deconstructed Actions.,https://doi.org/10.1109/ICRA55743.2025.11127901
A Full-Cycle Assembly Operation: From Digital Planning to Trajectory Execution Using a Robotic Arm.,https://doi.org/10.1109/ICRA55743.2025.11128356
Robust Nonprehensile Dynamic Object Transportation: A Closed-Loop Sensitivity Approach.,https://doi.org/10.1109/ICRA55743.2025.11127885
Hierarchical Contact-Rich Trajectory Optimization for Multi-Modal Manipulation Using Tight Convex Relaxations.,https://doi.org/10.1109/ICRA55743.2025.11127667
Optimizing Complex Control Systems with Differentiable Simulators: A Hybrid Approach to Reinforcement Learning and Trajectory Planning.,https://doi.org/10.1109/ICRA55743.2025.11127746
TransformerMPC: Accelerating Model Predictive Control via Transformers.,https://doi.org/10.1109/ICRA55743.2025.11128126
A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling.,https://doi.org/10.1109/ICRA55743.2025.11128850
C-Uniform Trajectory Sampling for Fast Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127482
ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128869
Composite Learning Neural Network Tracking Control of Articulated Soft Robots.,https://doi.org/10.1109/ICRA55743.2025.11128701
Multi-Segment Soft Robot Control Via Deep Koopman-Based Model Predictive Control.,https://doi.org/10.1109/ICRA55743.2025.11127925
Physics-Informed Split Koopman Operators for Data-Efficient Soft Robotic Simulation.,https://doi.org/10.1109/ICRA55743.2025.11127545
Robust Swimming Controller for Soft Robots via Drop-Out Learning.,https://doi.org/10.1109/ICRA55743.2025.11128123
Optimal Gait Control for a Tendon-Driven Soft Quadruped Robot by Model-Based Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128611
VascularPilot3D: Toward a 3D Fully Autonomous Navigation for Endovascular Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127370
Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization.,https://doi.org/10.1109/ICRA55743.2025.11128773
Towards Evaluating the User Comfort and Experience of a Novel Steerable Drilling Robotic System in Pedicle Screw Fixation Procedures: A User Study.,https://doi.org/10.1109/ICRA55743.2025.11128706
Minimally Invasive Endotracheal Inside-Out Flexible Needle Driving System Towards Microendoscope-Guided Robotic Tracheostomy.,https://doi.org/10.1109/ICRA55743.2025.11127689
Evaluating Global Geo-Alignment for Precision Learned Autonomous Vehicle Localization Using Aerial Data.,https://doi.org/10.1109/ICRA55743.2025.11128282
Under Pressure: Altimeter-Aided ICP for 3D Maps Consistency.,https://doi.org/10.1109/ICRA55743.2025.11128024
Neural Ranging Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128550
GazeHTA: End-to-End Gaze Target Detection with Head-Target Association.,https://doi.org/10.1109/ICRA55743.2025.11128763
Gaze and Go: Harnessing Visual Attention Valence in Upper-Limb Robotic Rehabilitation With Tailored Gamification and Eye Tracking for Neuroplasticity.,https://doi.org/10.1109/ICRA55743.2025.11127634
Wearable Soft Sensing Band with Stretchable Sensors for Torque Estimation and Hand Gesture Recognition.,https://doi.org/10.1109/ICRA55743.2025.11128378
Plug-and-Play Multi-Domain Fusion Adaptation for Cross-Subject EEG-Based Motor Imagery Classification.,https://doi.org/10.1109/ICRA55743.2025.11128281
SpatialBot: Precise Spatial Understanding with Vision Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128671
Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust.,https://doi.org/10.1109/ICRA55743.2025.11128017
KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation Without Robot Data.,https://doi.org/10.1109/ICRA55743.2025.11128156
GHIL-Glue: Hierarchical Control with Filtered Subgoal Images.,https://doi.org/10.1109/ICRA55743.2025.11128025
Simultaneous Localization and Affordance Prediction of Tasks from Egocentric Video.,https://doi.org/10.1109/ICRA55743.2025.11127830
Quart-Online: Latency-Free Multimodal Large Language Model for Quadruped Robot Learning.,https://doi.org/10.1109/ICRA55743.2025.11127693
IntelliRMS: A Robotic Manipulation System for Domain-Specific Tasks Using Vision and Language Foundational Models.,https://doi.org/10.1109/ICRA55743.2025.11127298
SCA3D: Enhancing Cross-Modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation.,https://doi.org/10.1109/ICRA55743.2025.11128851
TrajSSL: Trajectory-Enhanced Semi-Supervised 3D Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11128266
Single-Shot Metric Depth from Focused Plenoptic Cameras.,https://doi.org/10.1109/ICRA55743.2025.11128276
TREND: Tri-Teaching for Robust Preference-based Reinforcement Learning with Demonstrations.,https://doi.org/10.1109/ICRA55743.2025.11128278
SynerGuard: A Robust Framework for Point Cloud Classification via Local Geometry and Spatial Topology.,https://doi.org/10.1109/ICRA55743.2025.11127583
Is Discretization Fusion All You Need for Collaborative Perception?,https://doi.org/10.1109/ICRA55743.2025.11128776
Tri-AutoAug: Single Domain Generalization for Bird's-Eye-View 3D Object Detection Through Pixel-2D-3D Features.,https://doi.org/10.1109/ICRA55743.2025.11128244
Interpretable Active Inference Gait Control Learning.,https://doi.org/10.1109/ICRA55743.2025.11128724
DOPT: D-Learning with Off-Policy Target toward Sample Efficiency and Fast Convergence Control.,https://doi.org/10.1109/ICRA55743.2025.11127827
DFM: Deep Fourier Mimic for Expressive Dance Motion Learning.,https://doi.org/10.1109/ICRA55743.2025.11128480
Uncertainty-Aware Deep Reinforcement Learning with Calibrated Quantile Regression and Evidential Learning.,https://doi.org/10.1109/ICRA55743.2025.11127742
EMATO: Energy-Model-Aware Trajectory Optimization for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127833
Task-Oriented Pre-Training for Drivable Area Detection.,https://doi.org/10.1109/ICRA55743.2025.11127457
UA-PnP: Uncertainty-Aware End-to-End Bird's Eye View Visual Perception and Prediction for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127861
HGAT-CP: Heterogeneous Graph Attention Network for Collision Prediction in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11128557
SE-STDGNN: A Self-Evolving Spatial-Temporal Directed Graph Neural Network for Multi-Vehicle Trajectory Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128077
A Generalized Control Revision Method for Autonomous Driving Safety.,https://doi.org/10.1109/ICRA55743.2025.11128206
DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-Temporal Fusion.,https://doi.org/10.1109/ICRA55743.2025.11127668
Hier-SLAM: Scaling-Up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127775
ROD: RGB-Only Fast and Efficient Off-Road Freespace Detection.,https://doi.org/10.1109/ICRA55743.2025.11127491
JORD: A Benchmark Dataset for Off-Road LiDAR Place Recognition and SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128215
Self-Reflective Perceptual Adaptation for Robust Ground Navigation in Unstructured Off-Road Environments.,https://doi.org/10.1109/ICRA55743.2025.11127392
Dynamics Modeling Using Visual Terrain Features for High-Speed Autonomous Off-Road Driving.,https://doi.org/10.1109/ICRA55743.2025.11128031
Digital Twins Meet the Koopman Operator: Data-Driven Learning for Robust Autonomy.,https://doi.org/10.1109/ICRA55743.2025.11128858
Off-Road Freespace Detection with LiDAR-Camera Fusion and Self-Distillation.,https://doi.org/10.1109/ICRA55743.2025.11128182
Robust 4D Radar-Aided Inertial Navigation for Aerial Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128496
Semi-Elastic LiDAR-Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128078
DOGE: An Extrinsic Orientation and Gyroscope Bias Estimation for Visual-Inertial Odometry Initialization.,https://doi.org/10.1109/ICRA55743.2025.11127321
GaRLIO: Gravity Enhanced Radar-LiDAR-Inertial Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128334
Flying Through Moving Gates without Full State Estimation.,https://doi.org/10.1109/ICRA55743.2025.11127465
Collapsible Airfoil Single Actuator ROtor-Craft (CASARO) - Construction and Analysis of a Soft Rotary Wing Robot.,https://doi.org/10.1109/ICRA55743.2025.11128632
VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial Robots.,https://doi.org/10.1109/ICRA55743.2025.11128463
Distributed Loitering Synchronization with Fixed-Wing UAVs.,https://doi.org/10.1109/ICRA55743.2025.11128219
A Map-Free Deep Learning-Based Framework for Gate-to-Gate Monocular Visual Navigation Aboard Miniaturized Aerial Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128029
Offline Adaptation of Quadruped Locomotion Using Diffusion Models.,https://doi.org/10.1109/ICRA55743.2025.11128726
High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures.,https://doi.org/10.1109/ICRA55743.2025.11128575
HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots.,https://doi.org/10.1109/ICRA55743.2025.11128549
Learning Humanoid Locomotion with Perceptive Internal Model.,https://doi.org/10.1109/ICRA55743.2025.11128333
A Learning Framework for Diverse Legged Robot Locomotion Using Barrier-Based Style Rewards.,https://doi.org/10.1109/ICRA55743.2025.11128517
WildLMa: Long Horizon Loco-Manipulation in the Wild.,https://doi.org/10.1109/ICRA55743.2025.11128535
Drive with the Flow.,https://doi.org/10.1109/ICRA55743.2025.11128822
Potential Fields as Scene Affordance for Behavior Change-Based Visual Risk Object Identification.,https://doi.org/10.1109/ICRA55743.2025.11128295
SCAM-P: Spatial Channel Attention Module for Panoptic Driving Perception.,https://doi.org/10.1109/ICRA55743.2025.11128461
IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain.,https://doi.org/10.1109/ICRA55743.2025.11127814
Fast LiDAR Data Generation with Rectified Flows.,https://doi.org/10.1109/ICRA55743.2025.11127894
LamPro: Multi-Prototype Representation Learning for Enhanced Visual Pattern Recognition.,https://doi.org/10.1109/ICRA55743.2025.11127977
SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement.,https://doi.org/10.1109/ICRA55743.2025.11127882
Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation Learning of Vision-Based Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127749
Towards Open-Ended Robotic Exploration Using Vision-Inspired Similarity and Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11128592
MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception.,https://doi.org/10.1109/ICRA55743.2025.11127302
Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap.,https://doi.org/10.1109/ICRA55743.2025.11128318
Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions.,https://doi.org/10.1109/ICRA55743.2025.11128661
Autonomous Navigation in Ice-Covered Waters with Learned Predictions on Ship-Ice Interactions.,https://doi.org/10.1109/ICRA55743.2025.11128202
iKap: Kinematics-Aware Planning with Imperative Learning.,https://doi.org/10.1109/ICRA55743.2025.11127660
Generalized Mission Planning for Heterogeneous Multi-Robot Teams via LLM-Constructed Hierarchical Trees.,https://doi.org/10.1109/ICRA55743.2025.11128711
Efficient Coordination and Synchronization of Multi-Robot Systems Under Recurring Linear Temporal Logic.,https://doi.org/10.1109/ICRA55743.2025.11127554
HULK: Large-Scale Hierarchical Coordination Under Continual and Uncertain Temporal Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127952
COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127808
LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner.,https://doi.org/10.1109/ICRA55743.2025.11127951
FlyKites: Human-Centric Interactive Exploration and Assistance Under Limited Communication.,https://doi.org/10.1109/ICRA55743.2025.11128546
Work Smarter Not Harder: Simple Imitation Learning with CS-PIBT Outperforms Large-Scale Imitation Learning for MAPF.,https://doi.org/10.1109/ICRA55743.2025.11128836
Mastering Agile Jumping Skills from Simple Practices with Iterative Learning Control.,https://doi.org/10.1109/ICRA55743.2025.11128629
Agile Continuous Jumping in Discontinuous Terrains.,https://doi.org/10.1109/ICRA55743.2025.11127314
High Accuracy Aerial Maneuvers on Legged Robots using Variational Integrator Discretized Trajectory Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128413
Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128080
Stage-Wise Reward Shaping for Acrobatic Robots: A Constrained Multi-Objective Reinforcement Learning Approach.,https://doi.org/10.1109/ICRA55743.2025.11128552
Design and Implementation of a Swimming and Walking Quadruped for Seafloor Exploration.,https://doi.org/10.1109/ICRA55743.2025.11127516
Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain.,https://doi.org/10.1109/ICRA55743.2025.11128639
FACET: Fast and Accurate Event-Based Eye Tracking Using Ellipse Modeling for Extended Reality.,https://doi.org/10.1109/ICRA55743.2025.11127327
"RaggeDi: Diffusion-Based State Estimation of Disordered Rags, Sheets, Towels and Blankets.",https://doi.org/10.1109/ICRA55743.2025.11128504
Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127604
Robotic Framework for Iterative and Adaptive Profile Grading of Sand.,https://doi.org/10.1109/ICRA55743.2025.11128288
Autonomous Excavation of Challenging Terrain using Oscillatory Primitives and Adaptive Impedance Control.,https://doi.org/10.1109/ICRA55743.2025.11128330
Diffusion-Based Self-Supervised Imitation Learning from Imperfect Visual Servoing Demonstrations for Robotic Glass Installation.,https://doi.org/10.1109/ICRA55743.2025.11127752
Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles.,https://doi.org/10.1109/ICRA55743.2025.11127745
Fast Online Learning of CLiFF-Maps in Changing Environments.,https://doi.org/10.1109/ICRA55743.2025.11127602
A Hybrid Approach to Indoor Social Navigation: Integrating Reactive Local Planning and Proactive Global Planning.,https://doi.org/10.1109/ICRA55743.2025.11127475
Overlapping Social Navigation Principles: A Framework for Social Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128863
Relative Velocity-Based Reward Model for Socially-Aware Navigation with Deep Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127606
Autonomous Continuous Capsulorhexis Based on a Force-Vision-Guided Robot System.,https://doi.org/10.1109/ICRA55743.2025.11127235
Ultrasound-Guided Robotic Blood Drawing and In Vivo Studies on Submillimetre Vessels of Rats.,https://doi.org/10.1109/ICRA55743.2025.11128596
Sensory Glove-Based Surgical Robot User Interface.,https://doi.org/10.1109/ICRA55743.2025.11128128
Self-Deformable Magnetic Miniature Robot for Traction Assistance in Endoscopic Submucosal Dissection.,https://doi.org/10.1109/ICRA55743.2025.11127846
Variable-Stiffness Nasotracheal Intubation Robot with Passive Buffering: A Modular Platform in Mannequin Studies.,https://doi.org/10.1109/ICRA55743.2025.11128279
SurgPose: a Dataset for Articulated Robotic Surgical Tool Pose Estimation and Tracking.,https://doi.org/10.1109/ICRA55743.2025.11127958
Real-Time Deformation-Aware Control for Autonomous Robotic Subretinal Injection Under iOCT Guidance.,https://doi.org/10.1109/ICRA55743.2025.11128595
6-DoF Shape Servoing of Deformable Objects in Co-Rotated Space of Modal Graph.,https://doi.org/10.1109/ICRA55743.2025.11128328
Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes.,https://doi.org/10.1109/ICRA55743.2025.11128551
One-Shot Video Imitation via Parameterized Symbolic Abstraction Graphs.,https://doi.org/10.1109/ICRA55743.2025.11128560
KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128038
"Label Anything: An Interpretable, High-Fidelity and Prompt-Free Annotator.",https://doi.org/10.1109/ICRA55743.2025.11128321
Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding.,https://doi.org/10.1109/ICRA55743.2025.11128818
Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127383
Intelligence Evaluation Methods for Autonomous Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128468
Na Vid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D Videos for Vision-and-Language Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128467
Generating Out-of-Distribution Scenarios Using Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127950
Resolution Optimal Motion Planning for Medical Needle Steering from Airway Walls in the Lung.,https://doi.org/10.1109/ICRA55743.2025.11128472
Self-Sufficient 5-DoF Discrete Global Localization for Magnetically-Actuated Endoscope in Bronchoscopy.,https://doi.org/10.1109/ICRA55743.2025.11128301
Intraoperative 3D Shape Estimation of Magnetic Soft Guidewire.,https://doi.org/10.1109/ICRA55743.2025.11128736
Semi-Autonomous 2.5D Control of Untethered Magnetic Suture Needle.,https://doi.org/10.1109/ICRA55743.2025.11127850
Steerable Tape-Spring Needle for Autonomous Sharp Turns Through Tissue.,https://doi.org/10.1109/ICRA55743.2025.11127659
Multi-Heuristic Robotic Bin Packing of Regular and Irregular Objects.,https://doi.org/10.1109/ICRA55743.2025.11128421
MultiTalk: Introspective and Extrospective Dialogue for Human-Environment-LLM Alignment.,https://doi.org/10.1109/ICRA55743.2025.11128486
Goal-Guided Reinforcement Learning: Leveraging Large Language Models for Long-Horizon Task Decomposition.,https://doi.org/10.1109/ICRA55743.2025.11128715
Trustworthy Robot Behavior Tree Generation Based on Multi-Source Heterogeneous Knowledge Graph.,https://doi.org/10.1109/ICRA55743.2025.11127406
Enabling In-Flight Metamorphosis in Multirotors with a Center-Driven Scissor Extendable Airframe for Adaptive Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128791
SafePCA: Enhancing Autonomous Robot Navigation in Dynamic Crowds Using Proximal Policy Optimization and Cellular Automata.,https://doi.org/10.1109/ICRA55743.2025.11127802
Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments.,https://doi.org/10.1109/ICRA55743.2025.11128102
Diff-Refiner: Enhancing Multi-Agent Trajectory Prediction with a Plug-and-Play Diffusion Refiner.,https://doi.org/10.1109/ICRA55743.2025.11127226
Scene-Aware Explainable Multimodal Trajectory Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128379
Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions.,https://doi.org/10.1109/ICRA55743.2025.11128569
The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation.,https://doi.org/10.1109/ICRA55743.2025.11128250
Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128561
TSPDiffuser: Diffusion Models as Learned Samplers for Traveling Salesperson Path Planning Problems.,https://doi.org/10.1109/ICRA55743.2025.11128405
Anticipatory Planning for Performant Long-Lived Robot in Large-Scale Home-Like Environments.,https://doi.org/10.1109/ICRA55743.2025.11128153
Scaling Diffusion Policy in Transformer to 1 Billion Parameters for Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128074
Implicit Contact Diffuser: Sequential Contact Reasoning With Latent Point Cloud Diffusion.,https://doi.org/10.1109/ICRA55743.2025.11128192
Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127641
PRESTO: Fast Motion Planning Using Diffusion Models Based on Key-Configuration Environment Representation.,https://doi.org/10.1109/ICRA55743.2025.11128590
Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension.,https://doi.org/10.1109/ICRA55743.2025.11128399
Improving Generalization Ability for 3D Object Detection by Learning Sparsity-Invariant Features.,https://doi.org/10.1109/ICRA55743.2025.11128274
Camera-Lidar Consistent Neural Radiance Fields.,https://doi.org/10.1109/ICRA55743.2025.11128072
Iterative Volume Fusion for Asymmetric Stereo Matching.,https://doi.org/10.1109/ICRA55743.2025.11127981
OccRWKV: Rethinking Efficient 3D Semantic Occupancy Prediction with Linear Complexity.,https://doi.org/10.1109/ICRA55743.2025.11128516
ZSORN: Language-Driven Object-Centric Zero-Shot Object Retrieval and Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128436
PRIDEV: A Plug-and-Play Refinement for Improved Depth Estimation in Videos.,https://doi.org/10.1109/ICRA55743.2025.11128698
Efficient Online Learning of Contact Force Models for Connector Insertion.,https://doi.org/10.1109/ICRA55743.2025.11127717
Flying Quadrotors in Tight Formations Using Learning-Based Model Predictive Control.,https://doi.org/10.1109/ICRA55743.2025.11127756
Learning Based MPC for Autonomous Driving Using a Low Dimensional Residual Model.,https://doi.org/10.1109/ICRA55743.2025.11128642
Modeling of Deformable Linear Objects Under Incomplete State Information.,https://doi.org/10.1109/ICRA55743.2025.11127734
Impedance Primitive-Augmented Hierarchical Reinforcement Learning for Sequential Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128462
Plug-and-Play Physics-Informed Learning Using Uncertainty Quantified Port-Hamiltonian Models.,https://doi.org/10.1109/ICRA55743.2025.11127683
DELTA: Decomposed Efficient Long-Term Robot Task Planning Using Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127838
Hey Robot! Personalizing Robot Navigation Through Model Predictive Control with a Large Language Model.,https://doi.org/10.1109/ICRA55743.2025.11128826
Large Language Model Based Autonomous Task Planning for Abstract Commands.,https://doi.org/10.1109/ICRA55743.2025.11128343
Self-Corrective Task Planning by Inverse Prompting with Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128028
Traffic Regulation-aware Path Planning with Regulation Databases and Vision-Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128730
GARAD-SLAM: 3D Gaussian Splatting for Real-Time Anti Dynamic SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128757
Optimizing NeRF-Based SLAM with Trajectory Smoothness Constraints.,https://doi.org/10.1109/ICRA55743.2025.11128806
MGSO: Monocular Real-Time Photometric SLAM with Efficient 3D Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127380
RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes.,https://doi.org/10.1109/ICRA55743.2025.11128286
FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field.,https://doi.org/10.1109/ICRA55743.2025.11128403
Kinematic-ICP: Enhancing LiDAR Odometry with Kinematic Constraints for Wheeled Mobile Robots Moving on Planar Surfaces.,https://doi.org/10.1109/ICRA55743.2025.11128503
GERA: Geometric Embedding for Efficient Point Registration Analysis.,https://doi.org/10.1109/ICRA55743.2025.11127232
KISS-Matcher: Fast and Robust Point Cloud Registration Revisited.,https://doi.org/10.1109/ICRA55743.2025.11127458
SANDRO: A Robust Solver with a Splitting Strategy for Point Cloud Registration.,https://doi.org/10.1109/ICRA55743.2025.11128360
Bridging In-Situ and Satellite Data: Enhancing Gas Concentration Estimation Through Integration of Data-Driven and Physics-Based Modeling.,https://doi.org/10.1109/ICRA55743.2025.11128290
A Novel Decomposed Feature-Oriented Framework for Open-Set Semantic Segmentation on LiDAR Data.,https://doi.org/10.1109/ICRA55743.2025.11128866
SAM-Guided Pseudo Label Enhancement for Multi-Modal 3D Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11128411
Robot Manipulation in Salient Vision Through Referring Image Segmentation and Geometric Constraints.,https://doi.org/10.1109/ICRA55743.2025.11128563
Boosting Cross-Spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127724
VideoSAM: Open-World Video Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11128804
Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128401
Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms.,https://doi.org/10.1109/ICRA55743.2025.11128861
Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets.,https://doi.org/10.1109/ICRA55743.2025.11128738
PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128382
Whole-Body End-Effector Pose Tracking.,https://doi.org/10.1109/ICRA55743.2025.11127964
MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models.,https://doi.org/10.1109/ICRA55743.2025.11128601
mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11127877
Human Activity Recognition by Using Enhanced Radar Point Cloud 2D Histograms and Doppler Feature Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128165
Estimating User Engagement in Human Robot Interaction Using a Dynamic Bayesian Network.,https://doi.org/10.1109/ICRA55743.2025.11128383
Hri-Free: Cognitive Robotic Simulation for Evaluating Embodied Social Attention Models.,https://doi.org/10.1109/ICRA55743.2025.11127539
An EEG Conformer Model for Error Feedback During Human-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11127307
Cross-Platform Learning-Based Fault Tolerant Surfacing Controller for Underwater Robots.,https://doi.org/10.1109/ICRA55743.2025.11127355
Optimizing Underwater Robot Navigation: A Study of DRL Algorithms and Multi-Modal Sensor Fusion.,https://doi.org/10.1109/ICRA55743.2025.11127836
PUGS: Perceptual Uncertainty for Grasp Selection in Underwater Environments.,https://doi.org/10.1109/ICRA55743.2025.11128019
Learning to Swim: Reinforcement Learning for 6-DOF Control of Thruster-Driven Autonomous Underwater Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128688
Underwater Motions Analysis and Control of a Coupling-Tiltable Unmanned Aerial-Aquatic Vehicle.,https://doi.org/10.1109/ICRA55743.2025.11128368
Whole-Body Control Through Narrow Gaps from Pixels to Action.,https://doi.org/10.1109/ICRA55743.2025.11128088
VisFly: An Efficient and Versatile Simulator for Training Vision-Based Flight.,https://doi.org/10.1109/ICRA55743.2025.11128458
Environment as Policy: Learning to Race in Unseen Tracks.,https://doi.org/10.1109/ICRA55743.2025.11127376
GMF: Gravitational Mass-Force Framework for Parametric Multi-Level Coordination in Multi-Robot and Swarm Robotic Systems.,https://doi.org/10.1109/ICRA55743.2025.11128543
Versatile Distributed Maneuvering With Generalized Formations Using Guiding Vector Fields.,https://doi.org/10.1109/ICRA55743.2025.11127716
Cooperative Distributed Model Predictive Control for Embedded Systems: Experiments with Hovercraft Formations.,https://doi.org/10.1109/ICRA55743.2025.11128032
Coordinated Multi-Robot Navigation with Formation Adaptation.,https://doi.org/10.1109/ICRA55743.2025.11128457
MARVEL: Multi-Agent Reinforcement Learning for Constrained Field-of-View Multi-Robot Exploration in Large-Scale Environments.,https://doi.org/10.1109/ICRA55743.2025.11127700
Reinforcement Learning Driven Multi-Robot Exploration via Explicit Communication and Density-Based Frontier Search.,https://doi.org/10.1109/ICRA55743.2025.11128566
Integrating Multi-Robot Adaptive Sampling and Informative Path Planning for Spatiotemporal Natural Environment Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128099
A Haptic Feedback Device Actuated by Electromagnetic Torque.,https://doi.org/10.1109/ICRA55743.2025.11127276
Vibrotactile Haptics with Soft Magnetoresponsive Surface Interface.,https://doi.org/10.1109/ICRA55743.2025.11127478
Haptic Shoulder for Rendering Biomechanically Accurate Joint Limits for Human-Robot Physical Interactions.,https://doi.org/10.1109/ICRA55743.2025.11127862
"Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly.",https://doi.org/10.1109/ICRA55743.2025.11127610
Robotic Dry-Stacking of Clochin with Irregular Stones.,https://doi.org/10.1109/ICRA55743.2025.11128654
Synthesizing Depowdering Trajectories for Robot Arms using Deep Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128066
World Model-Based Perception for Visual Legged Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128762
V-Pilot: A Velocity Vector Control Agent for Fixed-Wing UAVs from Imperfect Demonstrations.,https://doi.org/10.1109/ICRA55743.2025.11128340
Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning.,https://doi.org/10.1109/ICRA55743.2025.11128740
Model-Based Control Strategies Comparison of One Bionic Ankle Tensegrity Exoskeleton: BATE.,https://doi.org/10.1109/ICRA55743.2025.11127237
Human-Like Walking Motion Generation for Self-Balancing Lower Limb Rehabilitation Exoskeletons.,https://doi.org/10.1109/ICRA55743.2025.11128170
A Simple Dynamics Model for Cable Driven Continuum Robots with Actuator Coupling.,https://doi.org/10.1109/ICRA55743.2025.11127537
A Novel Tendon-Driven Articulated Continuum Robot with Stabilized Self-Locking Joints.,https://doi.org/10.1109/ICRA55743.2025.11128849
Tensiworm: A Novel Tensegrity Robot with Enhanced Peristaltic Locomotion Efficiency.,https://doi.org/10.1109/ICRA55743.2025.11127767
Accelerated Quasi-Static FEM for Real-Time Modeling of Continuum Robots with Multiple Contacts and Large Deformation.,https://doi.org/10.1109/ICRA55743.2025.11128335
vMF-Contact: Uncertainty-Aware Evidential Learning for Probabilistic Contact-Grasp in Noisy Clutter.,https://doi.org/10.1109/ICRA55743.2025.11127888
QuadWBG: Generalizable Quadrupedal Whole-Body Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128027
Composing Dextrous Grasping and In-Hand Manipulation via Scoring with a Reinforcement Learning Critic.,https://doi.org/10.1109/ICRA55743.2025.11127792
Bring Your Own Grasp Generator: Leveraging Robot Grasp Generation for Prosthetic Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127995
AIR-HLoc: Adaptive Retrieved Images Selection for Efficient Visual Localisation.,https://doi.org/10.1109/ICRA55743.2025.11127807
Neuraloc: Visual Localization in Neural Implicit Map With Dual Complementary Features.,https://doi.org/10.1109/ICRA55743.2025.11127718
LiftFeat: 3D Geometry-Aware Local Feature Matching.,https://doi.org/10.1109/ICRA55743.2025.11127853
DVS-Aware Visual Perception for Pose Estimation of Mobile Robots with Neuromorphic Implementation.,https://doi.org/10.1109/ICRA55743.2025.11128155
Feedback RoI Features Improve Aerial Object Detection.,https://doi.org/10.1109/ICRA55743.2025.11127970
Keypoint Detection and Description for Raw Bayer Images.,https://doi.org/10.1109/ICRA55743.2025.11128220
Delayed-Decision Motion Planning in the Presence of Multiple Predictions.,https://doi.org/10.1109/ICRA55743.2025.11128178
Stochastic Trajectory Prediction Under Unstructured Constraints.,https://doi.org/10.1109/ICRA55743.2025.11127647
A Control Barrier Function for Safe Navigation with Online Gaussian Splatting Maps.,https://doi.org/10.1109/ICRA55743.2025.11128723
A Skeleton-Based Topological Planner for Exploration in Complex Unknown Environments.,https://doi.org/10.1109/ICRA55743.2025.11128159
Safety-Critical Online Quadrotor Trajectory Planner for Agile Flights in Unknown Environments.,https://doi.org/10.1109/ICRA55743.2025.11127864
The Role of Tactile Sensing for Learning Reach and Grasp.,https://doi.org/10.1109/ICRA55743.2025.11127409
Task-Specific Embodied Tactile Sensing for Dexterous Hand.,https://doi.org/10.1109/ICRA55743.2025.11127318
TacDiffusion: Force-Domain Diffusion Policy for Precise Tactile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127334
UpViTaL: Unpaired Visual-Tactile Self-Supervised Representation Learning for Dexterous Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127230
Fostering Trust Through Gesture and Voice-Controlled Robot Trajectories in Industrial Human-Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11128617
Would You Trust Me Now? A Study on Trust Repair Strategies in Human-Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11127698
"Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner.",https://doi.org/10.1109/ICRA55743.2025.11127472
A Novel Computational Framework of Robot Trust for Human-Robot Teams.,https://doi.org/10.1109/ICRA55743.2025.11127674
Planning for Tabletop Object Rearrangement.,https://doi.org/10.1109/ICRA55743.2025.11127905
Da-Vil: Adaptive Dual-Arm Manipulation with Reinforcement Learning and Variable Impedance Control.,https://doi.org/10.1109/ICRA55743.2025.11127487
Goal-Driven Robotic Pushing Manipulation Under Uncertain Object Properties.,https://doi.org/10.1109/ICRA55743.2025.11127379
Synthesizing Grasps and Regrasps for Complex Manipulation Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128212
A Helping (Human) Hand in Kinematic Structure Estimation.,https://doi.org/10.1109/ICRA55743.2025.11127847
Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?,https://doi.org/10.1109/ICRA55743.2025.11127776
Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution.,https://doi.org/10.1109/ICRA55743.2025.11128194
ME-PATS: Mutually Enhancing Search-Based Planner and Learning-Based Agent for Tractor-Trailer Systems.,https://doi.org/10.1109/ICRA55743.2025.11127954
Jailbreaking LLM-Controlled Robots.,https://doi.org/10.1109/ICRA55743.2025.11128119
CaStL: Constraints as Specifications Through Llm Translation for Long-Horizon Task and Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127555
Skills Made to Order: Efficient Acquisition of Robot Cooking Skills Guided by Multiple Forms of Internet Data.,https://doi.org/10.1109/ICRA55743.2025.11128115
LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration for Planning Sequential Contact-Rich Manipulation Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127842
DARE: Diffusion Policy for Autonomous Robot Exploration.,https://doi.org/10.1109/ICRA55743.2025.11128196
NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127466
NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants.,https://doi.org/10.1109/ICRA55743.2025.11127507
FDPP: Fine-Tune Diffusion Policy with Human Preference.,https://doi.org/10.1109/ICRA55743.2025.11128127
Learning Diverse Robot Striking Motions with Diffusion Models and Kinematically Constrained Gradient Guidance.,https://doi.org/10.1109/ICRA55743.2025.11127310
Extended Friction Models for the Physics Simulation of Servo Actuators.,https://doi.org/10.1109/ICRA55743.2025.11128640
Hierarchically Accelerated Coverage Path Planning for Redundant Manipulators.,https://doi.org/10.1109/ICRA55743.2025.11128545
Decentralized Safe and Scalable Multi-Agent Control Under Limited Actuation.,https://doi.org/10.1109/ICRA55743.2025.11128344
Multi-Agent Collective Construction of General Modular Structures.,https://doi.org/10.1109/ICRA55743.2025.11128525
DiTer++: Diverse Terrain and Multi-Modal Dataset for Multi-Robot SLAM in Multi-Session Environments.,https://doi.org/10.1109/ICRA55743.2025.11128593
CELLmap: Enhancing LiDAR SLAM Through Elastic and Lightweight Spherical Map Representation.,https://doi.org/10.1109/ICRA55743.2025.11127331
Kalman-Filter-Based Pose Estimation of Cable-Driven Parallel Robots Using Cable-Length Measurements with Colored Noise.,https://doi.org/10.1109/ICRA55743.2025.11128427
A Unified End-to-End Network for Category-Level and Instance-Level Object Pose Estimation from RGB Images.,https://doi.org/10.1109/ICRA55743.2025.11128247
MonoLDP: LED Assisted Indoor Mobile Bot Monocular Depth Prediction and Pose Estimation System.,https://doi.org/10.1109/ICRA55743.2025.11128305
"LCSPose: Efficient, Accurate and Scalable Markerless 6-DoF Pose Estimation of a Quay Crane Spreader Based on LiDAR and Camera.",https://doi.org/10.1109/ICRA55743.2025.11128533
ZeroBP: Learning Position-Aware Correspondence for Zero-Shot 6D Pose Estimation in Bin-Picking.,https://doi.org/10.1109/ICRA55743.2025.11128347
Continuous Convolution for Automated Measurement of Sperm Flagella.,https://doi.org/10.1109/ICRA55743.2025.11127820
Adaptive Concertina Locomotion of a Robotic Snake Through Narrow Uncertain Channels.,https://doi.org/10.1109/ICRA55743.2025.11127631
Bio-Inspired Distributed Neural Locomotion Controller (D-NLC) for Robust Locomotion and Emergent Behaviors.,https://doi.org/10.1109/ICRA55743.2025.11128090
Reduced-Order Model-Based Gait Generation for Snake Robot Locomotion Using NMPC.,https://doi.org/10.1109/ICRA55743.2025.11128852
AquaMILR+: Design of an Untethered Limbless Robot for Complex Aquatic Terrain Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128548
Adapting Gait Frequency for Posture-Regulating Humanoid Push-Recovery via Hierarchical Model Predictive Control.,https://doi.org/10.1109/ICRA55743.2025.11127608
Robots with Attitude: Singularity-Free Quaternion-Based Model-Predictive Control for Agile Legged Robots.,https://doi.org/10.1109/ICRA55743.2025.11128267
Online Nonlinear MPC for Multimodal Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11127343
Terrain-Aware Model Predictive Control of Heterogeneous Bipedal and Aerial Robot Coordination for Search and Rescue Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128808
Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting.,https://doi.org/10.1109/ICRA55743.2025.11128374
Kinodynamic Model Predictive Control for Energy Efficient Locomotion of Legged Robots with Parallel Elasticity.,https://doi.org/10.1109/ICRA55743.2025.11128416
Dynamic Bipedal MPC with Foot-Level Obstacle Avoidance and Adjustable Step Timing.,https://doi.org/10.1109/ICRA55743.2025.11128284
Enhancing Robotic Perception with Low-Cost Fast Active Vision Achieving Sub-Millimeter Accurate Marker-Based Pose Estimation.,https://doi.org/10.1109/ICRA55743.2025.11127902
PhysPart: Physically Plausible Part Completion for Interactable Objects.,https://doi.org/10.1109/ICRA55743.2025.11127496
Generalizable Zero-Shot Object Pose Estimation for Bin-Picking.,https://doi.org/10.1109/ICRA55743.2025.11128200
Visuo-Tactile Object Pose Estimation for a Multi-Finger Robot Hand With Low-Resolution in-Hand Tactile Sensing.,https://doi.org/10.1109/ICRA55743.2025.11127966
Proactive Tactile Exploration for Object-Agnostic Shape Reconstruction from Minimal Visual Priors.,https://doi.org/10.1109/ICRA55743.2025.11127653
Multi-Layer Feature Exchange Transformer for Multi-View 6D Object Pose Estimation in Robot Bin Picking.,https://doi.org/10.1109/ICRA55743.2025.11127530
Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog Robots Assisting the Visually Impaired.,https://doi.org/10.1109/ICRA55743.2025.11128120
FitnessAgent: A Unified Agent Framework for Open-Set and Personalized Fitness Evaluation.,https://doi.org/10.1109/ICRA55743.2025.11128810
A Reinforcement Learning-Based Social Robot for Personalized Learning in Children with Autism.,https://doi.org/10.1109/ICRA55743.2025.11127369
Comparison of User Interface Paradigms for Assistive Robotic Manipulators.,https://doi.org/10.1109/ICRA55743.2025.11127869
VQA-Driven Event Maps for Assistive Navigation for People with Low Vision in Urban Environments.,https://doi.org/10.1109/ICRA55743.2025.11128754
Multi-View Stereo with Geometric Encoding for Dense Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128506
MicroASV: An Affordable 3D-Printed Centimeter-Scale Autonomous Surface Vehicle.,https://doi.org/10.1109/ICRA55743.2025.11127526
Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor.,https://doi.org/10.1109/ICRA55743.2025.11128722
Automated Planning Domain Inference for Task and Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127817
Shadow Program Inversion with Differentiable Planning: A Framework for Unified Robot Program Parameter and Trajectory Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128226
AlignBot: Aligning VLM-Powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots.,https://doi.org/10.1109/ICRA55743.2025.11128775
Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation.,https://doi.org/10.1109/ICRA55743.2025.11128650
Optimization-Based Task and Motion Planning Under Signal Temporal Logic Specifications Using Logic Network Flow.,https://doi.org/10.1109/ICRA55743.2025.11128784
"Integrating Active Sensing and Rearrangement Planning for Efficient Object Retrieval from Unknown, Confined, Cluttered Environments.",https://doi.org/10.1109/ICRA55743.2025.11128147
A Cooperative Bearing-Rate Approach for Observability-Enhanced Target Motion Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128217
Overlapping Free: Anchorless UWB-Assisted Relative Pose Estimation for Multi-Robot Systems.,https://doi.org/10.1109/ICRA55743.2025.11127904
Online Waypoint Recognition of Controlled Agents in Uncertain Environments.,https://doi.org/10.1109/ICRA55743.2025.11128799
MARF: Cooperative Multi-Agent Path Finding with Reinforcement Learning and Frenet Lattice in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11128009
Robust Self-Reconfiguration for Fault-Tolerant Control of Modular Aerial Robot Systems.,https://doi.org/10.1109/ICRA55743.2025.11127899
Deep Height Decoupling for Precise Vision-Based 3D Occupancy Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128708
RE0: Recognize Everything with 3D Zero-Shot Instance Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127468
PTQ4RIS: Post-Training Quantization for Referring Image Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11128841
LeAP: Consistent multi-domain 3D labeling using Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11128483
Hybrid State Estimation and Mode Identification of an Amphibious Robot.,https://doi.org/10.1109/ICRA55743.2025.11127611
LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments.,https://doi.org/10.1109/ICRA55743.2025.11127917
How About Them Apples: 3D Pose and Cluster Estimation of Apple Fruitlets in a Commercial Orchard.,https://doi.org/10.1109/ICRA55743.2025.11128653
Active Semantic Mapping with Mobile Manipulator in Horticultural Environments.,https://doi.org/10.1109/ICRA55743.2025.11127547
Surface Roughness Estimation for Terrain Perception.,https://doi.org/10.1109/ICRA55743.2025.11127990
RipGAN: A GAN-Based Rip Current Data Augmentation Method.,https://doi.org/10.1109/ICRA55743.2025.11128547
"Points, Images and Texts: Boosting Point Cloud Completion with Multi-Modal Features.",https://doi.org/10.1109/ICRA55743.2025.11127305
3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment.,https://doi.org/10.1109/ICRA55743.2025.11127295
MPI-Mamba: Cross Propagation Mamba for Multipath Interference Correction.,https://doi.org/10.1109/ICRA55743.2025.11127297
SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference.,https://doi.org/10.1109/ICRA55743.2025.11127834
Real-Time LiDAR Point Cloud Compression and Transmission for Resource-Constrained Robots.,https://doi.org/10.1109/ICRA55743.2025.11127859
E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128669
Improving Zero-Shot ObjectNav with Generative Communication.,https://doi.org/10.1109/ICRA55743.2025.11127870
Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127234
Language-Guided Object-Centric Diffusion Policy for Generalizable and Collision-Aware Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127231
This&That: Language-Gesture Controlled Video Generation for Robot Planning.,https://doi.org/10.1109/ICRA55743.2025.11128780
Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility and Safety Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11128109
Dynamic Gap: Safe Gap-based Navigation in Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127543
Conformalized Reachable Sets for Obstacle Avoidance with Spheres.,https://doi.org/10.1109/ICRA55743.2025.11128542
System-Level Safety Monitoring and Recovery for Perception Failures in Autonomous Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128400
Pneumatic Logic Systems for Selectively Operating Distributed Pneumatic Elements.,https://doi.org/10.1109/ICRA55743.2025.11128307
Helical Structured Soft Growing Robot for Hazardous Gas Suction in Inaccessible Environments.,https://doi.org/10.1109/ICRA55743.2025.11128325
Shape-Programming Robotic Reflectors for Wireless Networks.,https://doi.org/10.1109/ICRA55743.2025.11127474
MORF: Magnetic Origami Reprogramming and Folding System for Repeatably Reconfigurable Structures with Fold Angle Control.,https://doi.org/10.1109/ICRA55743.2025.11127789
Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles.,https://doi.org/10.1109/ICRA55743.2025.11128587
Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders.,https://doi.org/10.1109/ICRA55743.2025.11127652
Efficient Cross-Boundary Grasping in Stacked Clutter with Single-Visual Mapping Multi-Step.,https://doi.org/10.1109/ICRA55743.2025.11127976
Efficient Second-Order Cone Programming for the Close Enough Traveling Salesman Problem.,https://doi.org/10.1109/ICRA55743.2025.11127623
Decoupled Training Neural Solver for Dynamic Traveling Salesman Problem.,https://doi.org/10.1109/ICRA55743.2025.11128415
Multi-Drone-Truck Collaborative Delivery with En Route Operations: A Hierarchical MARL-Based Approach.,https://doi.org/10.1109/ICRA55743.2025.11128479
Risk-Aware Energy-Constrained UAV-UGV Cooperative Routing Using Attention-Guided Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128262
Ground-Aware Automotive Radar Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128161
CAO-RONet: A Robust 4D Radar Odometry with Exploring More Information from Low-Quality Points.,https://doi.org/10.1109/ICRA55743.2025.11128487
Radar Teach and Repeat: Architecture and Initial Field Testing.,https://doi.org/10.1109/ICRA55743.2025.11128412
Structure-Aware Radar-Camera Depth Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128760
Doppler Former: Velocity Supervision of Raw Radar Data.,https://doi.org/10.1109/ICRA55743.2025.11127345
Dynamic Multi-Objective Ergodic Path Planning Using Decomposition Methods.,https://doi.org/10.1109/ICRA55743.2025.11128676
Rapid Autonomous Exploration of Large-Scale Environments for Ground Robots Based on Region Partitioning.,https://doi.org/10.1109/ICRA55743.2025.11127661
MapEx: Indoor Structure Exploration with Probabilistic Information Gain from Global Map Predictions.,https://doi.org/10.1109/ICRA55743.2025.11128862
Ergodic Exploration over Meshable Surfaces.,https://doi.org/10.1109/ICRA55743.2025.11127626
Robotic Mushroom Harvesting with Real2Sim2Real and Model Predictive Path Integral (MPPI) Based Planning.,https://doi.org/10.1109/ICRA55743.2025.11127567
Collision-Aware Traversability Analysis for Autonomous Vehicles in the Context of Agricultural Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127764
Enhanced View Planning for Robotic Harvesting: Tackling Occlusions with Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11127892
Precision Harvesting in Cluttered Environments: Integrating End Effector Design with Dual Camera Perception.,https://doi.org/10.1109/ICRA55743.2025.11127557
"S2BEV: Lightweight, Robust, and Precise SLAM-Oriented Segmentation Bird Eye's View Mapping Approach.",https://doi.org/10.1109/ICRA55743.2025.11127686
Implicit Physics-aware Policy for Dynamic Manipulation of Rigid Objects via Soft Body Tools.,https://doi.org/10.1109/ICRA55743.2025.11127880
General-Purpose Clothes Manipulation with Semantic Keypoints.,https://doi.org/10.1109/ICRA55743.2025.11128573
ReloPush: Multi-Object Rearrangement in Confined Spaces with a Nonholonomic Mobile Robot Pusher.,https://doi.org/10.1109/ICRA55743.2025.11128108
Non-Prehensile Shape Manipulation of Elastoplastic Objects With Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127639
ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A.,https://doi.org/10.1109/ICRA55743.2025.11127569
EgoMimic: Scaling Imitation Learning via Egocentric Video.,https://doi.org/10.1109/ICRA55743.2025.11127989
Neural Dynamics Augmented Diffusion Policy.,https://doi.org/10.1109/ICRA55743.2025.11128651
Cage: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127739
RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot Learning from Demonstrations.,https://doi.org/10.1109/ICRA55743.2025.11128694
AVD2: Accident Video Diffusion for Accident Video Description.,https://doi.org/10.1109/ICRA55743.2025.11127968
LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models.,https://doi.org/10.1109/ICRA55743.2025.11127546
SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely Incomplete Depth Maps.,https://doi.org/10.1109/ICRA55743.2025.11128449
Dualdiff: Dual-Branch Diffusion Model for Autonomous Driving with Semantic Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128068
Anomalies-by-Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128540
EHC-MM: Embodied Holistic Control for Mobile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128612
BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128444
Dynamem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127619
Whole-Body Model Predictive Control for Mobile Manipulation With Task Priority Transition.,https://doi.org/10.1109/ICRA55743.2025.11127515
Dynamic Object Goal Pushing with Mobile Manipulators Through Model-Free Constrained Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128166
HSRL: A Hierarchical Control System Based on Spiking Deep Reinforcement Learning for Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128063
Materials Matter: Investigating Functional Advantages of Bio-Inspired Materials via Simulated Robotic Hopping.,https://doi.org/10.1109/ICRA55743.2025.11127856
SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128459
Hyperdimensional Computing-Based Federated Learning in Mobile Robots Through Synthetic Oversampling.,https://doi.org/10.1109/ICRA55743.2025.11127388
Brain-Inspired Spatial Continuous State Encoding for Efficient Spiking-Based Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128824
LuVo: Lunar Visual Odometry Using Homography-Based Image Feature Matching.,https://doi.org/10.1109/ICRA55743.2025.11127410
Instance Segmentation-Based Hazard Detection with Lunar South Pole Lighting.,https://doi.org/10.1109/ICRA55743.2025.11127967
Resettable Land Anchor Launcher for Unmanned Rover Rescue and Slope Climbing.,https://doi.org/10.1109/ICRA55743.2025.11128100
SOF-E: An Energy Efficient Robot for Collaborative Transport and Placement of Mechanical Meta-Material Modules.,https://doi.org/10.1109/ICRA55743.2025.11127784
Quarry-Bot: A Reconfigurable Cable-Suspended Robot for Lunar Site Engineering.,https://doi.org/10.1109/ICRA55743.2025.11127900
A Tugging Controller that Maximizes Lateral Resistive Force by Mounding Sandy Terrain.,https://doi.org/10.1109/ICRA55743.2025.11127386
RMSeg-UDA: Unsupervised Domain Adaptation for Road Marking Segmentation Under Adverse Conditions.,https://doi.org/10.1109/ICRA55743.2025.11128761
Enhancing the Utilization of Color Information in Point Cloud Semantic Segmentation.,https://doi.org/10.1109/ICRA55743.2025.11127522
UltraFastCrackSeg: A Lightweight Real-Time Crack Segmentation Model with Task-Oriented Pretraining.,https://doi.org/10.1109/ICRA55743.2025.11128493
Enhancing 3D Scene Graphs with Real-Time Room Classification.,https://doi.org/10.1109/ICRA55743.2025.11128432
Affordance-Based Explanations of Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128010
Explainable Reinforcement Learning via Dynamic Mixture Policies.,https://doi.org/10.1109/ICRA55743.2025.11127782
3D Spatial Understanding in MLLMs: Disambiguation and Evaluation.,https://doi.org/10.1109/ICRA55743.2025.11128559
Through the Clutter: Exploring the Impact of Complex Environments on the Legibility of Robot Motion.,https://doi.org/10.1109/ICRA55743.2025.11128635
OpenSU3D: Open World 3D Scene Understanding Using Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11127896
Task-Aware Semantic Map: Autonomous Robot Task Assignment Beyond Commands.,https://doi.org/10.1109/ICRA55743.2025.11127372
High-Quality Unknown Object Instance Segmentation via Quadruple Boundary Error Refinement.,https://doi.org/10.1109/ICRA55743.2025.11127393
Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph.,https://doi.org/10.1109/ICRA55743.2025.11128059
A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space.,https://doi.org/10.1109/ICRA55743.2025.11127317
Sea-U-Whale: A Reconfigurable Marine Robot with Multi-Modal Motion.,https://doi.org/10.1109/ICRA55743.2025.11128623
MERLION: Marine ExploRation with Language guIded Online iNformative Visual Sampling and Enhancement.,https://doi.org/10.1109/ICRA55743.2025.11127541
PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal.,https://doi.org/10.1109/ICRA55743.2025.11128583
Lightweight Yet High-Performance Defect Detector for Uav-Based Large-Scale Infrastructure Real-Time Inspection.,https://doi.org/10.1109/ICRA55743.2025.11127758
ProxFly: Robust Control for Close Proximity Quadcopter Flight Via Residual Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127714
HBTP: Heuristic Behavior Tree Planning with Large Language Model Reasoning.,https://doi.org/10.1109/ICRA55743.2025.11127999
SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments.,https://doi.org/10.1109/ICRA55743.2025.11128238
Closed Loop Interactive Embodied Reasoning for Robot Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127480
SayComply: Grounding Field Robotic Tasks in Operational Compliance Through Retrieval-Based Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128684
A Method for Constructing Building Structure Grid Map Based on a Climbing Algorithm.,https://doi.org/10.1109/ICRA55743.2025.11128509
Efficient Scale-Uniform 3D Visual Coverage Algorithm for UAV Based on Elastic Photogrammetric Constraints.,https://doi.org/10.1109/ICRA55743.2025.11127373
Target-Aware Viewpoint Generation for Active Robotic Exploration in Unknown Environments.,https://doi.org/10.1109/ICRA55743.2025.11127467
Online Multi-Robot Federated Learning for Distributed Coverage Control of Unknown Spatial Processes.,https://doi.org/10.1109/ICRA55743.2025.11127504
Constrained Learning for Decentralized Multi-Objective Coverage Control.,https://doi.org/10.1109/ICRA55743.2025.11128085
RAR-6: An Optimized Reconfigurable Asymmetric 6-DOF Haptic Robot for Gross and Fine Motor Tasks.,https://doi.org/10.1109/ICRA55743.2025.11128771
"Design, Implementation, and Validation of an Ungrounded Visuo-Tactile Haptic Interface for Robotic Teleoperation in High-Risk Steel Production.",https://doi.org/10.1109/ICRA55743.2025.11128864
"Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark.",https://doi.org/10.1109/ICRA55743.2025.11128430
GPU-Accelerated Subsystem-Based ADMM for Large-Scale Interactive Simulation.,https://doi.org/10.1109/ICRA55743.2025.11128665
Local Policies Enable Zero-Shot Long-Horizon Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128407
DART: Dexterous Augmented Reality Teleoperation Platform for Large-Scale Robot Data Collection in Simulation.,https://doi.org/10.1109/ICRA55743.2025.11128299
A Large-Scale Dataset for Humanoid Robotics Enabling a Novel Data-Driven Fall Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128646
Social-MAE: Social Masked Autoencoder for Multi-Person Motion Representation Learning.,https://doi.org/10.1109/ICRA55743.2025.11128614
Depth-Temporal Attention with Dual Modality Data for Walking Intention Prediction in Close-Proximity Front-Following.,https://doi.org/10.1109/ICRA55743.2025.11128562
UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11128323
PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance Fields.,https://doi.org/10.1109/ICRA55743.2025.11128604
FreeDriveRF: Monocular RGB Dynamic NeRF Without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling.,https://doi.org/10.1109/ICRA55743.2025.11127897
LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment.,https://doi.org/10.1109/ICRA55743.2025.11127924
Hash-GS: Anchor-Based 3D Gaussian Splatting with Multi-Resolution Hash Encoding for Efficient Scene Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128324
Elite-EvGS: Learning Event-based 3D Gaussian Splatting by Distilling Event-to-Video Priors.,https://doi.org/10.1109/ICRA55743.2025.11127518
Command Filtered Cartesian Impedance Control for Tendon Driven Continuum Manipulators with Actuator Fault Compensation.,https://doi.org/10.1109/ICRA55743.2025.11128624
A Synergistic Framework for Learning Shape Estimation and Shape-Aware Whole-Body Control Policy for Continuum Robots.,https://doi.org/10.1109/ICRA55743.2025.11128198
On the Benefits of Hysteresis in Tendon Driven Continuum Robots.,https://doi.org/10.1109/ICRA55743.2025.11128117
Automating Tension Calibration for Tendon-Driven Continuum Robots: A Low-Cost Approach Towards Consistent Teleoperation.,https://doi.org/10.1109/ICRA55743.2025.11128689
GraspSAM: When Segment Anything Model Meets Grasp Detection.,https://doi.org/10.1109/ICRA55743.2025.11128811
Dexterous Ungrasping Manipulation in Three Dimensions.,https://doi.org/10.1109/ICRA55743.2025.11127227
"You Only Estimate Once: Unified, One-stage, Real-Time Category-Level Articulated Object 6D Pose Estimation for Robotic Grasping.",https://doi.org/10.1109/ICRA55743.2025.11128513
Point Cloud Decomposition for Task-Oriented Grasping.,https://doi.org/10.1109/ICRA55743.2025.11127703
Adaptive Grasping of Moving Objects in Dense Clutter via Global-to-Local Detection and Static-to-Dynamic Planning.,https://doi.org/10.1109/ICRA55743.2025.11127936
UASTHN: Uncertainty-Aware Deep Homography Estimation for UAV Satellite-Thermal Geo-Localization.,https://doi.org/10.1109/ICRA55743.2025.11128423
Enhancing Feature Tracking Reliability for Visual Navigation Using Real-Time Safety Filter.,https://doi.org/10.1109/ICRA55743.2025.11128371
SuperLoc: The Key to Robust Lidar-Inertial Localization Lies in Predicting Alignment Risks Superodometry.Com/SuperLoc.,https://doi.org/10.1109/ICRA55743.2025.11127720
Active Illumination for Visual Ego-Motion Estimation in the Dark.,https://doi.org/10.1109/ICRA55743.2025.11127536
Belief Roadmaps with Uncertain Landmark Evanescence.,https://doi.org/10.1109/ICRA55743.2025.11128352
Rao-Blackwellized POMDP Planning.,https://doi.org/10.1109/ICRA55743.2025.11128426
Nearest-Neighbourless Asymptotically Optimal Motion Planning with Fully Connected Informed Trees (FCIT*).,https://doi.org/10.1109/ICRA55743.2025.11127785
Efficient Path Planning in Complex Environments with Trust Region Continuous Belief Tree Search.,https://doi.org/10.1109/ICRA55743.2025.11127956
PRIMER: Perception-Aware Robust Learning-Based Multiagent Trajectory Planner.,https://doi.org/10.1109/ICRA55743.2025.11128011
HGS-Planner: Hierarchical Planning Framework for Active Scene Reconstruction Using 3D Gaussian Splatting.,https://doi.org/10.1109/ICRA55743.2025.11127649
An Active Perception Game for Robust Information Gathering.,https://doi.org/10.1109/ICRA55743.2025.11128798
Take Your Best Shot: Sampling-Based Planning for Autonomous Photography.,https://doi.org/10.1109/ICRA55743.2025.11127973
GET-Zero: Graph Embodiment Transformer for Zero-Shot Embodiment Generalization.,https://doi.org/10.1109/ICRA55743.2025.11127922
Proprioceptive Object Shape and Size Extraction via In-Hand-Manipulation with a Variable Friction Robot Gripper.,https://doi.org/10.1109/ICRA55743.2025.11128387
Diffusion-Informed Probabilistic Contact Search for Multi-Finger Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127844
Variable-Friction In-Hand Manipulation for Arbitrary Objects via Diffusion-Based Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11127479
From Simple to Complex Skills: The Case of In-Hand Object Reorientation.,https://doi.org/10.1109/ICRA55743.2025.11128016
DROP: Dexterous Reorientation via Online Planning.,https://doi.org/10.1109/ICRA55743.2025.11128433
Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks.,https://doi.org/10.1109/ICRA55743.2025.11127878
MonLog: MONotonic-Constrained LOGistic Regressions for Automated Safety Curve Design.,https://doi.org/10.1109/ICRA55743.2025.11128207
Passivity Filters for Bilateral Teleoperation with Variable Impedance Control.,https://doi.org/10.1109/ICRA55743.2025.11128576
Robots that Learn to Safely Influence via Prediction-Informed Reach-Avoid Dynamic Games.,https://doi.org/10.1109/ICRA55743.2025.11128803
Multi-Layered Safety of Redundant Robot Manipulators Via Task-Oriented Planning and Control.,https://doi.org/10.1109/ICRA55743.2025.11127733
A Parameter-Efficient Tuning Framework for Language-Guided Object Grounding and Robot Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128679
Cascaded Diffusion Models for Neural Motion Planning.,https://doi.org/10.1109/ICRA55743.2025.11127413
Reinforcement Learning with Lie Group Orientations for Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128743
Catch It! Learning to Catch in Flight with Mobile Dexterous Hands.,https://doi.org/10.1109/ICRA55743.2025.11127596
Benchmarking Different QP Formulations and Solvers for Dynamic Quadrupedal Walking.,https://doi.org/10.1109/ICRA55743.2025.11128397
Indoor and Outdoor Multi-Terrain Stair-Climbing Robot Design.,https://doi.org/10.1109/ICRA55743.2025.11128391
WaLTER: A Wheel and Leg Tumbling Expedition Robot.,https://doi.org/10.1109/ICRA55743.2025.11128221
Deformable Multibody Modeling for Model Predictive Control in Legged Locomotion with Embodied Compliance.,https://doi.org/10.1109/ICRA55743.2025.11128827
Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing.,https://doi.org/10.1109/ICRA55743.2025.11128478
Time-Correlated Model Predictive Path Integral: Smooth Action Generation for Sampling-Based Control.,https://doi.org/10.1109/ICRA55743.2025.11128021
Gradient-Based Trajectory Optimization with Parallelized Differentiable Traffic Simulation.,https://doi.org/10.1109/ICRA55743.2025.11128070
Swept Volume-Aware Trajectory Planning and MPC Tracking for Multi-Axle Swerve-Drive AMRs.,https://doi.org/10.1109/ICRA55743.2025.11127914
Efficient Trajectory Generation Based on Traversable Planes in 3D Complex Architectural Spaces.,https://doi.org/10.1109/ICRA55743.2025.11128727
Model Predictive Control with Visibility Graphs for Humanoid Path Planning and Tracking Against Adversarial Opponents.,https://doi.org/10.1109/ICRA55743.2025.11128033
Learning Time-Optimal Online Replanning for Distributed Model Predictive Contouring Control of Quadrotors.,https://doi.org/10.1109/ICRA55743.2025.11128315
"M3DSS: A Multi-Platform, Multi-Sensor, and Multi-Scenario Dataset for SLAM System.",https://doi.org/10.1109/ICRA55743.2025.11128631
Uncertainty-Aware Visual-Inertial SLAM with Volumetric Occupancy Mapping.,https://doi.org/10.1109/ICRA55743.2025.11127824
Real-Time 3D Reconstruction via Camera-Lidar (2D) Fusion for Mobile Robots: A Gaussian Splatting Approach.,https://doi.org/10.1109/ICRA55743.2025.11128228
DVN-SLAM: Dynamic Visual Neural Slam Based on Local-Global Encoding.,https://doi.org/10.1109/ICRA55743.2025.11127308
Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic Environments.,https://doi.org/10.1109/ICRA55743.2025.11127324
AstroLoc2: Fast Sequential Depth-Enhanced Localization for Free-Flying Robots.,https://doi.org/10.1109/ICRA55743.2025.11128673
Mixing Data-Driven and Geometric Models for Satellite Docking Port State Estimation Using an Rgb or Event Camera.,https://doi.org/10.1109/ICRA55743.2025.11128225
A Control Strategy for an Orbital Manipulator Equipped with an External Actuator at the End-Effector.,https://doi.org/10.1109/ICRA55743.2025.11128098
Robotic Space Simulator: Controls Implementation for Auxiliary Axes and Zero-G Dynamics.,https://doi.org/10.1109/ICRA55743.2025.11128455
Development of Multi-Joint Biohybrid Soft Robot by Using Skeletal Muscle Tissue.,https://doi.org/10.1109/ICRA55743.2025.11128419
A Novel Underwater Robot with Carangiform Locomotion Achieved via Single Degree of Actuation and Magnetically Transmitted Traveling Wave.,https://doi.org/10.1109/ICRA55743.2025.11127395
AquaMILR: Mechanical Intelligence Simplifies Control of Undulatory Robots in Cluttered Fluid Environments.,https://doi.org/10.1109/ICRA55743.2025.11128233
Ambient Flow Perception of Freely Swimming Robotic Fish Using an Artificial Lateral Line System.,https://doi.org/10.1109/ICRA55743.2025.11128034
Leader-Follower Formation Enabled by Pressure Sensing in Free-Swimming Undulatory Robotic Fish.,https://doi.org/10.1109/ICRA55743.2025.11127903
Analysis of Kinematics and Propulsion of a Self-Sensing Multi-DoF Undulating Soft Robotic Fish.,https://doi.org/10.1109/ICRA55743.2025.11127497
Humanoid Walking Stabilization via Model Predictive Control with Step Adjustment Based on the 3D Divergent Component of Motion.,https://doi.org/10.1109/ICRA55743.2025.11128625
MPC-QP-Based Control Framework for Compliant Behavior of Humanoid Robots in Physical Collaboration with Humans.,https://doi.org/10.1109/ICRA55743.2025.11128037
Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control.,https://doi.org/10.1109/ICRA55743.2025.11128271
Wallbounce: Push Wall to Navigate with Contact-Implicit MPC.,https://doi.org/10.1109/ICRA55743.2025.11127229
Reduced-Order Model Guided Contact-Implicit Model Predictive Control for Humanoid Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11127605
Exploring the Domain-Invariant Flow Representation in Vision-Based Tactile Sensors for Omni-Hardness Perception.,https://doi.org/10.1109/ICRA55743.2025.11128719
Focused Blind Switching Manipulation Based on Constrained and Regional Touch States of Multi-Fingered Hand Using Deep Learning.,https://doi.org/10.1109/ICRA55743.2025.11127786
GAPartManip: A Large-Scale Part-Centric Dataset for Material-Agnostic Articulated Object Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128643
High-Precision Object Pose Estimation Using Visual-Tactile Information for Dynamic Interactions in Robotic Grasping.,https://doi.org/10.1109/ICRA55743.2025.11128649
SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model.,https://doi.org/10.1109/ICRA55743.2025.11128428
Lab2Car: A Versatile Wrapper for Deploying Experimental Planners in Complex Real-World Environments.,https://doi.org/10.1109/ICRA55743.2025.11128008
One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128393
Exploring Adversarial Obstacle Attacks in Search-Based Path Planning for Autonomous Mobile Robots.,https://doi.org/10.1109/ICRA55743.2025.11128179
Topological Mapping for Traversability-Aware Long-Range Navigation in Off-Road Terrain.,https://doi.org/10.1109/ICRA55743.2025.11128536
Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety.,https://doi.org/10.1109/ICRA55743.2025.11128218
Dual-AEB: Synergizing Rule-Based and Multimodal Large Language Models for Effective Emergency Braking.,https://doi.org/10.1109/ICRA55743.2025.11128453
Estimating Control Barriers from Offline Data.,https://doi.org/10.1109/ICRA55743.2025.11128526
Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control Barrier Functions.,https://doi.org/10.1109/ICRA55743.2025.11128366
FuzzRisk: Online Collision Risk Estimation for Autonomous Vehicles Based on Depth-Aware Object Detection via Fuzzy Inference.,https://doi.org/10.1109/ICRA55743.2025.11127390
Adaptive Deadlock Avoidance for Decentralized Multi-Agent Systems via CBF-Inspired Risk Measurement.,https://doi.org/10.1109/ICRA55743.2025.11128764
SEAL: A Sample-Efficient Adjustment-Learning Method for Table Tennis Robot Serve.,https://doi.org/10.1109/ICRA55743.2025.11128201
Planning with Adaptive World Models for Autonomous Driving.,https://doi.org/10.1109/ICRA55743.2025.11127860
Subassembly to Full Assembly: Effective Assembly Sequence Planning Through Graph-Based Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127710
Fuel-Optimal Operational Speed Planning for Autonomous Trucking on Highways.,https://doi.org/10.1109/ICRA55743.2025.11128565
Language-Conditioned Offline RL for Multi-Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127288
Deep Reinforcement Learning for Coordinated Payload Transport in Biped-Wheeled Robots.,https://doi.org/10.1109/ICRA55743.2025.11127939
Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer.,https://doi.org/10.1109/ICRA55743.2025.11128707
Residual Descent Differential Dynamic Game (RD3G) - A Fast Newton Solver for Constrained General Sum Games.,https://doi.org/10.1109/ICRA55743.2025.11127513
MARLadona - Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128256
Multi-Agent Inverse Q-Learning from Demonstrations.,https://doi.org/10.1109/ICRA55743.2025.11128370
LoGS: Visual Localization via Gaussian Splatting with Fewer Training Images.,https://doi.org/10.1109/ICRA55743.2025.11127759
Unified Human Localization and Trajectory Prediction with Monocular Vision.,https://doi.org/10.1109/ICRA55743.2025.11128645
Continuous Wrist Control on the Hannes Prosthesis: A Vision-Based Shared Autonomy Framework.,https://doi.org/10.1109/ICRA55743.2025.11127529
Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control.,https://doi.org/10.1109/ICRA55743.2025.11127940
Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems.,https://doi.org/10.1109/ICRA55743.2025.11127455
FedDet: Data Poisoning Attack Detection for Federated Skeleton-based Action Recognition.,https://doi.org/10.1109/ICRA55743.2025.11128277
ROS2WASM: Bringing the Robot Operating System to the Web.,https://doi.org/10.1109/ICRA55743.2025.11127821
Prepared for the Worst: Resilience Analysis of the ICP Algorithm via Learning-Based Worst-Case Adversarial Attacks.,https://doi.org/10.1109/ICRA55743.2025.11128007
Slamspoof: Practical Lidar Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis.,https://doi.org/10.1109/ICRA55743.2025.11127495
Gradient-Based Adversarial Attacks on Deep LiDAR Odometry.,https://doi.org/10.1109/ICRA55743.2025.11128081
Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through Curriculum Training.,https://doi.org/10.1109/ICRA55743.2025.11128558
Adaptive Perching and Grasping by Aerial Robot with Light-Weight and High Grip-Force Tendon-Driven Three-Fingered Hand Using Single Actuator.,https://doi.org/10.1109/ICRA55743.2025.11128377
CAFEs: Cable-Driven Collaborative Floating End-Effectors for Agriculture Applications.,https://doi.org/10.1109/ICRA55743.2025.11127592
High Speed Robotic Table Tennis Swinging Using Lightweight Hardware with Model Predictive Control.,https://doi.org/10.1109/ICRA55743.2025.11127907
Learning Quiet Walking for a Small Home Robot.,https://doi.org/10.1109/ICRA55743.2025.11128174
Evaluating Human-Robot Skill Gaps in Electrical Circuit Inspection: A New Electronic Task Board for Benchmarking Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128714
RaccoonBot: An Autonomous Wire-Traversing Solar-Tracking Robot for Persistent Environmental Monitoring.,https://doi.org/10.1109/ICRA55743.2025.11127963
Tendon Locking for Antagonistic Configuration- and Stiffness-Control in Soft Robots.,https://doi.org/10.1109/ICRA55743.2025.11127937
Large-Expansion Bi-Layer Auxetics Create Compliant Cellular Motion.,https://doi.org/10.1109/ICRA55743.2025.11127822
eViper-2D: A Thin Large-Area Soft Robotics Platform.,https://doi.org/10.1109/ICRA55743.2025.11128737
Bio-Inspired Soft Magnetic Swimming Robot for Flexible Motions.,https://doi.org/10.1109/ICRA55743.2025.11127462
Magnetic Programming of Soft Materials Using Digitally Processed Laser Heating.,https://doi.org/10.1109/ICRA55743.2025.11127804
Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation.,https://doi.org/10.1109/ICRA55743.2025.11128786
One-Shot Manipulation Strategy Learning by Making Contact Analogies.,https://doi.org/10.1109/ICRA55743.2025.11127356
Incremental Few-Shot Adaptation for Non-Prehensile Object Manipulation Using Parallelizable Physics Simulators.,https://doi.org/10.1109/ICRA55743.2025.11128222
Efficient Gradient-Based Inference for Manipulation Planning in Contact Factor Graphs.,https://doi.org/10.1109/ICRA55743.2025.11127769
Polyhedral Collision Detection via Vertex Enumeration.,https://doi.org/10.1109/ICRA55743.2025.11128289
Automatic Robotic-Assisted Diffuse Reflectance Spectroscopy Scanning System.,https://doi.org/10.1109/ICRA55743.2025.11127481
Robust and Accurate Multi-View 2D/3D Image Registration with Differentiable X-Ray Rendering and Dual Cross-View Constraints.,https://doi.org/10.1109/ICRA55743.2025.11128375
Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-Assisted Radioguided Surgery.,https://doi.org/10.1109/ICRA55743.2025.11128351
Improving Probe Localization for Freehand 3D Ultrasound Using Lightweight Cameras.,https://doi.org/10.1109/ICRA55743.2025.11128230
Robot-Based Automatic Charging for Electric Vehicles Using Incremental Learning and Biomimetic Control.,https://doi.org/10.1109/ICRA55743.2025.11128620
CC-STAR: An Estimation for Contact State Transition Using Reconstruction-Based Anomaly Detection for Peg-in-Hole Assembly.,https://doi.org/10.1109/ICRA55743.2025.11128660
"Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset.",https://doi.org/10.1109/ICRA55743.2025.11127489
Geometry and Force-Informed Robotic Assembly with Small Relative Initial Deviations for Circular Electrical Connectors.,https://doi.org/10.1109/ICRA55743.2025.11127385
MatchMaker: Automated Asset Generation for Robotic Assembly.,https://doi.org/10.1109/ICRA55743.2025.11128440
CNSv2: Probabilistic Correspondence Encoded Neural Image Servo.,https://doi.org/10.1109/ICRA55743.2025.11128608
Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms.,https://doi.org/10.1109/ICRA55743.2025.11128713
Fault Management System for the Safety of Perception Systems in Highly Automated Agricultural Machines.,https://doi.org/10.1109/ICRA55743.2025.11128195
Learning to Prune Branches in Modern Tree-Fruit Orchards.,https://doi.org/10.1109/ICRA55743.2025.11128361
Towards Safe and Efficient Through-the-Canopy Autonomous Fruit Counting with UAVs.,https://doi.org/10.1109/ICRA55743.2025.11127839
Language-Guided Object Search in Agricultural Environments.,https://doi.org/10.1109/ICRA55743.2025.11128175
ProDapt: Proprioceptive Adaptation Using Long-Term Memory Diffusion.,https://doi.org/10.1109/ICRA55743.2025.11128171
Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners.,https://doi.org/10.1109/ICRA55743.2025.11128293
Joint Localization and Planning Using Diffusion.,https://doi.org/10.1109/ICRA55743.2025.11127640
Diverse Motion Planning with Stein Diffusion Trajectory Inference.,https://doi.org/10.1109/ICRA55743.2025.11127953
The Ingredients for Robotic Diffusion Transformers.,https://doi.org/10.1109/ICRA55743.2025.11127493
Inference-Time Policy Steering Through Human Interactions.,https://doi.org/10.1109/ICRA55743.2025.11127931
Visually Robust Adversarial Imitation Learning from Videos with Contrastive Learning.,https://doi.org/10.1109/ICRA55743.2025.11127595
One-Shot Imitation Under Mismatched Execution.,https://doi.org/10.1109/ICRA55743.2025.11128594
RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11127799
Improving Vision-Language-Action Model with Online Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127299
MILE: Model-Based Intervention Learning.,https://doi.org/10.1109/ICRA55743.2025.11128018
Validity Learning on Failures: Mitigating the Distribution Shift in Autonomous Vehicle Planning.,https://doi.org/10.1109/ICRA55743.2025.11128610
SaViD: Spectravista Aesthetic Vision Integration for Robust and Discerning 3D Object Detection in Challenging Environments.,https://doi.org/10.1109/ICRA55743.2025.11128252
CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection Based View Transformation.,https://doi.org/10.1109/ICRA55743.2025.11128369
Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning.,https://doi.org/10.1109/ICRA55743.2025.11127669
RoBiFusion: A Robust and Bidirectional Interaction Camera-LiDAR 3D Object Detection Framework.,https://doi.org/10.1109/ICRA55743.2025.11127735
Towards Accurate Semi-Supervised BEV 3D Object Detection with Depth-Aware Refinement and Denoising-Aided Alignment.,https://doi.org/10.1109/ICRA55743.2025.11128845
SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation.,https://doi.org/10.1109/ICRA55743.2025.11128619
Bandwidth-Adaptive Spatiotemporal Correspondence Identification for Collaborative Perception.,https://doi.org/10.1109/ICRA55743.2025.11127581
Polyp-Gen: Realistic and Diverse Polyp Image Generation for Endoscopic Dataset Expansion.,https://doi.org/10.1109/ICRA55743.2025.11128145
DetailRefine: Towards Fine-Grained and Efficient Online Monocular 3D Reconstruction.,https://doi.org/10.1109/ICRA55743.2025.11128107
DAP-LED: Learning Degradation-Aware Priors with Clip for Joint Low-Light Enhancement and Deblurring.,https://doi.org/10.1109/ICRA55743.2025.11128232
"Fusionsense: Bridging Common Sense, Vision, and Touch for Robust Sparse-View Reconstruction.",https://doi.org/10.1109/ICRA55743.2025.11128188
TCAFF: Temporal Consistency for Robot Frame Alignment.,https://doi.org/10.1109/ICRA55743.2025.11127311
Effective Heterogeneous Point Cloud-Based Place Recognition and Relative Localization for Ground and Aerial Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11128163
Distributed Invariant Kalman Filter for Object-Level Multi-Robot Pose SLAM.,https://doi.org/10.1109/ICRA55743.2025.11128538
MT-PCR: Leveraging Modality Transformation for Large-Scale Point Cloud Registration with Limited Overlap.,https://doi.org/10.1109/ICRA55743.2025.11128154
"The qPCRBot: Combining Automated Data Handling, Standardization, and Robotic Labware Transport for Better qPCR Measurements.",https://doi.org/10.1109/ICRA55743.2025.11128567
Distributed Pursuit of an Evader with Adaptive Robust Path Control Under State Measurement Uncertainty.,https://doi.org/10.1109/ICRA55743.2025.11128445
Multimodal Behaviour Trees for Robotic Laboratory Task Automation.,https://doi.org/10.1109/ICRA55743.2025.11128408
A Hierarchical Graph-Based Terrain-Aware Autonomous Navigation Approach for Complementary Multimodal Ground-Aerial Exploration.,https://doi.org/10.1109/ICRA55743.2025.11128079
Introducing Collaborative Robots as a First Step Towards Autonomous Reprocessing of Medical Equipment.,https://doi.org/10.1109/ICRA55743.2025.11127893
CloudTrack: Scalable UAV Tracking with Cloud Semantics.,https://doi.org/10.1109/ICRA55743.2025.11128514
The Experiment Orchestration System (EOS): Comprehensive Foundation for Laboratory Automation.,https://doi.org/10.1109/ICRA55743.2025.11128578
Design of a Bioinspired Jumping Mechanism for Self-Takeoff of Flapping Robot.,https://doi.org/10.1109/ICRA55743.2025.11128655
Embodied Adaptive Sensing for Odor Concentration Maximization in Bio-Inspired Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128093
"SKOOTR: A Skating, Omni-Oriented, Tripedal Robot.",https://doi.org/10.1109/ICRA55743.2025.11127357
AllGaits: Learning All Quadruped Gaits and Transitions.,https://doi.org/10.1109/ICRA55743.2025.11127285
Bird-Inspired Tendon Coupling Improves Paddling Efficiency by Shortening Phase Transition Times.,https://doi.org/10.1109/ICRA55743.2025.11128691
A Bio-Inspired Sand-Rolling Robot: Effect of Body Shape on Sand Rolling Performance.,https://doi.org/10.1109/ICRA55743.2025.11128406
Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving.,https://doi.org/10.1109/ICRA55743.2025.11128263
Think on Your Feet: Seamless Transition Between Human-Like Locomotion in Response to Changing Commands.,https://doi.org/10.1109/ICRA55743.2025.11127948
RINA: Rapid Introspective Neural Adaptation for Out-of-Distribution Payload Configurations on Quadruped Robots.,https://doi.org/10.1109/ICRA55743.2025.11128634
Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion.,https://doi.org/10.1109/ICRA55743.2025.11128700
Robust Robot Walker: Learning Agile Locomotion over Tiny Traps.,https://doi.org/10.1109/ICRA55743.2025.11128309
FRASA: An End-to-End Reinforcement Learning Agent for Fall Recovery and Stand Up of Humanoid Robots.,https://doi.org/10.1109/ICRA55743.2025.11128664
DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for Anomaly Situation in Rough Terrains.,https://doi.org/10.1109/ICRA55743.2025.11127805
OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World.,https://doi.org/10.1109/ICRA55743.2025.11128589
Self-Supervised Learning of Reconstructing Deformable Linear Objects Under Single-Frame Occluded View.,https://doi.org/10.1109/ICRA55743.2025.11128280
PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128208
Segment Any Repeated Object.,https://doi.org/10.1109/ICRA55743.2025.11128131
ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128495
DoorBot: Closed-Loop Task Planning and Manipulation for Door Opening in the Wild with Haptic Feedback.,https://doi.org/10.1109/ICRA55743.2025.11128394
Probabilistic Latent Variable Modeling for Dynamic Friction Identification and Estimation.,https://doi.org/10.1109/ICRA55743.2025.11128668
Learning Three-Dimensional Bin Packing with Adjustable-Order Semi-Online Setting.,https://doi.org/10.1109/ICRA55743.2025.11127655
Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization.,https://doi.org/10.1109/ICRA55743.2025.11128797
Magnetometer-Calibrated Hybrid Transformer for Robust Inertial Tracking in Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127359
MotionGlot: A Multi-Embodied Motion Generation Model.,https://doi.org/10.1109/ICRA55743.2025.11127473
Retinex-BEVFormer: Using Retinex to Enhance Multi-View Image-Based BEV Detector in Low Light Scenes.,https://doi.org/10.1109/ICRA55743.2025.11127731
Reactive Collision Avoidance for Safe Agile Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127284
Hardware-Accelerated Ray Tracing for Discrete and Continuous Collision Detection on GPUs.,https://doi.org/10.1109/ICRA55743.2025.11128528
Collision Avoidance in Model Predictive Control Using Velocity Damper.,https://doi.org/10.1109/ICRA55743.2025.11128856
On the Synthesis of Reactive Collision-Free Whole-Body Robot Motions: A Complementarity-Based Approach.,https://doi.org/10.1109/ICRA55743.2025.11127913
Rapid Dynamic Obstacle Avoidance for UAVs Enhanced by DVS and Neuromorphic Computing.,https://doi.org/10.1109/ICRA55743.2025.11128605
Efficient Collision Detection Framework for Enhancing Collision-Free Robot Motion.,https://doi.org/10.1109/ICRA55743.2025.11128733
Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments.,https://doi.org/10.1109/ICRA55743.2025.11128172
Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-Level Goal Decomposition.,https://doi.org/10.1109/ICRA55743.2025.11127617
OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics.,https://doi.org/10.1109/ICRA55743.2025.11127476
Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following.,https://doi.org/10.1109/ICRA55743.2025.11128677
Hypergraph-Based Coordinated Task Allocation and Socially-Aware Navigation for Multi-Robot Systems.,https://doi.org/10.1109/ICRA55743.2025.11128092
Bootstrapping Object-Level Planning with Large Language Models.,https://doi.org/10.1109/ICRA55743.2025.11127365
CognitiveOS: Large Multimodal Model Based System to Endow Any Type of Robot with Generative AI.,https://doi.org/10.1109/ICRA55743.2025.11128224
CLSTR: Capability-Level System for Tracking Robots.,https://doi.org/10.1109/ICRA55743.2025.11127750
Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment.,https://doi.org/10.1109/ICRA55743.2025.11128149
Decentralized Drone Swaps for Online Rebalancing of Drone Delivery Tasks.,https://doi.org/10.1109/ICRA55743.2025.11127517
A Fairness-Oriented Control Framework for Safety-Critical Multi-Robot Systems: Alternative Authority Control.,https://doi.org/10.1109/ICRA55743.2025.11127533
FogROS2-PLR: Probabilistic Latency-Reliability for Cloud Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127588
Jointly Assigning Processes to Machines and Generating Plans for Autonomous Mobile Robots in a Smart Factory.,https://doi.org/10.1109/ICRA55743.2025.11127768
A Control Scheme for Collaborative Object Transportation between a Human and a Quadruped Robot Using the MIGHTY Suction Cup.,https://doi.org/10.1109/ICRA55743.2025.11127351
DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11127335
Learning-Based Dynamic Robot-to-Human Handover.,https://doi.org/10.1109/ICRA55743.2025.11128251
A Novel Dynamic Motion Primitives Framework for Safe Human-Robot Collaboration.,https://doi.org/10.1109/ICRA55743.2025.11127783
Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot Handover.,https://doi.org/10.1109/ICRA55743.2025.11128648
Leveraging Semantic and Geometric Information for Zero-Shot Robot-to-Human Handover.,https://doi.org/10.1109/ICRA55743.2025.11128058
"Manual, Semi or Fully Autonomous Flipper Control? A Framework for Fair Comparison.",https://doi.org/10.1109/ICRA55743.2025.11128663
Safety-Critical Locomotion of Biped Robots in Infeasible Paths: Overcoming Obstacles During Navigation Toward Destination.,https://doi.org/10.1109/ICRA55743.2025.11127337
Optimal Framework for Constrained Admittance Path-Following Control.,https://doi.org/10.1109/ICRA55743.2025.11127361
Robust Orientation Control of Robot Manipulator Using Orientation Disturbance Observer.,https://doi.org/10.1109/ICRA55743.2025.11128130
Predictive Kinematic Coordinate Control for Aerial Manipulators Based on Modified Kinematics Learning.,https://doi.org/10.1109/ICRA55743.2025.11128580
Investigating Security Threats in Multi-Tenant ROS 2 Systems.,https://doi.org/10.1109/ICRA55743.2025.11127490
Multi-Task Robustness Enhancement Framework against Various Adversarial Patches.,https://doi.org/10.1109/ICRA55743.2025.11127806
Perfectly Undetectable False Data Injection Attacks on Encrypted Bilateral Teleoperation System based on Dynamic Symmetry and Malleability.,https://doi.org/10.1109/ICRA55743.2025.11128026
A Novel Under-Actuated Gripper Based on Passive-Locking Mechanism for Stable Gripping Under Environmental Constraints.,https://doi.org/10.1109/ICRA55743.2025.11127662
Juzu Type Gripper That Can Change Both Shape and Firmness.,https://doi.org/10.1109/ICRA55743.2025.11127377
A Direct-Drive Gripper Designed by Ellipse Synthesis Across Two Output Modes.,https://doi.org/10.1109/ICRA55743.2025.11127705
Mechanisms and Computational Design of Multi-Modal End-Effector with Force Sensing Using Gated Networks.,https://doi.org/10.1109/ICRA55743.2025.11127886
STEER: Flexible Robotic Manipulation via Dense Language Grounding.,https://doi.org/10.1109/ICRA55743.2025.11127404
MBE-ARI: A Multimodal Dataset Mapping Bi-Directional Engagement in Animal-Robot Interaction.,https://doi.org/10.1109/ICRA55743.2025.11127962
A Piezoresistive Printable Strain Sensor for Monitoring and Control of Soft Robotic Links.,https://doi.org/10.1109/ICRA55743.2025.11128728
AnySkin: Plug-and-Play Skin Sensing for Robotic Touch.,https://doi.org/10.1109/ICRA55743.2025.11128638
Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation.,https://doi.org/10.1109/ICRA55743.2025.11128186
Spatial Sensitivity Equalization of ERT-Based Robotic Skin Through Gauge Factor Distribution Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128214
Milli-Scale AcousTac Sensing Using Soft Helmholtz Resonators.,https://doi.org/10.1109/ICRA55743.2025.11128844
Efficient and Diverse Generative Robot Designs Using Evolution and Intrinsic Motivation.,https://doi.org/10.1109/ICRA55743.2025.11127289
A Novel Hybrid Hysteresis Modeling Method for Multiloop-Asymmetry Hysteresis Behavior of Nonlinear Compliant Actuators.,https://doi.org/10.1109/ICRA55743.2025.11128054
Dynamic Mode Decomposition with Sonomyography and Electromyography for Predictive Modeling of Lower Limb Exoskeleton Walking.,https://doi.org/10.1109/ICRA55743.2025.11128825
Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation.,https://doi.org/10.1109/ICRA55743.2025.11127456
Agile Mobility with Rapid Online Adaptation via Meta-Learning and Uncertainty-Aware MPPI.,https://doi.org/10.1109/ICRA55743.2025.11128505
A Data-Driven Aggressive Autonomous Racing Framework Utilizing Local Trajectory Planning with Velocity Prediction.,https://doi.org/10.1109/ICRA55743.2025.11128227
RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on Scaled Platforms.,https://doi.org/10.1109/ICRA55743.2025.11127278
Uncertainty-Aware Probabilistic Risk Quantification of SOTIF for Autonomous Vehicles.,https://doi.org/10.1109/ICRA55743.2025.11127514
Think Deep and Fast: Learning Neural Nonlinear Opinion Dynamics from Inverse Dynamic Games for Split-Second Interactions.,https://doi.org/10.1109/ICRA55743.2025.11127283
Online Risk-Bounded Graph-Based Local Planning for Autonomous Driving With Theoretical Guarantees.,https://doi.org/10.1109/ICRA55743.2025.11128187
Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11128442
Domain Randomization for Object Detection in Manufacturing Applications Using Synthetic Data: A Comprehensive Study.,https://doi.org/10.1109/ICRA55743.2025.11128647
Component-Aware Unsupervised Logical Anomaly Generation for Industrial Anomaly Detection.,https://doi.org/10.1109/ICRA55743.2025.11127296
"Use the Force, Bot! - Force-Aware ProDMP with Event-Based Replanning.",https://doi.org/10.1109/ICRA55743.2025.11128718
Reinforcement Learning on Reconfigurable Hardware: Overcoming Material Variability in Laser Material Processing.,https://doi.org/10.1109/ICRA55743.2025.11127658
GenCo: A Dual VLM Generate-Correct Framework for Adaptive Peg-in-Hole Robotics.,https://doi.org/10.1109/ICRA55743.2025.11128409
ASCENT: Autonomous Skill Learning Toward Complex Embodied Tasks With Foundation Models.,https://doi.org/10.1109/ICRA55743.2025.11127927
Ms. NAMI: Multimodal Semantic Navigation on Relative Metric Intention Graph.,https://doi.org/10.1109/ICRA55743.2025.11128364
A Dataset and Benchmark for Shape Completion of Fruits for Agricultural Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127685
A Novel Control Strategy for Offset Points Tracking in the Context of Agricultural Robotics.,https://doi.org/10.1109/ICRA55743.2025.11127590
Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields.,https://doi.org/10.1109/ICRA55743.2025.11127550
Safe Leaf Manipulation for Accurate Shape and Pose Estimation of Occluded Fruits.,https://doi.org/10.1109/ICRA55743.2025.11128788
Autonomous Sensor Exchange and Calibration for Cornstalk Nitrate Monitoring Robot.,https://doi.org/10.1109/ICRA55743.2025.11127469
Enhancing Agricultural Environment Perception via Active Vision and Zero-Shot Learning.,https://doi.org/10.1109/ICRA55743.2025.11127532
From Configuration-Space Clearance to Feature-Space Margin: Sample Complexity in Learning-Based Collision Detection.,https://doi.org/10.1109/ICRA55743.2025.11128311
CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented Robot Exploration.,https://doi.org/10.1109/ICRA55743.2025.11127932
Guiding Long-Horizon Task and Motion Planning with Vision Language Models.,https://doi.org/10.1109/ICRA55743.2025.11128705
CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized Variational AutoEncoder for Dense Crowd Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128132
Safe Multi-Agent Navigation Guided by Goal-Conditioned Safe Reinforcement Learning.,https://doi.org/10.1109/ICRA55743.2025.11127461
Towards Effective Utilization of Mixed-Quality Demonstrations in Robotic Manipulation via Segment-Level Selection and Optimization.,https://doi.org/10.1109/ICRA55743.2025.11128787
DABI: Evaluation of Data Augmentation Methods Using Downsampling in Bilateral Control-Based Imitation Learning with Images.,https://doi.org/10.1109/ICRA55743.2025.11128686
Learning From Imperfect Demonstrations With Self-Supervision for Robotic Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11127918
Match Policy: A Simple Pipeline from Point Cloud Registration to Manipulation Policies.,https://doi.org/10.1109/ICRA55743.2025.11128725
Self-Improving Autonomous Underwater Manipulation.,https://doi.org/10.1109/ICRA55743.2025.11128759
DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning.,https://doi.org/10.1109/ICRA55743.2025.11127809
ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos.,https://doi.org/10.1109/ICRA55743.2025.11128283
3D Multi-Modal Object Detection Based on Cross-Attention Feature Fusion.,https://doi.org/10.1109/ICRA55743.2025.11128243
Multi-Modality Test-Time Adaptation for Semantic Segmentation in Robotic Perception.,https://doi.org/10.1109/ICRA55743.2025.11127565
Illumination Adaptation for SAM to Achieve Accurate Segmentation of Images Taken in Low-Light Scenes.,https://doi.org/10.1109/ICRA55743.2025.11128183
4DRadDet: Cluster-Queried Enhanced 3D Object Detection with 4D Radar.,https://doi.org/10.1109/ICRA55743.2025.11127889
SALON: Self-supervised Adaptive Learning for Off-road Navigation.,https://doi.org/10.1109/ICRA55743.2025.11128268
In-Pipe Navigation Development Environment and a Smooth Path Planning Method on Pipeline Surface.,https://doi.org/10.1109/ICRA55743.2025.11128124
$U^2$ Frame: A Unified and Unsupervised Learning Framework for LiDAR-Based Loop Closing.,https://doi.org/10.1109/ICRA55743.2025.11127309
